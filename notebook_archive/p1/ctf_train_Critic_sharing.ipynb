{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- On-Policy\n",
    "- \n",
    "\n",
    "### Sampling\n",
    "- [ ] Mini-batch to update 'average' gradient\n",
    "- [ ] Experience Replay for Random Sampling\n",
    "- [x] Importance Sampling\n",
    "    \n",
    "### Policy Gradient\n",
    "- [x] MA-A3C\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [ ] Normalized Reward/Advantage\n",
    "- [x] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [x] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [ ] Self-play\n",
    "- [ ] Batch Policy\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "- https://arxiv.org/pdf/1706.02275.pdf\n",
    "\n",
    "## TODO:\n",
    "\n",
    "- Try to add experience buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf logs/Critic_sharing/ model/Critic_sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='Critic_sharing'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME\n",
    "GPU_CAPACITY=0.95 # gpu capacity in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "import threading\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.random\n",
    "import policy.roomba\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import discount_rewards\n",
    "from utility.buffer import Trajectory\n",
    "\n",
    "from network.critic_sharing import Critic_sharing as Network\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Variables\n",
    "total_episodes= 200000\n",
    "max_ep = 150\n",
    "update_frequency = 32\n",
    "policy_rotation = 128\n",
    "\n",
    "# Saving Related\n",
    "save_network_frequency = 1200\n",
    "save_stat_frequency = 128\n",
    "moving_average_step = 128\n",
    "\n",
    "# Training Variables\n",
    "lr_a = 5e-5\n",
    "lr_c = 4e-4\n",
    "\n",
    "\n",
    "gamma = 0.98 # discount_factor\n",
    "\n",
    "# Env Settings\n",
    "MAP_SIZE = 20\n",
    "VISION_RANGE = 19 # What decide the network size !!!\n",
    "VISION_dX, VISION_dY = 2*VISION_RANGE+1, 2*VISION_RANGE+1\n",
    "in_size = [None,VISION_dX,VISION_dY,6]\n",
    "num_policy_pool = 8\n",
    "nenv = 8\n",
    "\n",
    "# Asynch Settings\n",
    "global_scope = 'global'\n",
    "\n",
    "RED_POLICY = policy.zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_space = 5\n",
    "n_agent = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A3C Network Structure\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*YtnGhtSAMnnHSL8PvS7t_w.png\" width=\"450\">\n",
    "\n",
    "- Network is given in network.ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*Hzql_1t0-wwDxiz0C97AcQ.png\" width=\"450\">\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/MADDPG.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_ep_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "global_episodes = 0\n",
    "\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "#sess = tf.Session()\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    def __init__(self, name, global_network, sess, global_step=0):\n",
    "        # Initialize Environment worker\n",
    "        self.env = gym.make(\"cap-v0\").unwrapped\n",
    "        self.env.reset(map_size=MAP_SIZE,\n",
    "                       policy_red=RED_POLICY.PolicyGen(self.env.get_map, self.env.get_team_red))\n",
    "        self.name = name\n",
    "        \n",
    "        # Create AC Network for Worker\n",
    "        self.network = Network(in_size=in_size,\n",
    "                     action_size=action_space,\n",
    "                     num_agent=n_agent,\n",
    "                     lr_actor=lr_a,\n",
    "                     lr_critic=lr_c,\n",
    "                     scope=self.name,\n",
    "                     entropy_beta = 0.01,\n",
    "                     sess=sess,\n",
    "                     global_network=global_network,\n",
    "                     num_policy_pool=num_policy_pool,\n",
    "                     allow_same_policy=True)\n",
    "        \n",
    "        self.sess=sess\n",
    "        \n",
    "    def work(self, saver, writer):\n",
    "        global global_rewards, global_ep_rewards, global_episodes, global_length, global_succeed\n",
    "        traj_depth = 5\n",
    "        local_ep = 0\n",
    "                \n",
    "        with self.sess.as_default(), self.sess.graph.as_default():\n",
    "            while global_episodes < total_episodes:\n",
    "                local_ep += 1\n",
    "                self.env.reset()\n",
    "                s1 = one_hot_encoder(self.env._env, self.env.get_team_blue, VISION_RANGE)\n",
    "                a1, a_probs1, v1 = self.network.get_action_critic(s1)\n",
    "                \n",
    "                # parameters \n",
    "                ep_r = 0 # Episodic Reward\n",
    "                prev_r = 0\n",
    "                rc = 0\n",
    "                is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "\n",
    "                trajs = [Trajectory(depth=traj_depth) for _ in range(n_agent)]\n",
    "                \n",
    "                for step in range(max_ep+1):\n",
    "                    # Iteration\n",
    "                    a, a_probs, v0 = a1, a_probs1, v1\n",
    "                    was_alive = is_alive\n",
    "                    s0 = s1\n",
    "                    prev_r = rc\n",
    "                    \n",
    "                    s1, rc, d, _ = self.env.step(a)\n",
    "                    s1 = one_hot_encoder(self.env._env, self.env.get_team_blue, VISION_RANGE)\n",
    "                    is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "\n",
    "                    r = (rc - prev_r)\n",
    "                    if step == max_ep and d == False:\n",
    "                        r = -100\n",
    "                        rc = -100\n",
    "                        d = True\n",
    "\n",
    "                    r /= 100.0\n",
    "                    ep_r += r\n",
    "\n",
    "                    if d:\n",
    "                        v1 = [0.0 for _ in range(len(self.env.get_team_blue))]\n",
    "                    else:\n",
    "                        a1, a_probs1, v1 = self.network.get_action_critic(s1)\n",
    "\n",
    "                    # push to buffer\n",
    "                    for idx, agent in enumerate(self.env.get_team_blue):\n",
    "                        if was_alive[idx]:\n",
    "                            trajs[idx].append([s0[idx],\n",
    "                                              a[idx],\n",
    "                                              r,\n",
    "                                              v0[idx],\n",
    "                                              a_probs[idx][a[idx]]\n",
    "                                            ])\n",
    "\n",
    "\n",
    "                    \n",
    "                    if local_ep % update_frequency == 0 or d:\n",
    "                        alosses, closs = self.train(trajs, sess, v1)\n",
    "                        trajs = [Trajectory(depth=traj_depth) for _ in range(n_agent)]\n",
    "\n",
    "                    if d:\n",
    "                        break\n",
    "                        \n",
    "                global_ep_rewards.append(ep_r)\n",
    "                global_rewards.append(rc)\n",
    "                global_length.append(step)\n",
    "                global_succeed.append(self.env.blue_win)\n",
    "                global_episodes += 1\n",
    "                self.sess.run(global_step_next)\n",
    "                progbar.update(global_episodes)\n",
    "                if global_episodes % save_stat_frequency == 0 and global_episodes != 0:\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Records/mean_reward', simple_value=global_rewards())\n",
    "                    summary.value.add(tag='Records/mean_length', simple_value=global_length())\n",
    "                    summary.value.add(tag='Records/mean_succeed', simple_value=global_succeed())\n",
    "                    summary.value.add(tag='Records/mean_episode_reward', simple_value=global_ep_rewards())\n",
    "                    summary.value.add(tag='summary/actor_loss_mean', simple_value=np.mean(alosses))\n",
    "                    summary.value.add(tag='summary/critic_loss', simple_value=closs)\n",
    "                    writer.add_summary(summary,global_episodes)\n",
    "                    writer.flush()\n",
    "                if global_episodes % save_network_frequency == 0:\n",
    "                    saver.save(self.sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "                    \n",
    "                if local_ep % policy_rotation == 0:\n",
    "                    self.network.select_policy()\n",
    "                    self.network.pull_global()\n",
    "\n",
    "                        \n",
    "    def train(self, trajs, sess, bootstrap=0.0):\n",
    "        obs = []\n",
    "        act = []\n",
    "        adv = []\n",
    "        td  = []\n",
    "        beta= []\n",
    "        alosses = []\n",
    "        closses = []\n",
    "        for idx, traj in enumerate(trajs):\n",
    "            if len(traj) == 0:\n",
    "                alosses.append(None)\n",
    "                closses.append(None)\n",
    "                obs.append(None)\n",
    "                act.append(None)\n",
    "                adv.append(None)\n",
    "                td.append(None)\n",
    "                beta.append(None)\n",
    "                continue\n",
    "            observations = traj[0]\n",
    "            actions = traj[1]\n",
    "            rewards = np.array(traj[2])\n",
    "            values = np.array(traj[3])\n",
    "            behavior_policy = np.array(traj[4])\n",
    "            \n",
    "            rewards_ext = np.append(rewards, [bootstrap[idx]])\n",
    "            #discounted_rewards = discount_rewards(rewards_ext,gamma)[:-1]\n",
    "            value_ext = np.append(values, [bootstrap[idx]])\n",
    "            td_targets = rewards + gamma * value_ext[1:]\n",
    "            advantages = rewards + gamma * value_ext[1:] - value_ext[:-1]\n",
    "            advantages = discount_rewards(advantages,gamma)\n",
    "            \n",
    "            obs.append(observations)\n",
    "            act.append(actions)\n",
    "            adv.append(advantages)\n",
    "            td.append(td_targets)\n",
    "            beta.append(behavior_policy)\n",
    "            \n",
    "        alosses, closs = self.network.update_full(obs,act,adv,td,beta)\n",
    "        \n",
    "        self.network.pull_global()\n",
    "        \n",
    "        return alosses, closs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: W_0 initializing\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namsong/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: W_1 initializing\n",
      "worker: W_2 initializing\n",
      "worker: W_3 initializing\n",
      "worker: W_4 initializing\n",
      "worker: W_5 initializing\n",
      "worker: W_6 initializing\n",
      "worker: W_7 initializing\n",
      "INFO:tensorflow:Restoring parameters from ./model/Critic_sharing/ctf_policy.ckpt-99600\n",
      "Load Model :  ./model/Critic_sharing/ctf_policy.ckpt-99600\n",
      "INFO:tensorflow:Summary name global/actor0/Conv/weights:0 is illegal; using global/actor0/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor0/Conv/biases:0 is illegal; using global/actor0/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor0/Conv_1/weights:0 is illegal; using global/actor0/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor0/Conv_1/biases:0 is illegal; using global/actor0/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor0/fully_connected/weights:0 is illegal; using global/actor0/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor0/fully_connected/biases:0 is illegal; using global/actor0/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor1/Conv/weights:0 is illegal; using global/actor1/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor1/Conv/biases:0 is illegal; using global/actor1/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor1/Conv_1/weights:0 is illegal; using global/actor1/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor1/Conv_1/biases:0 is illegal; using global/actor1/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor1/fully_connected/weights:0 is illegal; using global/actor1/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor1/fully_connected/biases:0 is illegal; using global/actor1/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor2/Conv/weights:0 is illegal; using global/actor2/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor2/Conv/biases:0 is illegal; using global/actor2/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor2/Conv_1/weights:0 is illegal; using global/actor2/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor2/Conv_1/biases:0 is illegal; using global/actor2/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor2/fully_connected/weights:0 is illegal; using global/actor2/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor2/fully_connected/biases:0 is illegal; using global/actor2/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor3/Conv/weights:0 is illegal; using global/actor3/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor3/Conv/biases:0 is illegal; using global/actor3/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor3/Conv_1/weights:0 is illegal; using global/actor3/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor3/Conv_1/biases:0 is illegal; using global/actor3/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor3/fully_connected/weights:0 is illegal; using global/actor3/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor3/fully_connected/biases:0 is illegal; using global/actor3/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor4/Conv/weights:0 is illegal; using global/actor4/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor4/Conv/biases:0 is illegal; using global/actor4/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor4/Conv_1/weights:0 is illegal; using global/actor4/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor4/Conv_1/biases:0 is illegal; using global/actor4/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor4/fully_connected/weights:0 is illegal; using global/actor4/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor4/fully_connected/biases:0 is illegal; using global/actor4/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor5/Conv/weights:0 is illegal; using global/actor5/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor5/Conv/biases:0 is illegal; using global/actor5/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor5/Conv_1/weights:0 is illegal; using global/actor5/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor5/Conv_1/biases:0 is illegal; using global/actor5/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor5/fully_connected/weights:0 is illegal; using global/actor5/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor5/fully_connected/biases:0 is illegal; using global/actor5/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor6/Conv/weights:0 is illegal; using global/actor6/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor6/Conv/biases:0 is illegal; using global/actor6/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor6/Conv_1/weights:0 is illegal; using global/actor6/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor6/Conv_1/biases:0 is illegal; using global/actor6/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor6/fully_connected/weights:0 is illegal; using global/actor6/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor6/fully_connected/biases:0 is illegal; using global/actor6/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor7/Conv/weights:0 is illegal; using global/actor7/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor7/Conv/biases:0 is illegal; using global/actor7/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor7/Conv_1/weights:0 is illegal; using global/actor7/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor7/Conv_1/biases:0 is illegal; using global/actor7/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor7/fully_connected/weights:0 is illegal; using global/actor7/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/actor7/fully_connected/biases:0 is illegal; using global/actor7/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/Conv/weights:0 is illegal; using global/critic/Conv/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/Conv/biases:0 is illegal; using global/critic/Conv/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/Conv_1/weights:0 is illegal; using global/critic/Conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/Conv_1/biases:0 is illegal; using global/critic/Conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/fully_connected/weights:0 is illegal; using global/critic/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic/fully_connected/biases:0 is illegal; using global/critic/fully_connected/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "# Global Network\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "global_step_next = tf.assign_add(global_step, 1)\n",
    "global_ac = Network(in_size=in_size,\n",
    "                    action_size=action_space,\n",
    "                    scope=global_scope,\n",
    "                    num_agent=n_agent,\n",
    "                    num_policy_pool=num_policy_pool,\n",
    "                    sess=sess)\n",
    "\n",
    "# Local workers\n",
    "workers = []\n",
    "# loop for each workers\n",
    "for idx in range(nenv):\n",
    "    name = 'W_%i' % idx\n",
    "    print(f'worker: {name} initializing')\n",
    "    workers.append(Worker(name, global_ac, sess, global_step=global_step))\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "    \n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")\n",
    "    \n",
    "coord = tf.train.Coordinator()\n",
    "worker_threads = []\n",
    "global_episodes = sess.run(global_step)\n",
    "\n",
    "saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "\n",
    "# Summarize\n",
    "for var in tf.trainable_variables(scope=global_scope):\n",
    "    tf.summary.histogram(var.name, var)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 99601/200000 [=============>................] - ETA: 54s"
     ]
    }
   ],
   "source": [
    "for worker in workers:\n",
    "    job = lambda: worker.work(saver, writer)\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
