{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- On Policy\n",
    "\n",
    "### Sampling\n",
    "- [ ] Mini-batch to update 'average' gradient\n",
    "- [ ] Experience Replay for Random Sampling\n",
    "- [ ] Importance Sampling\n",
    "    \n",
    "### Deterministic Policy\n",
    "Gradient\n",
    "- [ ] DDPG\n",
    "- [ ] MADDPG\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [ ] Normalized Reward/Advantage\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [x] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [ ] Self-play\n",
    "- [ ] Batch Policy\n",
    "\n",
    "## Notes\n",
    "\n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf model/task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESET_GLOBAL_EPISODE=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_NAME='task'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME\n",
    "RENDER_PATH='./render/' + TRAIN_NAME\n",
    "GPU_CAPACITY=0.7 # gpu capacity in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "import signal\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.layers as layers\n",
    "from tensorflow.python.client import device_lib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.random\n",
    "import policy.roomba\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import state_processor, meta_state_processor\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import discount_rewards, store_args\n",
    "from utility.buffer import Trajectory, Replay_buffer\n",
    "\n",
    "from network.HAC_task import HAC_subcontroller as Network\n",
    "from network.HAC_task import HAC_meta_controller as Meta_Network\n",
    "\n",
    "import imageio\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "# Importing global configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "## Environment\n",
    "action_space = config.getint('DEFAULT','ACTION_SPACE')\n",
    "n_agent = 3 #config.getint('DEFAULT','NUM_AGENT')\n",
    "n_enemy = 5\n",
    "map_size = 30# config.getint('DEFAULT','MAP_SIZE')\n",
    "vision_range = config.getint('DEFAULT','VISION_RANGE')\n",
    "\n",
    "## Training\n",
    "total_episodes = 300000#config.getint('TRAINING','TOTAL_EPISODES')\n",
    "epsilon_meta = 20000\n",
    "max_ep = config.getint('TRAINING','MAX_STEP')\n",
    "critic_beta = config.getfloat('TRAINING', 'CRITIC_BETA')\n",
    "entropy_beta = config.getfloat('TRAINING', 'ENTROPY_BETA')\n",
    "gamma = config.getfloat('TRAINING', 'DISCOUNT_RATE')\n",
    "\n",
    "decay_lr = config.getboolean('TRAINING','DECAYING_LR')\n",
    "lr_a = 1e-5#config.getfloat('TRAINING','LR_ACTOR')\n",
    "lr_c = 1e-4#config.getfloat('TRAINING','LR_CRITIC')\n",
    "\n",
    "## Save/Summary\n",
    "save_network_frequency = config.getint('TRAINING','SAVE_NETWORK_FREQ')\n",
    "save_stat_frequency = config.getint('TRAINING','SAVE_STATISTICS_FREQ')\n",
    "moving_average_step = 2 * config.getint('TRAINING','MOVING_AVERAGE_SIZE')\n",
    "\n",
    "## GPU\n",
    "gpu_capacity = config.getfloat('GPU_CONFIG','GPU_CAPACITY')\n",
    "gpu_allowgrow = config.getboolean('GPU_CONFIG', 'GPU_ALLOWGROW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "# Local configuration parameters\n",
    "minibatch_size = 500\n",
    "optimization_steps = 5\n",
    "batch_size = 500\n",
    "\n",
    "# Env Settings\n",
    "vision_dx, vision_dy = 2*vision_range+1, 2*vision_range+1\n",
    "nchannel = 6\n",
    "in_size = [None,vision_dx,vision_dy,nchannel]\n",
    "shared_size = [None, 4]  # (Flag location, num allies, num enemy)\n",
    "nenv = 8  #(int) (multiprocessing.cpu_count())\n",
    "num_strategy = 3\n",
    "\n",
    "# Asynch Settings\n",
    "global_scope = 'global'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)\n",
    "    \n",
    "if not os.path.exists(RENDER_PATH):\n",
    "    os.makedirs(RENDER_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_ep_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "mean_strategy_rewards = [MA(moving_average_step) for _ in range(num_strategy)]\n",
    "global_episodes = 0\n",
    "\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    @store_args\n",
    "    def __init__(self, name, global_network, global_meta_network, sess, global_step=0):\n",
    "        # Initialize Environment worker\n",
    "        print(f'worker: {name} initiated')\n",
    "        self.env = gym.make(\"cap-v0\").unwrapped\n",
    "        self.env.num_blue_ugv = n_agent\n",
    "        self.env.num_red_ugv = n_enemy\n",
    "        self.sparse_reward = True\n",
    "        self.env.reset()\n",
    "        self.env.reset(map_size=map_size,\n",
    "                       policy_red=policy.roomba.PolicyGen(self.env.get_map, self.env.get_team_red))\n",
    "        print(f'worker: {name} environment info')\n",
    "        print(f'    number of blue agents : {len(self.env.get_team_blue)}')\n",
    "        print(f'    number of red agents  : {len(self.env.get_team_red)}')\n",
    "        \n",
    "        # Create Network for Worker\n",
    "        self.network = Network(local_state_shape=in_size,\n",
    "                               shared_state_shape=shared_size,\n",
    "                               action_size=action_space,\n",
    "                               scope=self.name, lr_actor=lr_a, lr_critic=lr_c,\n",
    "                               entropy_beta = entropy_beta, critic_beta=1.0,\n",
    "                               sess=self.sess, global_network=global_network)\n",
    "        \n",
    "        self.meta_network = Meta_Network(local_state_shape=[None, map_size, map_size, 8],\n",
    "                              shared_state_shape=[None, 4],\n",
    "                              action_size=3,\n",
    "                              scope='meta_'+self.name,\n",
    "                              sess=sess, global_network=global_meta_network)\n",
    "    def get_action(self, local_obs, shared_obs, strategies):\n",
    "        action = []\n",
    "        for local, share, strategy in zip(local_obs, shared_obs, strategies):\n",
    "            a1, _ = self.network.run_network(local[np.newaxis,:], share[np.newaxis,:], strategy)\n",
    "            action.append(a1[0])\n",
    "        return action\n",
    "        \n",
    "    def work(self, saver, writer):\n",
    "        global global_rewards, global_episodes, global_length, global_succeed\n",
    "        global mean_strategy_rewards\n",
    "        summary = tf.Summary()\n",
    "        \n",
    "        self.network.pull_global_all()\n",
    "        self.meta_network.pull_global_all()\n",
    "        \n",
    "        # loop\n",
    "        with self.sess.as_default(), self.sess.graph.as_default():\n",
    "            while not coord.should_stop() and global_episodes < total_episodes:\n",
    "                s0 = self.env.reset()\n",
    "                s_local_1, s_gps_1, _ = state_processor(s0, self.env.get_team_blue, vision_range, self.env._env,\n",
    "                                                               flatten=False, partial=False)\n",
    "                meta_obs1, meta_shared_obs1 = meta_state_processor(self.env._env, game_info=None, map_size=map_size, flatten=False, reverse=False)\n",
    "            \n",
    "                # Bootstrap\n",
    "                if epsilon_meta < global_episodes:\n",
    "                    strategy, _ = self.meta_network.run_network(meta_obs1, meta_shared_obs1)\n",
    "                else:\n",
    "                    #strategy = random.sample(range(3), n_agent)\n",
    "                    strategy = [1]*n_agent\n",
    "                a1 = self.get_action(s_local_1, s_gps_1, strategy)\n",
    "\n",
    "                is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "                indv_history = [ [] for _ in range(n_agent) ]\n",
    "                indv_meta_history = [ [] for _ in range(n_agent) ]\n",
    "                \n",
    "                prev_num_enemy = n_enemy\n",
    "                prev_env_reward = 0\n",
    "                ep_strategy_reward = [0]*num_strategy\n",
    "\n",
    "                for step in range(max_ep+1):\n",
    "                    # Iteration Reset\n",
    "                    s_local_0=s_local_1\n",
    "                    s_gps_0=s_gps_1\n",
    "                    a0 = a1\n",
    "                    was_alive = is_alive\n",
    "                    meta_obs0 = meta_obs1\n",
    "                    meta_shared_obs0 = meta_shared_obs1\n",
    "                    \n",
    "                    # Action\n",
    "                    s1, env_reward, d, info = self.env.step(a0)\n",
    "                    ep_reward = env_reward - prev_env_reward\n",
    "                    prev_env_reward = env_reward\n",
    "                    ep_reward /= 100\n",
    "                    if step == max_ep and d == False:\n",
    "                        ep_reward = -1\n",
    "                        d = True\n",
    "                        \n",
    "                    s_local_1, s_gps_1, _ = state_processor(s1, self.env.get_team_blue, vision_range, self.env._env,\n",
    "                                                               flatten=False, partial=False)\n",
    "                    meta_obs1, meta_shared_obs1 = meta_state_processor(self.env._env, game_info=info,\n",
    "                                                               map_size=map_size, flatten=False, reverse=False)\n",
    "                    \n",
    "                    # Get Next Action\n",
    "                    a1 = self.get_action(s_local_1, s_gps_1, strategy)\n",
    "                    is_alive = info['blue_alive'][-1]\n",
    "\n",
    "                    # Reward Expansion\n",
    "                    strategy_reward = [0]*3\n",
    "                    # Attack\n",
    "                    num_enemy = sum(info['red_alive'][-1])\n",
    "                    r = int(prev_num_enemy - num_enemy)\n",
    "                    prev_num_enemy = num_enemy\n",
    "                    strategy_reward[0] = r\n",
    "                    ep_strategy_reward[0] += r\n",
    "                    # Search \n",
    "                    if info['red_flag_caught'][-1]:\n",
    "                        r= 1\n",
    "                    elif d:\n",
    "                        r=-1\n",
    "                    else:\n",
    "                        r= 0\n",
    "                    strategy_reward[1] = r\n",
    "                    ep_strategy_reward[1] += r\n",
    "                    # Defend\n",
    "                    if info['blue_flag_caught'][-1]:\n",
    "                        r = -1\n",
    "                    elif d:\n",
    "                        r = 1\n",
    "                    else:\n",
    "                        r = 0\n",
    "                    strategy_reward[2] = r\n",
    "                    ep_strategy_reward[2] += r\n",
    "                    # Push to buffer\n",
    "                    for idx, agent in enumerate(self.env.get_team_blue):\n",
    "                        if was_alive[idx]:\n",
    "                            indv_history[idx].append([[s_local_0[idx], s_gps_0[idx]],\n",
    "                                                      a0[idx],\n",
    "                                                      strategy_reward[strategy[idx]]\n",
    "                                                     ])\n",
    "                            indv_meta_history[idx].append([[meta_obs0[idx], meta_shared_obs0[idx]],\n",
    "                                                      strategy[idx],\n",
    "                                                      ep_reward\n",
    "                                                     ])\n",
    "                            \n",
    "                    if d:\n",
    "                        aloss = []\n",
    "                        closs = []\n",
    "                        etrpy = []\n",
    "                        v1 = [self.network.get_critic(loc[np.newaxis,:,:,:], shr[np.newaxis,:], str_id)[0]\n",
    "                              for loc, shr, str_id in zip(s_local_1, s_gps_1, strategy)]\n",
    "                        v1_meta = [self.meta_network.get_critic(loc[np.newaxis,:], shr[np.newaxis,:])[0] for loc, shr in zip(meta_obs1, meta_shared_obs1)]\n",
    "                        for history, bootstrap, strategy_id in zip(indv_history, v1, strategy):\n",
    "                            if len(history) <= 0:\n",
    "                                continue\n",
    "                            al, cl, etr = self.process_history(history, bootstrap, strategy_id)\n",
    "                            aloss.append(al)\n",
    "                            closs.append(cl)\n",
    "                            etrpy.append(etr)\n",
    "                        for history, bootstrap in zip(indv_meta_history, v1_meta):\n",
    "                            if len(history) <= 0:\n",
    "                                continue\n",
    "                            al, cl, etr = self.process_meta_history(history, bootstrap)\n",
    "                        r_episode = 1 if self.env.blue_win else -1  # Global Reward\n",
    "                        break\n",
    "                        \n",
    "                global_rewards.append(r_episode)\n",
    "                global_length.append(step)\n",
    "                global_succeed.append(self.env.blue_win)\n",
    "                global_episodes += 1\n",
    "                self.sess.run(global_step_next)\n",
    "                progbar.update(global_episodes)\n",
    "                \n",
    "                for sid in range(num_strategy):\n",
    "                    mean_strategy_rewards[sid].append(ep_strategy_reward[sid])\n",
    "                \n",
    "                if global_episodes % save_stat_frequency == 0 and global_episodes != 0:\n",
    "                    summary.value.add(tag='Records/mean_reward', simple_value=global_rewards())\n",
    "                    summary.value.add(tag='Records/mean_length', simple_value=global_length())\n",
    "                    summary.value.add(tag='Records/mean_succeed', simple_value=global_succeed())\n",
    "                    summary.value.add(tag='summary/entropy', simple_value=np.mean(etrpy))\n",
    "                    summary.value.add(tag='summary/actor_loss', simple_value=np.mean(aloss))\n",
    "                    summary.value.add(tag='summary/critic_loss', simple_value=np.mean(closs))\n",
    "                    for sid in range(num_strategy):\n",
    "                        summary.value.add(tag=f'strategy/reward_{sid}', simple_value=mean_strategy_rewards[sid]())\n",
    "                        \n",
    "                    writer.add_summary(summary,global_episodes)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                if global_episodes % save_network_frequency == 0 and global_episodes != 0:\n",
    "                    saver.save(self.sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "                        \n",
    "    def process_history(self, buffer, bootstrap, strategy_id):\n",
    "        played_size = len(buffer)\n",
    "\n",
    "        # Extract matrix    \n",
    "        local_obs, gps_obs, action, reward = [],[],[],[]\n",
    "        for mdp in buffer:\n",
    "            local_obs.append(mdp[0][0])  # 0.0\n",
    "            gps_obs.append(mdp[0][1])    # 0.1\n",
    "            action.append(mdp[1])        # 1\n",
    "            reward.append(mdp[2])\n",
    "\n",
    "\n",
    "        # Discount Reward and Universal Advantage\n",
    "        critic = self.network.get_critic(local_obs,\n",
    "                                         gps_obs,\n",
    "                                         strategy_id)\n",
    "        value_ext = np.append(critic, bootstrap)\n",
    "        td_target = reward + gamma * value_ext[1:]\n",
    "        advantage = reward + gamma * value_ext[1:] - value_ext[:-1]\n",
    "        advantage = discount_rewards(advantage,gamma)\n",
    "\n",
    "        td_target = td_target.tolist()   # 2\n",
    "        advantage = advantage.tolist()   # 4\n",
    "\n",
    "        aloss, closs, entropy = self.train(local_obs, gps_obs, action, advantage, td_target, strategy_id)\n",
    "        return aloss, closs, entropy\n",
    "        \n",
    "    def train(self, local_obs, gps_obs, action, advantage, td_target, strategy_id):\n",
    "        aloss, closs, entropy = self.network.update_global(local_obs, gps_obs,\n",
    "                                   action, advantage, td_target, strategy_id)\n",
    "        self.network.pull_global(strategy_id)\n",
    "        return aloss, closs, entropy\n",
    "    \n",
    "    def process_meta_history(self, buffer, bootstrap):\n",
    "        played_size = len(buffer)\n",
    "\n",
    "        # Extract matrix    \n",
    "        local_obs, gps_obs, action, reward = [],[],[],[]\n",
    "        for mdp in buffer:\n",
    "            local_obs.append(mdp[0][0])  # 0.0\n",
    "            gps_obs.append(mdp[0][1])    # 0.1\n",
    "            action.append(mdp[1])        # 1\n",
    "            reward.append(mdp[2])\n",
    "\n",
    "\n",
    "        # Discount Reward and Universal Advantage\n",
    "        critic = self.meta_network.get_critic(local_obs, gps_obs)\n",
    "        value_ext = np.append(critic, bootstrap)\n",
    "        td_target = reward + gamma * value_ext[1:]\n",
    "        advantage = reward + gamma * value_ext[1:] - value_ext[:-1]\n",
    "        advantage = discount_rewards(advantage,gamma)\n",
    "\n",
    "        td_target = td_target.tolist()   # 2\n",
    "        advantage = advantage.tolist()   # 4\n",
    "\n",
    "        aloss, closs, entropy = self.train_meta(local_obs, gps_obs, action, advantage, td_target)\n",
    "        return aloss, closs, entropy\n",
    "        \n",
    "    def train_meta(self, local_obs, gps_obs, action, advantage, td_target):\n",
    "        aloss, closs, entropy = self.meta_network.update_global(local_obs, gps_obs,\n",
    "                                   action, advantage, td_target)\n",
    "        self.meta_network.pull_global_all()\n",
    "        return aloss, closs, entropy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: W_0 initializing\n",
      "worker: W_0 initiated\n",
      "worker: W_0 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namsong/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: W_1 initializing\n",
      "worker: W_1 initiated\n",
      "worker: W_1 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n",
      "worker: W_2 initializing\n",
      "worker: W_2 initiated\n",
      "worker: W_2 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n",
      "worker: W_3 initializing\n",
      "worker: W_3 initiated\n",
      "worker: W_3 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n",
      "worker: W_4 initializing\n",
      "worker: W_4 initiated\n",
      "worker: W_4 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n",
      "worker: W_5 initializing\n",
      "worker: W_5 initiated\n",
      "worker: W_5 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n",
      "worker: W_6 initializing\n",
      "worker: W_6 initiated\n",
      "worker: W_6 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n",
      "worker: W_7 initializing\n",
      "worker: W_7 initiated\n",
      "worker: W_7 environment info\n",
      "    number of blue agents : 3\n",
      "    number of red agents  : 5\n"
     ]
    }
   ],
   "source": [
    "# Global Network\n",
    "global_step = tf.Variable(0, trainable=False, name=global_scope+'/global_step')\n",
    "global_step_next = tf.assign_add(global_step, 1)\n",
    "global_network = Network(local_state_shape=in_size,\n",
    "                         shared_state_shape=shared_size,\n",
    "                         action_size=action_space,\n",
    "                         scope=global_scope,\n",
    "                         sess=sess)\n",
    "global_meta_network = Meta_Network(local_state_shape=[None, map_size, map_size, 8],\n",
    "                              shared_state_shape=[None, 4],\n",
    "                              action_size=3,\n",
    "                              scope='meta_'+global_scope,\n",
    "                              sess=sess)\n",
    "\n",
    "# Local workers\n",
    "workers = []\n",
    "# loop for each workers\n",
    "for idx in range(nenv):\n",
    "    name = 'W_%i' % idx\n",
    "    print(f'worker: {name} initializing')\n",
    "    workers.append(Worker(name, global_network, global_meta_network, sess, global_step=global_step))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare summary to only record global\n",
    "#regular_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=global_scope)\n",
    "#saver2 = tf.train.Saver(var_list = regular_vars, max_to_keep=3)\n",
    "global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=global_scope) + tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='meta_'+global_scope)\n",
    "global_vars.append(global_step)\n",
    "saver = tf.train.Saver(var_list = global_vars, max_to_keep=3)\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "\n",
    "def initialize_uninitialized_vars(sess):\n",
    "    from itertools import compress\n",
    "    global_vars = tf.global_variables()\n",
    "    is_not_initialized = sess.run([~(tf.is_variable_initialized(var)) \\\n",
    "                                   for var in global_vars])\n",
    "    not_initialized_vars = list(compress(global_vars, is_not_initialized))\n",
    "\n",
    "    if len(not_initialized_vars):\n",
    "        sess.run(tf.variables_initializer(not_initialized_vars))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Variables\n",
      "    initial save done\n",
      "INFO:tensorflow:Summary name global/policy_0/conv_0/weights:0 is illegal; using global/policy_0/conv_0/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/conv_0/biases:0 is illegal; using global/policy_0/conv_0/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/conv_1/weights:0 is illegal; using global/policy_0/conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/conv_1/biases:0 is illegal; using global/policy_0/conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/conv_2/weights:0 is illegal; using global/policy_0/conv_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/conv_2/biases:0 is illegal; using global/policy_0/conv_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/dense_0gps_proc/weights:0 is illegal; using global/policy_0/dense_0gps_proc/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/dense_0gps_proc/biases:0 is illegal; using global/policy_0/dense_0gps_proc/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/dense_1gps_proc/weights:0 is illegal; using global/policy_0/dense_1gps_proc/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/dense_1gps_proc/biases:0 is illegal; using global/policy_0/dense_1gps_proc/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/fully_connected/weights:0 is illegal; using global/policy_0/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/fully_connected/biases:0 is illegal; using global/policy_0/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/fully_connected_1/weights:0 is illegal; using global/policy_0/fully_connected_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_0/fully_connected_1/biases:0 is illegal; using global/policy_0/fully_connected_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic_0/fully_connected/weights:0 is illegal; using global/critic_0/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic_0/fully_connected/biases:0 is illegal; using global/critic_0/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/conv_0/weights:0 is illegal; using global/policy_1/conv_0/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/conv_0/biases:0 is illegal; using global/policy_1/conv_0/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/conv_1/weights:0 is illegal; using global/policy_1/conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/conv_1/biases:0 is illegal; using global/policy_1/conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/conv_2/weights:0 is illegal; using global/policy_1/conv_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/conv_2/biases:0 is illegal; using global/policy_1/conv_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/dense_0gps_proc/weights:0 is illegal; using global/policy_1/dense_0gps_proc/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/dense_0gps_proc/biases:0 is illegal; using global/policy_1/dense_0gps_proc/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/dense_1gps_proc/weights:0 is illegal; using global/policy_1/dense_1gps_proc/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/dense_1gps_proc/biases:0 is illegal; using global/policy_1/dense_1gps_proc/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/fully_connected/weights:0 is illegal; using global/policy_1/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/fully_connected/biases:0 is illegal; using global/policy_1/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/fully_connected_1/weights:0 is illegal; using global/policy_1/fully_connected_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_1/fully_connected_1/biases:0 is illegal; using global/policy_1/fully_connected_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic_1/fully_connected/weights:0 is illegal; using global/critic_1/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic_1/fully_connected/biases:0 is illegal; using global/critic_1/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/conv_0/weights:0 is illegal; using global/policy_2/conv_0/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/conv_0/biases:0 is illegal; using global/policy_2/conv_0/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/conv_1/weights:0 is illegal; using global/policy_2/conv_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/conv_1/biases:0 is illegal; using global/policy_2/conv_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/conv_2/weights:0 is illegal; using global/policy_2/conv_2/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/conv_2/biases:0 is illegal; using global/policy_2/conv_2/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/dense_0gps_proc/weights:0 is illegal; using global/policy_2/dense_0gps_proc/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/dense_0gps_proc/biases:0 is illegal; using global/policy_2/dense_0gps_proc/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/dense_1gps_proc/weights:0 is illegal; using global/policy_2/dense_1gps_proc/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/dense_1gps_proc/biases:0 is illegal; using global/policy_2/dense_1gps_proc/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/fully_connected/weights:0 is illegal; using global/policy_2/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/fully_connected/biases:0 is illegal; using global/policy_2/fully_connected/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/fully_connected_1/weights:0 is illegal; using global/policy_2/fully_connected_1/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/policy_2/fully_connected_1/biases:0 is illegal; using global/policy_2/fully_connected_1/biases_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic_2/fully_connected/weights:0 is illegal; using global/critic_2/fully_connected/weights_0 instead.\n",
      "INFO:tensorflow:Summary name global/critic_2/fully_connected/biases:0 is illegal; using global/critic_2/fully_connected/biases_0 instead.\n"
     ]
    }
   ],
   "source": [
    "# Restore Weights\n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    initialize_uninitialized_vars(sess)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")\n",
    "if RESET_GLOBAL_EPISODE:\n",
    "    sess.run(tf.assign(global_step, 0))\n",
    "    \n",
    "coord = tf.train.Coordinator()\n",
    "worker_threads = []\n",
    "global_episodes = sess.run(global_step)\n",
    "\n",
    "saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "print('    initial save done')\n",
    "\n",
    "# Summarize\n",
    "for var in tf.trainable_variables(scope=global_scope):\n",
    "    tf.summary.histogram(var.name, var)\n",
    "merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 20001/300000 [=>............................] - ETA: 84:59:08"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namsong/github/ctf_public/network/HAC_task.py:311: RuntimeWarning: invalid value encountered in less\n",
      "  return [np.random.choice(self.action_size, p=prob/sum(prob)) for prob in a_probs], critic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80516/300000 [=======>......................] - ETA: 81:00:18"
     ]
    }
   ],
   "source": [
    "for worker in workers:\n",
    "    job = lambda: worker.work(saver, writer)\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
