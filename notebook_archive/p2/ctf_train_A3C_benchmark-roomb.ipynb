{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (A3C Benchmark Training)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- On Policy\n",
    "- Self-play\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [x] Normalized Reward/Advantage\n",
    "- [ ] Target Network\n",
    "- [ ] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [x] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [x] Self-play\n",
    "- [ ] Batch Policy\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "\n",
    "## TODO:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!rm -rf logs/A3C_benchmark2/ model/A3C_benchmark2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='A3C_benchmark2'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME\n",
    "GPU_CAPACITY=0.3 # gpu capacity in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import configparser\n",
    "\n",
    "import signal\n",
    "import threading\n",
    "import multiprocessing\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.roomba\n",
    "import policy.policy_A3C\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import one_hot_encoder as one_hot_encoder\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import Experience_buffer, discount_rewards\n",
    "\n",
    "\n",
    "from network.a3c import ActorCritic as Network\n",
    "from network.base import initialize_uninitialized_vars\n",
    "\n",
    "from worker.worker import Worker\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing global configuration\n",
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "## Environment\n",
    "action_space = config.getint('DEFAULT','ACTION_SPACE')\n",
    "vision_range = 9#config.getint('DEFAULT','VISION_RANGE')\n",
    "\n",
    "moving_average_step = config.getint('TRAINING','MOVING_AVERAGE_SIZE')\n",
    "\n",
    "## GPU\n",
    "gpu_capacity = GPU_CAPACITY #config.getfloat('GPU_CONFIG','GPU_CAPACITY')\n",
    "gpu_allowgrow = config.getboolean('GPU_CONFIG', 'GPU_ALLOWGROW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Settings\n",
    "vision_dx, vision_dy = 2*vision_range+1, 2*vision_range+1\n",
    "nchannel = 6\n",
    "in_size = [None,vision_dx,vision_dy,nchannel]\n",
    "nenv = 4#(int) (multiprocessing.cpu_count())\n",
    "\n",
    "# Asynch Settings\n",
    "global_scope = 'global'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_ep_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "global_episodes = 0\n",
    "\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "progbar = tf.keras.utils.Progbar(1e6,interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Global Network\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "global_step_next = tf.assign_add(global_step, 1)\n",
    "global_network = Network(in_size=in_size, action_size=action_space, scope=global_scope, sess=sess)\n",
    "global_vars = global_network.get_vars\n",
    "global_vars.append(global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namsong/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- String Representation of Environment ----\n",
      "map objects : \n",
      "  Blue : 5 ground, 0 air\n",
      "  Red  : 5 ground, 0 air\n",
      "settings : \n",
      "  Stochastic Attack On\n",
      "  Stochastic Map On\n",
      "  Red operates under full vision\n",
      "  Dense Reward\n",
      "worker: W_0 initiated\n",
      "---- String Representation of Environment ----\n",
      "map objects : \n",
      "  Blue : 5 ground, 0 air\n",
      "  Red  : 5 ground, 0 air\n",
      "settings : \n",
      "  Stochastic Attack On\n",
      "  Stochastic Map On\n",
      "  Red operates under full vision\n",
      "  Dense Reward\n",
      "worker: W_1 initiated\n",
      "---- String Representation of Environment ----\n",
      "map objects : \n",
      "  Blue : 5 ground, 0 air\n",
      "  Red  : 5 ground, 0 air\n",
      "settings : \n",
      "  Stochastic Attack On\n",
      "  Stochastic Map On\n",
      "  Red operates under full vision\n",
      "  Dense Reward\n",
      "worker: W_2 initiated\n",
      "---- String Representation of Environment ----\n",
      "map objects : \n",
      "  Blue : 5 ground, 0 air\n",
      "  Red  : 5 ground, 0 air\n",
      "settings : \n",
      "  Stochastic Attack On\n",
      "  Stochastic Map On\n",
      "  Red operates under full vision\n",
      "  Dense Reward\n",
      "worker: W_3 initiated\n",
      "INFO:tensorflow:Restoring parameters from ./model/A3C_benchmark2/ctf_policy.ckpt-199003\n",
      "Load Model :  ./model/A3C_benchmark2/ctf_policy.ckpt-199003\n",
      "0 number of non-initialized variables found.\n",
      "Initialized uninitialized variables: Done\n",
      "    initial save done\n",
      " 199004/1000000 [====>.........................] - ETA: 46spath find: ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "path exist : ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "path find: ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "path exist : ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      " 199006/1000000 [====>.........................] - ETA: 52spath find: ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "path exist : ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      " 199007/1000000 [====>.........................] - ETA: 1:06path find: ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "path exist : ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "INFO:tensorflow:Restoring parameters from ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "INFO:tensorflow:Restoring parameters from ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "INFO:tensorflow:Restoring parameters from ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "Graph is succesfully loaded. ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      " 199008/1000000 [====>.........................] - ETA: 1:56Graph is succesfully loaded. ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "Graph is succesfully loaded. ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "INFO:tensorflow:Restoring parameters from ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      "Graph is succesfully loaded. ./model/A3C_benchmark/ctf_policy.ckpt-129001\n",
      " 199114/1000000 [====>.........................] - ETA: 4:24"
     ]
    }
   ],
   "source": [
    "# Local workers\n",
    "workers = []\n",
    "# loop for each workers\n",
    "for idx in range(nenv):\n",
    "    name = 'W_%i' % idx\n",
    "    workers.append(Worker(name, global_network, sess,\n",
    "                 global_episodes=global_step, increment_step_op=global_step_next,\n",
    "                 progbar=progbar, selfplay=False))\n",
    "    print(f'worker: {name} initiated')\n",
    "\n",
    "#saver = tf.train.Saver(var_list=global_vars, max_to_keep=3)\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "    \n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "    initialize_uninitialized_vars(sess)\n",
    "    print(\"Initialized uninitialized variables: Done\")\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")\n",
    "    \n",
    "coord = tf.train.Coordinator()\n",
    "worker_threads = []\n",
    "global_episodes = sess.run(global_step)\n",
    "\n",
    "saver.save(sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "print('    initial save done')\n",
    "\n",
    "recorder = {'reward':global_rewards,\n",
    "            'length':global_length,\n",
    "            'succeed':global_succeed}\n",
    "\n",
    "for worker in workers:\n",
    "    job = lambda: worker.work(saver, writer, coord, recorder, MODEL_PATH)\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
