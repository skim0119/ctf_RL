{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capture the Flag (RL - Policy Gradient)\n",
    "\n",
    "- Seung Hyun Kim\n",
    "- skim449@illinois.edu\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "- Actor-critic\n",
    "- Experience Replay + Retrace\n",
    "- TRPO\n",
    "- Off Policy\n",
    "- Asynchronous\n",
    "- Hindsight Experience Replay\n",
    "    - Goal Replay\n",
    "    - Action Replay\n",
    "\n",
    "### Sampling\n",
    "- [ ] Mini-batch to update 'average' gradient\n",
    "- [x] Experience Replay\n",
    "- [x] Importance Sampling\n",
    "    - [x] Retrace\n",
    "\n",
    "### Stability and Reducing Variance\n",
    "- [x] Gradient clipping\n",
    "- [x] Retrace\n",
    "- [ ] Normalized Reward/Advantage\n",
    "- [ ] Target Network\n",
    "- [x] TRPO\n",
    "- [ ] PPO\n",
    "\n",
    "### Multiprocessing\n",
    "- [ ] Synchronous Training (A2C)\n",
    "- [x] Asynchronous Training (A3C)\n",
    "\n",
    "### Applied Training Methods:\n",
    "- [ ] Self-play\n",
    "\n",
    "## Notes\n",
    "\n",
    "- This notebook includes:\n",
    "    - Building the structure of policy driven network.\n",
    "    - Training with/without render\n",
    "    - Saver that save model and weights to ./model directory\n",
    "    - Writer that will record some necessary datas to ./logs\n",
    "\n",
    "- This notebook does not include:\n",
    "    - Simulation with RL policy\n",
    "        - The simulation can be done using policy_RL.py\n",
    "    - cap_test.py is changed appropriately.\n",
    "    \n",
    "## References :\n",
    "- https://github.com/awjuliani/DeepRL-Agents/blob/master/Vanilla-Policy.ipynb (source)\n",
    "- https://www.youtube.com/watch?v=PDbXPBwOavc\n",
    "- https://github.com/lilianweng/deep-reinforcement-learning-gym/blob/master/playground/policies/actor_critic.py (source)\n",
    "- https://github.com/spro/practical-pytorch/blob/master/reinforce-gridworld/reinforce-gridworld.ipynb\n",
    "- https://arxiv.org/pdf/1611.01224.pdf\n",
    "- http://papers.nips.cc/paper/6538-safe-and-efficient-off-policy-reinforcement-learning.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "- [x] Check if simple off-policy VANILLA trains\n",
    "- [x] Build full ACER with asynch:\n",
    "    - [x] ER\n",
    "    - [ ] TRPO/PPO (optional)\n",
    "    - [ ] Retrace (Important Sampling)\n",
    "        - [ ] Q value retrace\n",
    "        - [ ] Truncated Importance Sampling\n",
    "    - [x] Asynchronous\n",
    "- [x] Modify and make universal MDP (include g)\n",
    "- [x] Modify environment pipeline\n",
    "- [x] Modify existing ACER network for Universal\n",
    "- [x] Implement HER algorithm\n",
    "    - [x] goal replay\n",
    "    - [x] action replay\n",
    "        - Disabled: The entire replay buffer is used as goal buffer\n",
    "        - The goal is sampled from replay buffer\n",
    "    - Some improvisation: multi-agent takes 2 global goal and 2 sampled goal\n",
    "- [x] Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs/HER_A3CER_v4/ model/HER_A3CER_v4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME='HER_A3CER_v4'\n",
    "LOG_PATH='./logs/'+TRAIN_NAME\n",
    "MODEL_PATH='./model/' + TRAIN_NAME\n",
    "GPU_CAPACITY=0.5 # gpu capacity in percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "import threading\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "import gym\n",
    "import gym_cap\n",
    "import gym_cap.envs.const as CONST\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# the modules that you can use to generate the policy. \n",
    "import policy.random\n",
    "import policy.roomba\n",
    "import policy.policy_RL\n",
    "import policy.zeros\n",
    "\n",
    "# Data Processing Module\n",
    "from utility.dataModule import state_processor\n",
    "from utility.utils import MovingAverage as MA\n",
    "from utility.utils import discount_rewards, store_args, q_retrace\n",
    "from utility.her import HER\n",
    "\n",
    "from network.u_ACER import UACER as Network\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Variables\n",
    "total_episodes= 200000\n",
    "max_ep = 150\n",
    "entropy_beta = 0.001\n",
    "trajectory_length = 150\n",
    "train_frequency = 1  # not yet implemented\n",
    "minibatch_size = 200\n",
    "action_replay_count = 1\n",
    "\n",
    "# Saving Related\n",
    "save_network_frequency = 1200\n",
    "save_stat_frequency = 128\n",
    "moving_average_step = 128\n",
    "\n",
    "# Training Variables\n",
    "lr_a = 1e-5\n",
    "lr_c = 5e-5\n",
    "\n",
    "gamma = 0.98 # discount_factor\n",
    "\n",
    "# Env Settings\n",
    "MAP_SIZE = 50\n",
    "VISION_RANGE = 19 # What decide the network size !!!\n",
    "VISION_dX, VISION_dY = 2*VISION_RANGE+1, 2*VISION_RANGE+1\n",
    "IN_SIZE = [None,VISION_dX,VISION_dY,6]\n",
    "GPS_SIZE = [None, 2]\n",
    "ACTION_SPACE = 5\n",
    "N_AGENT = 1\n",
    "NENV = 8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)\n",
    "    \n",
    "#Create a directory to save episode playback gifs to\n",
    "if not os.path.exists(LOG_PATH):\n",
    "    os.makedirs(LOG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_rewards = MA(moving_average_step)\n",
    "global_length = MA(moving_average_step)\n",
    "global_succeed = MA(moving_average_step)\n",
    "global_episodes = 0\n",
    "\n",
    "# Launch the session\n",
    "gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)\n",
    "\n",
    "sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "progbar = tf.keras.utils.Progbar(total_episodes,interval=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(object):\n",
    "    @store_args\n",
    "    def __init__(self, name, global_network, sess, global_step=0):\n",
    "        # Initialize Environment worker\n",
    "        print(f'worker: {name} initiated')\n",
    "        self.env = gym.make(\"cap-v0\").unwrapped\n",
    "        self.env.reset(map_size=MAP_SIZE,\n",
    "                       policy_red=policy.zeros.PolicyGen(self.env.get_map, self.env.get_team_red))\n",
    "        print(f'worker: {name} environment info')\n",
    "        print(f'    number of blue agents : {len(self.env.get_team_blue)}')\n",
    "        print(f'    number of red agents  : {len(self.env.get_team_red)}')\n",
    "        \n",
    "        # Create Network for Worker\n",
    "        self.network = Network(in_size=IN_SIZE, gps_size=GPS_SIZE, action_size=ACTION_SPACE,\n",
    "                               scope=self.name, lr_actor=lr_a, lr_critic=lr_c,\n",
    "                               grad_clip_norm=0, entropy_beta = entropy_beta,\n",
    "                               sess=sess, global_network=global_network)\n",
    "        \n",
    "        # Create HER Module\n",
    "        self.her = HER(depth=6)\n",
    "        \n",
    "    def work(self, saver, writer):\n",
    "        global global_rewards, global_episodes, global_length, global_succeed\n",
    "                \n",
    "        # loop\n",
    "        with self.sess.as_default(), self.sess.graph.as_default():\n",
    "            while not coord.should_stop() and global_episodes < total_episodes:\n",
    "                s0 = self.env.reset()\n",
    "                s_local_0, s_gps_0, g_global = state_processor(s0, self.env.get_team_blue, VISION_RANGE, self.env._env,\n",
    "                                                               flatten=False, partial=False)\n",
    "                self.goal = g_global[:]#2]+self.her.sample_goal(2, g_global[0])\n",
    "                \n",
    "                # Bootstrap\n",
    "                a1, pi1 = self.network.get_action(s_local_0, s_gps_0, self.goal)\n",
    "                is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "                indv_history = [ [] for _ in range(len(self.env.get_team_blue)) ]\n",
    "                for step in range(max_ep+1):\n",
    "                    # Transition\n",
    "                    a, pi0 = a1, pi1\n",
    "                    was_alive = is_alive\n",
    "                    \n",
    "                    # Action\n",
    "                    s1, _, d, _ = self.env.step(a)\n",
    "                    s_local_1, s_gps_1, _ = state_processor(s1, self.env.get_team_blue, VISION_RANGE, self.env._env,\n",
    "                                                            flatten=False, partial=False)\n",
    "                    reward = [self.her.reward(s,a,g) for s,a,g in zip(s_gps_1, a, self.goal)]\n",
    "                    a1, pi1 = self.network.get_action(s_local_1, s_gps_1, self.goal)\n",
    "                    is_alive = [ag.isAlive for ag in self.env.get_team_blue]\n",
    "                    \n",
    "                    if step == max_ep and d == False:\n",
    "                        d = True\n",
    "\n",
    "                    # push to buffer\n",
    "                    for idx, agent in enumerate(self.env.get_team_blue):\n",
    "                        if was_alive[idx]:\n",
    "                            indv_history[idx].append([[s_local_0[idx], s_gps_0[idx]],\n",
    "                                                      a[idx],\n",
    "                                                      [s_local_1[idx], s_gps_1[idx]],\n",
    "                                                      pi0[idx][a[idx]],\n",
    "                                                      reward[idx]\n",
    "                                                     ])\n",
    "                        # if not is_alive[idx]:\n",
    "                        #    self.her.action_replay(s_gps_1[idx])\n",
    "                            \n",
    "                    if (step % trajectory_length == 0 and step > 0) or d:\n",
    "                        self.process_history(indv_history)\n",
    "                        indv_history = [[] for _ in self.env.get_team_blue]\n",
    "                        \n",
    "                    # Iteration Reset\n",
    "                    s_local_0=s_local_1\n",
    "                    s_gps_0=s_gps_1\n",
    "\n",
    "                    if d:\n",
    "                        r_episode = 1 if self.env.blue_win else -1\n",
    "                        aloss, closs, etrpy = self.train()\n",
    "                        break\n",
    "\n",
    "                global_rewards.append(r_episode)\n",
    "                global_length.append(step)\n",
    "                global_succeed.append(self.env.blue_win)\n",
    "                global_episodes += 1\n",
    "                self.sess.run(global_step_next)\n",
    "                progbar.update(global_episodes)\n",
    "                \n",
    "                if global_episodes % save_stat_frequency == 0 and global_episodes != 0:\n",
    "                    summary = tf.Summary()\n",
    "                    summary.value.add(tag='Records/mean_reward', simple_value=global_rewards())\n",
    "                    summary.value.add(tag='Records/mean_length', simple_value=global_length())\n",
    "                    summary.value.add(tag='Records/mean_succeed', simple_value=global_succeed())\n",
    "                    summary.value.add(tag='summary/Entropy', simple_value=etrpy)\n",
    "                    summary.value.add(tag='summary/actor_loss', simple_value=aloss)\n",
    "                    summary.value.add(tag='summary/critic_loss', simple_value=closs)\n",
    "                    writer.add_summary(summary,global_episodes)\n",
    "                    writer.flush()\n",
    "                    \n",
    "                if global_episodes % save_network_frequency == 0 and global_episodes != 0:\n",
    "                    saver.save(self.sess, MODEL_PATH+'/ctf_policy.ckpt', global_step=global_episodes)\n",
    "                        \n",
    "    def process_history(self, indv_buffer):\n",
    "        for idx, buffer in enumerate(indv_buffer):\n",
    "            played_size = len(buffer)\n",
    "            if played_size == 0:\n",
    "                continue\n",
    "                \n",
    "            # Extract matrix    \n",
    "            local_obs, gps_obs, action, local_obs_1, gps_obs_1, beta_policy = [],[],[],[],[],[]\n",
    "            for mdp in buffer:\n",
    "                local_obs.append(mdp[0][0])  # 0.0\n",
    "                gps_obs.append(mdp[0][1])    # 0.1\n",
    "                action.append(mdp[1])        # 1\n",
    "                local_obs_1.append(mdp[2][0])\n",
    "                gps_obs_1.append(mdp[2][1])\n",
    "                beta_policy.append(mdp[3])   # 5\n",
    "                \n",
    "            goal = self.goal[idx]            # 3\n",
    "            # Goal Replay\n",
    "            #     global_goal + sampled_goal\n",
    "            goal_list = [goal] + random.sample(gps_obs_1, min(action_replay_count,len(gps_obs_1)))\n",
    "            for subgoal in goal_list:\n",
    "                # Discount Reward and Universal Advantage\n",
    "                reward, length = self.her.goal_replay(gps_obs_1, action, subgoal)\n",
    "                critic = self.network.get_critic(local_obs[:length],\n",
    "                                                 gps_obs[:length],\n",
    "                                                 [subgoal]*length)\n",
    "                bootstrap = self.network.get_critic(local_obs_1[length-1:length],\n",
    "                                                    gps_obs_1[length-1:length],\n",
    "                                                    [subgoal])\n",
    "                # Importance Sampling weight\n",
    "                _, action_probs = self.network.get_action(local_obs[:length], gps_obs[:length], [subgoal]*length)\n",
    "                pi_i = np.array([prob[a] for a, prob in zip(action, action_probs)])\n",
    "                is_weight = pi_i / beta_policy[:length]\n",
    "                \n",
    "                value_ext = np.append(critic, [bootstrap])\n",
    "                #td_target = reward + gamma * value_ext[1:]\n",
    "                q_ret = q_retrace(reward, value_ext, gamma, is_weight)\n",
    "                advantage = q_ret - value_ext[:-1]\n",
    "                advantage = discount_rewards(advantage,gamma)\n",
    "\n",
    "                td_target = q_ret[:]#.tolist()   # 2\n",
    "                advantage = advantage.tolist()   # 4\n",
    "\n",
    "                for i in range(length):\n",
    "                    transition = [[local_obs[i], gps_obs[i]],\n",
    "                                   action[i], td_target[i], subgoal, advantage[i], is_weight[i]]\n",
    "                    self.her.store_transition(transition)\n",
    "            # Action Replay\n",
    "            self.her.action_replay(gps_obs_1[-1])\n",
    "        \n",
    "    def train(self):\n",
    "        aloss, closs, entropy = [],[],[]\n",
    "        #print(f'Buffer Size : {len(self.her.replay_buffer.buffer)}')\n",
    "        while not self.her.buffer_empty():\n",
    "            minibatch = self.her.sample_minibatch(minibatch_size)\n",
    "            local_obs, gps_obs, action, advantage, goal, td_target, is_weight = [],[],[],[],[],[],[]\n",
    "            for tr in minibatch:\n",
    "                local_obs.append(tr[0][0])\n",
    "                gps_obs.append(tr[0][1])\n",
    "                action.append(tr[1])\n",
    "                td_target.append(tr[2])\n",
    "                goal.append(tr[3])\n",
    "                advantage.append(tr[4])\n",
    "                is_weight.append(tr[5])\n",
    "            al, cl, entr = self.network.update_global(local_obs, gps_obs, goal,\n",
    "                                       action, advantage, td_target, is_weight)\n",
    "            aloss.append(al)\n",
    "            closs.append(cl)\n",
    "            entropy.append(entr)\n",
    "            \n",
    "        self.network.pull_global()\n",
    "        return np.mean(aloss), np.mean(closs), np.mean(entropy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: W_0 initiated\n",
      "worker: W_0 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namsong/anaconda3/envs/py36/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "worker: W_0 initiated\n",
      "worker: W_1 initiated\n",
      "worker: W_1 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n",
      "worker: W_1 initiated\n",
      "worker: W_2 initiated\n",
      "worker: W_2 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n",
      "worker: W_2 initiated\n",
      "worker: W_3 initiated\n",
      "worker: W_3 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n",
      "worker: W_3 initiated\n",
      "worker: W_4 initiated\n",
      "worker: W_4 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n",
      "worker: W_4 initiated\n",
      "worker: W_5 initiated\n",
      "worker: W_5 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n",
      "worker: W_5 initiated\n",
      "worker: W_6 initiated\n",
      "worker: W_6 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n",
      "worker: W_6 initiated\n",
      "worker: W_7 initiated\n",
      "worker: W_7 environment info\n",
      "    number of blue agents : 1\n",
      "    number of red agents  : 0\n",
      "worker: W_7 initiated\n",
      "Initialized Variables\n",
      " 64890/200000 [========>.....................] - ETA: 19:45:07"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/namsong/github/ctf_public/network/u_ACER.py:161: RuntimeWarning: invalid value encountered in less\n",
      "  action = [np.random.choice(self.action_size, p=prob / sum(prob)) for prob in a_probs]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "170218/200000 [========================>.....] - ETA: 4:42:54170128/200000 [============"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-bf2488f17efd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mworker_threads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworker_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, threads, stop_grace_period_secs, ignore_live_threads)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;31m# Wait for all threads to stop or for request_stop() to be called.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m     \u001b[0;32mwhile\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/tensorflow/python/training/coordinator.py\u001b[0m in \u001b[0;36mwait_for_stop\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    309\u001b[0m       \u001b[0;32mTrue\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCoordinator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mtold\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0mexpired\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \"\"\"\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mregister_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "global_step_next = tf.assign_add(global_step, 1)\n",
    "global_weights = Network(in_size=IN_SIZE, gps_size=GPS_SIZE, action_size=ACTION_SPACE,\n",
    "                         scope='global', sess=sess,\n",
    "                         set_global=True)\n",
    "\n",
    "# Local workers\n",
    "workers = []\n",
    "# loop for each workers\n",
    "for idx in range(NENV):\n",
    "    name = 'W_%i' % idx\n",
    "    workers.append(Worker(name, global_weights, sess, global_step=global_step))\n",
    "    print(f'worker: {name} initiated')\n",
    "saver = tf.train.Saver(max_to_keep=3)\n",
    "writer = tf.summary.FileWriter(LOG_PATH, sess.graph)\n",
    "    \n",
    "ckpt = tf.train.get_checkpoint_state(MODEL_PATH)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    print(\"Load Model : \", ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    print(\"Initialized Variables\")\n",
    "    \n",
    "coord = tf.train.Coordinator()\n",
    "worker_threads = []\n",
    "global_episodes = sess.run(global_step)\n",
    "\n",
    "for worker in workers:\n",
    "    job = lambda: worker.work(saver, writer)\n",
    "    t = threading.Thread(target=job)\n",
    "    t.start()\n",
    "    worker_threads.append(t)\n",
    "coord.join(worker_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
