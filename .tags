!_TAG_FILE_FORMAT	2	/extended format; --format=1 will not append ;" to lines/
!_TAG_FILE_SORTED	0	/0=unsorted, 1=sorted, 2=foldcase/
!_TAG_PROGRAM_AUTHOR	Darren Hiebert	/dhiebert@users.sourceforge.net/
!_TAG_PROGRAM_NAME	Exuberant Ctags	//
!_TAG_PROGRAM_URL	http://ctags.sourceforge.net	/official site/
!_TAG_PROGRAM_VERSION	5.9~svn20110310	//
Network	/home/neale/ctf_RL/method/SF.py	/^class Network:$/;"	class	line:9
__init__	/home/neale/ctf_RL/method/SF.py	/^    def __init__($/;"	member	line:11	class:Network
run_network	/home/neale/ctf_RL/method/SF.py	/^    def run_network(self, states, return_action=True):$/;"	member	line:57	class:Network
update_network	/home/neale/ctf_RL/method/SF.py	/^    def update_network(self, state_input, action, td_target, advantage, old_logit, state_next, reward, global_episodes, writer=None, log=False):$/;"	member	line:69	class:Network
get_successor_feature_gradcam	/home/neale/ctf_RL/method/SF.py	/^    def get_successor_feature_gradcam(self):$/;"	member	line:107	class:Network
get_successor_weight	/home/neale/ctf_RL/method/SF.py	/^    def get_successor_weight(self):$/;"	member	line:115	class:Network
initialize_vars	/home/neale/ctf_RL/method/SF.py	/^    def initialize_vars(self):$/;"	member	line:118	class:Network
initiate	/home/neale/ctf_RL/method/SF.py	/^    def initiate(self, saver, model_path):$/;"	member	line:123	class:Network
save	/home/neale/ctf_RL/method/SF.py	/^    def save(self, saver, model_path, global_step):$/;"	member	line:134	class:Network
load	/home/neale/ctf_RL/method/SF.py	/^    def load(self, model_path):$/;"	member	line:137	class:Network
get_vars	/home/neale/ctf_RL/method/SF.py	/^    def get_vars(self):$/;"	member	line:145	class:Network
SF_CVDC	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^class SF_CVDC:$/;"	class	line:16
__init__	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def __init__($/;"	member	line:20	class:SF_CVDC
log	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def log(self, step, log_weights=True, logs=None):$/;"	member	line:85	class:SF_CVDC
run_network_central	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def run_network_central(self, states):$/;"	member	line:113	class:SF_CVDC
run_network_decentral	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def run_network_decentral(self, states_list):$/;"	member	line:119	class:SF_CVDC
update_central	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def update_central(self, datasets, writer=None, log=False, step=None, tag=None):$/;"	member	line:133	class:SF_CVDC
update_decentral	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def update_decentral(self, datasets, writer=None, log=False, step=None, tag=None, log_image=False):$/;"	member	line:155	class:SF_CVDC
initiate	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def initiate(self, verbose=1):$/;"	member	line:220	class:SF_CVDC
restore	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def restore(self):$/;"	member	line:235	class:SF_CVDC
save	/home/neale/ctf_RL/method/CVDC_sharedSF.py	/^    def save(self, checkpoint_number):$/;"	member	line:242	class:SF_CVDC
PPO	/home/neale/ctf_RL/method/ppo_lstm.py	/^class PPO:$/;"	class	line:9
__init__	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def __init__($/;"	member	line:11	class:PPO
hidden_init	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def hidden_init(self, batch_size):$/;"	member	line:49	class:PPO
run_network	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def run_network(self, states, return_action=True):$/;"	member	line:52	class:PPO
update_network	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def update_network(self, states, action, td_target, advantage, old_logit, prev_actions, prev_rewards, hhs, hcs, global_episodes, writer=None, log=False):$/;"	member	line:68	class:PPO
initialize_vars	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def initialize_vars(self):$/;"	member	line:104	class:PPO
initiate	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def initiate(self, saver, model_path):$/;"	member	line:109	class:PPO
save	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def save(self, saver, model_path, global_step):$/;"	member	line:120	class:PPO
get_vars	/home/neale/ctf_RL/method/ppo_lstm.py	/^    def get_vars(self):$/;"	member	line:124	class:PPO
VDN_Module	/home/neale/ctf_RL/method/VDN.py	/^class VDN_Module:$/;"	class	line:14
__init__	/home/neale/ctf_RL/method/VDN.py	/^    def __init__($/;"	member	line:16	class:VDN_Module
run_network	/home/neale/ctf_RL/method/VDN.py	/^    def run_network(self, states_list):$/;"	member	line:47	class:VDN_Module
update_network	/home/neale/ctf_RL/method/VDN.py	/^    def update_network(self, datasets_critic, agent_type_index, log=False, writer=None, step=None, tag=None):$/;"	member	line:54	class:VDN_Module
initiate	/home/neale/ctf_RL/method/VDN.py	/^    def initiate(self, verbose=1):$/;"	member	line:74	class:VDN_Module
restore	/home/neale/ctf_RL/method/VDN.py	/^    def restore(self):$/;"	member	line:85	class:VDN_Module
save	/home/neale/ctf_RL/method/VDN.py	/^    def save(self, checkpoint_number):$/;"	member	line:89	class:VDN_Module
SF_CVDC	/home/neale/ctf_RL/method/CVDC.py	/^class SF_CVDC:$/;"	class	line:16
__init__	/home/neale/ctf_RL/method/CVDC.py	/^    def __init__($/;"	member	line:20	class:SF_CVDC
log	/home/neale/ctf_RL/method/CVDC.py	/^    def log(self, step, log_weights=True, logs=None):$/;"	member	line:89	class:SF_CVDC
run_network_central	/home/neale/ctf_RL/method/CVDC.py	/^    def run_network_central(self, states):$/;"	member	line:117	class:SF_CVDC
run_network_decentral	/home/neale/ctf_RL/method/CVDC.py	/^    def run_network_decentral(self, states_list):$/;"	member	line:123	class:SF_CVDC
update_central	/home/neale/ctf_RL/method/CVDC.py	/^    def update_central(self, datasets, writer=None, log=False, step=None, tag=None):$/;"	member	line:134	class:SF_CVDC
update_decentral	/home/neale/ctf_RL/method/CVDC.py	/^    def update_decentral(self, datasets, writer=None, log=False, step=None, tag=None):$/;"	member	line:156	class:SF_CVDC
initiate	/home/neale/ctf_RL/method/CVDC.py	/^    def initiate(self, verbose=1):$/;"	member	line:206	class:SF_CVDC
restore	/home/neale/ctf_RL/method/CVDC.py	/^    def restore(self):$/;"	member	line:221	class:SF_CVDC
save	/home/neale/ctf_RL/method/CVDC.py	/^    def save(self, checkpoint_number):$/;"	member	line:228	class:SF_CVDC
REINFORCE	/home/neale/ctf_RL/method/REINFORCE.py	/^class REINFORCE:$/;"	class	line:9
__init__	/home/neale/ctf_RL/method/REINFORCE.py	/^    def __init__(self,$/;"	member	line:30	class:REINFORCE
_build_placeholders	/home/neale/ctf_RL/method/REINFORCE.py	/^    def _build_placeholders(self):$/;"	member	line:61	class:REINFORCE
_build_network	/home/neale/ctf_RL/method/REINFORCE.py	/^    def _build_network(self):$/;"	member	line:71	class:REINFORCE
_build_loss	/home/neale/ctf_RL/method/REINFORCE.py	/^    def _build_loss(self):$/;"	member	line:101	class:REINFORCE
_build_optimizer	/home/neale/ctf_RL/method/REINFORCE.py	/^    def _build_optimizer(self):$/;"	member	line:110	class:REINFORCE
_build_summary	/home/neale/ctf_RL/method/REINFORCE.py	/^    def _build_summary(self):$/;"	member	line:126	class:REINFORCE
get_action	/home/neale/ctf_RL/method/REINFORCE.py	/^    def get_action(self, states, deterministic=False):$/;"	member	line:149	class:REINFORCE
gradient_clear	/home/neale/ctf_RL/method/REINFORCE.py	/^    def gradient_clear(self):$/;"	member	line:156	class:REINFORCE
gradient_accumulate	/home/neale/ctf_RL/method/REINFORCE.py	/^    def gradient_accumulate(self, states, rewards, actions):$/;"	member	line:160	class:REINFORCE
update_network_batch	/home/neale/ctf_RL/method/REINFORCE.py	/^    def update_network_batch(self):$/;"	member	line:167	class:REINFORCE
update_network	/home/neale/ctf_RL/method/REINFORCE.py	/^    def update_network(self, states, rewards, actions):$/;"	member	line:171	class:REINFORCE
initialize_uninitialized_vars	/home/neale/ctf_RL/method/base.py	/^def initialize_uninitialized_vars(sess, global_vars=None):$/;"	function	line:18
put_ctf_state_on_grid	/home/neale/ctf_RL/method/base.py	/^def put_ctf_state_on_grid(images, pad=1):$/;"	function	line:39
put_kernels_on_grid	/home/neale/ctf_RL/method/base.py	/^def put_kernels_on_grid (kernel, grid_Y, grid_X, pad = 1):$/;"	function	line:57
put_channels_on_grid	/home/neale/ctf_RL/method/base.py	/^def put_channels_on_grid (image, grid_Y, grid_X, pad = 1):$/;"	function	line:113
put_flat_on_grid	/home/neale/ctf_RL/method/base.py	/^def put_flat_on_grid (image, grid_Y, grid_X, pad=1):$/;"	function	line:128
Deep_layer	/home/neale/ctf_RL/method/base.py	/^class Deep_layer:$/;"	class	line:133
conv2d_pool	/home/neale/ctf_RL/method/base.py	/^    def conv2d_pool(input_layer, channels, kernels, pools=None, strides=None,$/;"	member	line:135	class:Deep_layer
fc	/home/neale/ctf_RL/method/base.py	/^    def fc(input_layer, hidden_layers, dropout=1.0,$/;"	member	line:179	class:Deep_layer
Tensor_logger	/home/neale/ctf_RL/method/base.py	/^class Tensor_logger:$/;"	class	line:194
__init__	/home/neale/ctf_RL/method/base.py	/^    def __init__(self, log_path, summary_name, sess, writer_id='0'):$/;"	member	line:196	class:Tensor_logger
log_scalar	/home/neale/ctf_RL/method/base.py	/^    def log_scalar(self, tag, value, step):$/;"	member	line:200	class:Tensor_logger
set_histograms	/home/neale/ctf_RL/method/base.py	/^    def set_histograms(self, var_list):$/;"	member	line:215	class:Tensor_logger
Tensorboard_utility	/home/neale/ctf_RL/method/base.py	/^class Tensorboard_utility:$/;"	class	line:222
scalar_logger	/home/neale/ctf_RL/method/base.py	/^    def scalar_logger(tag, value, step, writer):$/;"	member	line:224	class:Tensorboard_utility
histogram_logger	/home/neale/ctf_RL/method/base.py	/^    def histogram_logger(summary, step, writer):$/;"	member	line:242	class:Tensorboard_utility
variable_statistic_logger	/home/neale/ctf_RL/method/base.py	/^    def variable_statistic_logger($/;"	member	line:246	class:Tensorboard_utility
Custom_initializers	/home/neale/ctf_RL/method/base.py	/^class Custom_initializers:$/;"	class	line:290
normalized_columns_initializer	/home/neale/ctf_RL/method/base.py	/^    def normalized_columns_initializer(std=1.0):$/;"	member	line:292	class:Custom_initializers
_initializer	/home/neale/ctf_RL/method/base.py	/^        def _initializer(shape, dtype=None, partition_info=None):$/;"	function	line:293	function:Custom_initializers.normalized_columns_initializer
log_uniform_initializer	/home/neale/ctf_RL/method/base.py	/^    def log_uniform_initializer(mu, std):$/;"	member	line:300	class:Custom_initializers
_initializer	/home/neale/ctf_RL/method/base.py	/^        def _initializer(shape, dtype=None, partition_info=None):$/;"	function	line:301	function:Custom_initializers.log_uniform_initializer
variance_scaling	/home/neale/ctf_RL/method/base.py	/^    def variance_scaling():$/;"	member	line:307	class:Custom_initializers
PPO_Module	/home/neale/ctf_RL/method/ActorCritic.py	/^class PPO_Module:$/;"	class	line:13
__init__	/home/neale/ctf_RL/method/ActorCritic.py	/^    def __init__($/;"	member	line:15	class:PPO_Module
run_network	/home/neale/ctf_RL/method/ActorCritic.py	/^    def run_network(self, states_list):$/;"	member	line:46	class:PPO_Module
update_network	/home/neale/ctf_RL/method/ActorCritic.py	/^    def update_network(self, train_datasets, log=False, writer=None, step=None, tag=None):$/;"	member	line:54	class:PPO_Module
initiate	/home/neale/ctf_RL/method/ActorCritic.py	/^    def initiate(self, verbose=1):$/;"	member	line:80	class:PPO_Module
restore	/home/neale/ctf_RL/method/ActorCritic.py	/^    def restore(self):$/;"	member	line:91	class:PPO_Module
save	/home/neale/ctf_RL/method/ActorCritic.py	/^    def save(self, checkpoint_number):$/;"	member	line:95	class:PPO_Module
PPO_Module_SC2	/home/neale/ctf_RL/method/ActorCritic.py	/^class PPO_Module_SC2(PPO_Module):$/;"	class	line:99
__init__	/home/neale/ctf_RL/method/ActorCritic.py	/^    def __init__($/;"	member	line:101	class:PPO_Module_SC2
run_network	/home/neale/ctf_RL/method/ActorCritic.py	/^    def run_network(self, states_list,validActions_list):$/;"	member	line:132	class:PPO_Module_SC2
SF_CVDC	/home/neale/ctf_RL/method/CVDC2.py	/^class SF_CVDC:$/;"	class	line:16
__init__	/home/neale/ctf_RL/method/CVDC2.py	/^    def __init__($/;"	member	line:20	class:SF_CVDC
log	/home/neale/ctf_RL/method/CVDC2.py	/^    def log(self, step, log_weights=True, logs=None):$/;"	member	line:85	class:SF_CVDC
run_network_central	/home/neale/ctf_RL/method/CVDC2.py	/^    def run_network_central(self, states):$/;"	member	line:113	class:SF_CVDC
run_network_decentral	/home/neale/ctf_RL/method/CVDC2.py	/^    def run_network_decentral(self, states_list):$/;"	member	line:119	class:SF_CVDC
update_central	/home/neale/ctf_RL/method/CVDC2.py	/^    def update_central(self, datasets, writer=None, log=False, step=None, tag=None):$/;"	member	line:133	class:SF_CVDC
update_decentral	/home/neale/ctf_RL/method/CVDC2.py	/^    def update_decentral(self, datasets, writer=None, log=False, step=None, tag=None, log_image=False):$/;"	member	line:155	class:SF_CVDC
initiate	/home/neale/ctf_RL/method/CVDC2.py	/^    def initiate(self, verbose=1):$/;"	member	line:220	class:SF_CVDC
restore	/home/neale/ctf_RL/method/CVDC2.py	/^    def restore(self):$/;"	member	line:235	class:SF_CVDC
save	/home/neale/ctf_RL/method/CVDC2.py	/^    def save(self, checkpoint_number):$/;"	member	line:242	class:SF_CVDC
SF_CVDC_SC2	/home/neale/ctf_RL/method/CVDC2.py	/^class SF_CVDC_SC2:$/;"	class	line:254
__init__	/home/neale/ctf_RL/method/CVDC2.py	/^    def __init__($/;"	member	line:258	class:SF_CVDC_SC2
log	/home/neale/ctf_RL/method/CVDC2.py	/^    def log(self, step, log_weights=True, logs=None):$/;"	member	line:324	class:SF_CVDC_SC2
run_network_central	/home/neale/ctf_RL/method/CVDC2.py	/^    def run_network_central(self, states):$/;"	member	line:352	class:SF_CVDC_SC2
run_network_decentral	/home/neale/ctf_RL/method/CVDC2.py	/^    def run_network_decentral(self, states_list,validActions_list):$/;"	member	line:358	class:SF_CVDC_SC2
update_central	/home/neale/ctf_RL/method/CVDC2.py	/^    def update_central(self, datasets, writer=None, log=False, step=None, tag=None):$/;"	member	line:377	class:SF_CVDC_SC2
update_decentral	/home/neale/ctf_RL/method/CVDC2.py	/^    def update_decentral(self, datasets, writer=None, log=False, step=None, tag=None, log_image=False):$/;"	member	line:399	class:SF_CVDC_SC2
initiate	/home/neale/ctf_RL/method/CVDC2.py	/^    def initiate(self, verbose=1):$/;"	member	line:465	class:SF_CVDC_SC2
restore	/home/neale/ctf_RL/method/CVDC2.py	/^    def restore(self):$/;"	member	line:480	class:SF_CVDC_SC2
save	/home/neale/ctf_RL/method/CVDC2.py	/^    def save(self, checkpoint_number):$/;"	member	line:487	class:SF_CVDC_SC2
TDCentral	/home/neale/ctf_RL/method/td.py	/^class TDCentral:$/;"	class	line:9
__init__	/home/neale/ctf_RL/method/td.py	/^    def __init__($/;"	member	line:11	class:TDCentral
run_network	/home/neale/ctf_RL/method/td.py	/^    def run_network(self, states):$/;"	member	line:44	class:TDCentral
update_network	/home/neale/ctf_RL/method/td.py	/^    def update_network(self, state_input, reward, done, next_state, td_target):$/;"	member	line:49	class:TDCentral
update_decoder	/home/neale/ctf_RL/method/td.py	/^    def update_decoder(self, state_input):$/;"	member	line:58	class:TDCentral
initiate	/home/neale/ctf_RL/method/td.py	/^    def initiate(self):$/;"	member	line:63	class:TDCentral
restore	/home/neale/ctf_RL/method/td.py	/^    def restore(self):$/;"	member	line:66	class:TDCentral
save	/home/neale/ctf_RL/method/td.py	/^    def save(self, checkpoint_number):$/;"	member	line:69	class:TDCentral
Loss	/home/neale/ctf_RL/method/ppo_modularized.py	/^class Loss:$/;"	class	line:31
_log	/home/neale/ctf_RL/method/ppo_modularized.py	/^    def _log(val):$/;"	member	line:40	class:Loss
ppo	/home/neale/ctf_RL/method/ppo_modularized.py	/^    def ppo(policy, log_prob, old_log_prob,$/;"	member	line:45	class:Loss
PPO_V3	/home/neale/ctf_RL/method/ppo_modularized.py	/^class PPO_V3(a3c):$/;"	class	line:82
__init__	/home/neale/ctf_RL/method/ppo_modularized.py	/^    def __init__($/;"	member	line:84	class:PPO_V3
run_network	/home/neale/ctf_RL/method/ppo_modularized.py	/^    def run_network(self, states):$/;"	member	line:138	class:PPO_V3
update_global	/home/neale/ctf_RL/method/ppo_modularized.py	/^    def update_global(self, state_input, action, td_target, advantage, old_logit, global_episodes, writer=None, log=False):$/;"	member	line:148	class:PPO_V3
_build_network	/home/neale/ctf_RL/method/ppo_modularized.py	/^    def _build_network(self, input_hold):$/;"	member	line:188	class:PPO_V3
a3c	/home/neale/ctf_RL/method/a3c.py	/^class a3c:$/;"	class	line:28
__init__	/home/neale/ctf_RL/method/a3c.py	/^    def __init__($/;"	member	line:35	class:a3c
_kl_entropy	/home/neale/ctf_RL/method/a3c.py	/^    def _kl_entropy(self):$/;"	member	line:86	class:a3c
_build_placeholder	/home/neale/ctf_RL/method/a3c.py	/^    def _build_placeholder(self, input_shape):$/;"	member	line:99	class:a3c
_build_summary	/home/neale/ctf_RL/method/a3c.py	/^    def _build_summary(self, vars_list: list):$/;"	member	line:105	class:a3c
_build_grad_summary	/home/neale/ctf_RL/method/a3c.py	/^    def _build_grad_summary(self, vars_list: list, grads_list: list):$/;"	member	line:130	class:a3c
_build_network	/home/neale/ctf_RL/method/a3c.py	/^    def _build_network(self, input_holder):$/;"	member	line:133	class:a3c
run_network	/home/neale/ctf_RL/method/a3c.py	/^    def run_network(self, states):$/;"	member	line:154	class:a3c
run_sample	/home/neale/ctf_RL/method/a3c.py	/^    def run_sample(self, states):$/;"	member	line:170	class:a3c
update_global	/home/neale/ctf_RL/method/a3c.py	/^    def update_global(self, state_input, action, td_target, advantage, global_episodes, writer=None, log=False):$/;"	member	line:174	class:a3c
pull_global	/home/neale/ctf_RL/method/a3c.py	/^    def pull_global(self):$/;"	member	line:217	class:a3c
initialize_vars	/home/neale/ctf_RL/method/a3c.py	/^    def initialize_vars(self):$/;"	member	line:220	class:a3c
initiate	/home/neale/ctf_RL/method/a3c.py	/^    def initiate(self, saver, model_path):$/;"	member	line:225	class:a3c
save	/home/neale/ctf_RL/method/a3c.py	/^    def save(self, saver, model_path, global_step):$/;"	member	line:237	class:a3c
get_vars	/home/neale/ctf_RL/method/a3c.py	/^    def get_vars(self):$/;"	member	line:241	class:a3c
ActorCritic	/home/neale/ctf_RL/method/a3c.py	/^class ActorCritic(a3c):$/;"	class	line:245
__init__	/home/neale/ctf_RL/method/a3c.py	/^    def __init__(self, in_size, action_size, scope,$/;"	member	line:252	class:ActorCritic
_build_network	/home/neale/ctf_RL/method/a3c.py	/^    def _build_network(self, input_hold):$/;"	member	line:264	class:ActorCritic
ACER	/home/neale/ctf_RL/method/ACER.py	/^class ACER:$/;"	class	line:10
__init__	/home/neale/ctf_RL/method/ACER.py	/^    def __init__(self, in_size, action_size, scope,$/;"	member	line:12	class:ACER
build_network	/home/neale/ctf_RL/method/ACER.py	/^    def build_network(self, input_):$/;"	member	line:72	class:ACER
update_global	/home/neale/ctf_RL/method/ACER.py	/^    def update_global(self, state, action, adv, td_target, retrace, retrace_prod):$/;"	member	line:104	class:ACER
pull_global	/home/neale/ctf_RL/method/ACER.py	/^    def pull_global(self):$/;"	member	line:117	class:ACER
get_ac	/home/neale/ctf_RL/method/ACER.py	/^    def get_ac(self, s):$/;"	member	line:121	class:ACER
SF_CVDC	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^class SF_CVDC:$/;"	class	line:16
__init__	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def __init__($/;"	member	line:20	class:SF_CVDC
log	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def log(self, step, log_weights=True, logs=None):$/;"	member	line:85	class:SF_CVDC
run_network_central	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def run_network_central(self, states):$/;"	member	line:113	class:SF_CVDC
run_network_decentral	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def run_network_decentral(self, states_list):$/;"	member	line:119	class:SF_CVDC
update_central	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def update_central(self, datasets, writer=None, log=False, step=None, tag=None):$/;"	member	line:133	class:SF_CVDC
update_decentral	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def update_decentral(self, datasets, writer=None, log=False, step=None, tag=None, log_image=False):$/;"	member	line:155	class:SF_CVDC
initiate	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def initiate(self, verbose=1):$/;"	member	line:220	class:SF_CVDC
restore	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def restore(self):$/;"	member	line:235	class:SF_CVDC
save	/home/neale/ctf_RL/method/CVDC_decOnly.py	/^    def save(self, checkpoint_number):$/;"	member	line:242	class:SF_CVDC
COMA	/home/neale/ctf_RL/method/COMA.py	/^class COMA:$/;"	class	line:14
__init__	/home/neale/ctf_RL/method/COMA.py	/^    def __init__($/;"	member	line:18	class:COMA
log	/home/neale/ctf_RL/method/COMA.py	/^    def log(self, step, log_weights=True, logs=None):$/;"	member	line:78	class:COMA
run_network_decentral	/home/neale/ctf_RL/method/COMA.py	/^    def run_network_decentral(self, states_list):$/;"	member	line:93	class:COMA
update	/home/neale/ctf_RL/method/COMA.py	/^    def update(self, dataset, writer=None, log=False, step=None, tag=None):$/;"	member	line:100	class:COMA
initiate	/home/neale/ctf_RL/method/COMA.py	/^    def initiate(self, verbose=1):$/;"	member	line:125	class:COMA
restore	/home/neale/ctf_RL/method/COMA.py	/^    def restore(self):$/;"	member	line:140	class:COMA
save	/home/neale/ctf_RL/method/COMA.py	/^    def save(self, checkpoint_number):$/;"	member	line:147	class:COMA
Encoder	/home/neale/ctf_RL/method/DQN.py	/^class Encoder(tf.keras.Model):$/;"	class	line:19
__init__	/home/neale/ctf_RL/method/DQN.py	/^    def __init__(self, phi_n=16, action_size=5, trainable=True, name='DQN_encoder'):$/;"	member	line:21	class:Encoder
call	/home/neale/ctf_RL/method/DQN.py	/^    def call(self, inputs):$/;"	member	line:52	class:Encoder
DQN	/home/neale/ctf_RL/method/DQN.py	/^class DQN:$/;"	class	line:66
__init__	/home/neale/ctf_RL/method/DQN.py	/^    def __init__($/;"	member	line:68	class:DQN
build_loss	/home/neale/ctf_RL/method/DQN.py	/^    def build_loss(self, curr_q, actions, rewards, next_q, dones, reward_predict):$/;"	member	line:101	class:DQN
run_network	/home/neale/ctf_RL/method/DQN.py	/^    def run_network(self, states, return_action=True):$/;"	member	line:123	class:DQN
update_network	/home/neale/ctf_RL/method/DQN.py	/^    def update_network(self, states, next_states, actions, rewards, dones, global_episodes, writer=None, log=False):$/;"	member	line:128	class:DQN
initialize_vars	/home/neale/ctf_RL/method/DQN.py	/^    def initialize_vars(self):$/;"	member	line:157	class:DQN
initiate	/home/neale/ctf_RL/method/DQN.py	/^    def initiate(self, saver, model_path):$/;"	member	line:162	class:DQN
save	/home/neale/ctf_RL/method/DQN.py	/^    def save(self, saver, model_path, global_step):$/;"	member	line:173	class:DQN
get_vars	/home/neale/ctf_RL/method/DQN.py	/^    def get_vars(self):$/;"	member	line:177	class:DQN
a	/home/neale/ctf_RL/method/DQN.py	/^    a = Encoder(5, trainable=False)$/;"	variable	line:182
a	/home/neale/ctf_RL/method/DQN.py	/^    a = Encoder(5, trainable=True)$/;"	variable	line:186
PG_Module	/home/neale/ctf_RL/method/pg.py	/^class PG_Module:$/;"	class	line:12
__init__	/home/neale/ctf_RL/method/pg.py	/^    def __init__($/;"	member	line:14	class:PG_Module
run_network	/home/neale/ctf_RL/method/pg.py	/^    def run_network(self, states_list):$/;"	member	line:45	class:PG_Module
update_network	/home/neale/ctf_RL/method/pg.py	/^    def update_network(self, train_datasets, log=False, writer=None, step=None, tag=None):$/;"	member	line:53	class:PG_Module
initiate	/home/neale/ctf_RL/method/pg.py	/^    def initiate(self, verbose=1):$/;"	member	line:77	class:PG_Module
restore	/home/neale/ctf_RL/method/pg.py	/^    def restore(self):$/;"	member	line:88	class:PG_Module
save	/home/neale/ctf_RL/method/pg.py	/^    def save(self, checkpoint_number):$/;"	member	line:92	class:PG_Module
SF_CVDC	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^class SF_CVDC:$/;"	class	line:16
__init__	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def __init__($/;"	member	line:20	class:SF_CVDC
log	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def log(self, step, log_weights=True, logs=None):$/;"	member	line:85	class:SF_CVDC
run_network_central	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def run_network_central(self, states):$/;"	member	line:113	class:SF_CVDC
run_network_decentral	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def run_network_decentral(self, states_list):$/;"	member	line:119	class:SF_CVDC
update_central	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def update_central(self, datasets, writer=None, log=False, step=None, tag=None):$/;"	member	line:133	class:SF_CVDC
update_decentral	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def update_decentral(self, datasets, writer=None, log=False, step=None, tag=None, log_image=False):$/;"	member	line:155	class:SF_CVDC
initiate	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def initiate(self, verbose=1):$/;"	member	line:220	class:SF_CVDC
restore	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def restore(self):$/;"	member	line:235	class:SF_CVDC
save	/home/neale/ctf_RL/method/CVDC_SFOnly.py	/^    def save(self, checkpoint_number):$/;"	member	line:242	class:SF_CVDC
physical_devices	/home/neale/ctf_RL/run_cvdc.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:18
parser	/home/neale/ctf_RL/run_cvdc.py	/^parser = argparse.ArgumentParser(description="CVDC(learnability) trainer for convoy")$/;"	variable	line:48
args	/home/neale/ctf_RL/run_cvdc.py	/^args = parser.parse_args()$/;"	variable	line:59
PROGBAR	/home/neale/ctf_RL/run_cvdc.py	/^PROGBAR = args.silence$/;"	variable	line:61
TRAIN_NAME	/home/neale/ctf_RL/run_cvdc.py	/^TRAIN_NAME = "CVDC_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:64
TRAIN_TAG	/home/neale/ctf_RL/run_cvdc.py	/^TRAIN_TAG = "Central value decentralized control(learnability), " + TRAIN_NAME$/;"	variable	line:73
LOG_PATH	/home/neale/ctf_RL/run_cvdc.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:74
MODEL_PATH	/home/neale/ctf_RL/run_cvdc.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:75
SAVE_PATH	/home/neale/ctf_RL/run_cvdc.py	/^SAVE_PATH = ".\/save\/" + TRAIN_NAME$/;"	variable	line:76
MAP_PATH	/home/neale/ctf_RL/run_cvdc.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:77
GPU_CAPACITY	/home/neale/ctf_RL/run_cvdc.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:78
NENV	/home/neale/ctf_RL/run_cvdc.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:82
env_setting_path	/home/neale/ctf_RL/run_cvdc.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:85
game_config	/home/neale/ctf_RL/run_cvdc.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:86
config_path	/home/neale/ctf_RL/run_cvdc.py	/^config_path = "config.ini"$/;"	variable	line:99
config	/home/neale/ctf_RL/run_cvdc.py	/^config = configparser.ConfigParser()$/;"	variable	line:100
total_episodes	/home/neale/ctf_RL/run_cvdc.py	/^total_episodes = 100000$/;"	variable	line:104
max_ep	/home/neale/ctf_RL/run_cvdc.py	/^max_ep = 200$/;"	variable	line:105
gamma	/home/neale/ctf_RL/run_cvdc.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:106
lambd	/home/neale/ctf_RL/run_cvdc.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:107
save_network_frequency	/home/neale/ctf_RL/run_cvdc.py	/^save_network_frequency = 1024$/;"	variable	line:109
save_stat_frequency	/home/neale/ctf_RL/run_cvdc.py	/^save_stat_frequency = 128$/;"	variable	line:110
save_image_frequency	/home/neale/ctf_RL/run_cvdc.py	/^save_image_frequency = 128$/;"	variable	line:111
moving_average_step	/home/neale/ctf_RL/run_cvdc.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:112
action_space	/home/neale/ctf_RL/run_cvdc.py	/^action_space = 5$/;"	variable	line:114
keep_frame	/home/neale/ctf_RL/run_cvdc.py	/^keep_frame = 1$/;"	variable	line:115
map_size	/home/neale/ctf_RL/run_cvdc.py	/^map_size = args.map_size$/;"	variable	line:116
vision_range	/home/neale/ctf_RL/run_cvdc.py	/^vision_range = map_size - 1$/;"	variable	line:117
nchannel	/home/neale/ctf_RL/run_cvdc.py	/^nchannel = 6 * keep_frame$/;"	variable	line:119
input_size	/home/neale/ctf_RL/run_cvdc.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:120
cent_input_size	/home/neale/ctf_RL/run_cvdc.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:121
minibatch_size	/home/neale/ctf_RL/run_cvdc.py	/^minibatch_size = 256$/;"	variable	line:123
epoch	/home/neale/ctf_RL/run_cvdc.py	/^epoch = 2$/;"	variable	line:124
minimum_batch_size	/home/neale/ctf_RL/run_cvdc.py	/^minimum_batch_size = 1024 * 4$/;"	variable	line:125
log_episodic_reward	/home/neale/ctf_RL/run_cvdc.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:128
log_winrate	/home/neale/ctf_RL/run_cvdc.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:129
log_redwinrate	/home/neale/ctf_RL/run_cvdc.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:130
log_looptime	/home/neale/ctf_RL/run_cvdc.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:131
log_traintime	/home/neale/ctf_RL/run_cvdc.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:132
_qenv	/home/neale/ctf_RL/run_cvdc.py	/^_qenv = gym.make("cap-v0", map_size=map_size, config_path=game_config)$/;"	variable	line:136
make_env	/home/neale/ctf_RL/run_cvdc.py	/^def make_env(map_size):$/;"	function	line:137
envs	/home/neale/ctf_RL/run_cvdc.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:141
envs	/home/neale/ctf_RL/run_cvdc.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:142
num_blue	/home/neale/ctf_RL/run_cvdc.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:143
num_red	/home/neale/ctf_RL/run_cvdc.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:144
num_agent	/home/neale/ctf_RL/run_cvdc.py	/^num_agent = num_blue  # +num_red$/;"	variable	line:145
agent_type	/home/neale/ctf_RL/run_cvdc.py	/^agent_type = []$/;"	variable	line:151
num_type	/home/neale/ctf_RL/run_cvdc.py	/^num_type = len(agent_type)$/;"	variable	line:156
agent_type_masking	/home/neale/ctf_RL/run_cvdc.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:157
agent_type_index	/home/neale/ctf_RL/run_cvdc.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:158
prev_i	/home/neale/ctf_RL/run_cvdc.py	/^prev_i = 0$/;"	variable	line:159
prev_i	/home/neale/ctf_RL/run_cvdc.py	/^    prev_i = i$/;"	variable	line:163
agent_type_masking	/home/neale/ctf_RL/run_cvdc.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:164
atoms	/home/neale/ctf_RL/run_cvdc.py	/^atoms = 128$/;"	variable	line:167
network	/home/neale/ctf_RL/run_cvdc.py	/^network = Network($/;"	variable	line:168
central_obs_shape	/home/neale/ctf_RL/run_cvdc.py	/^    central_obs_shape=cent_input_size,$/;"	variable	line:169
decentral_obs_shape	/home/neale/ctf_RL/run_cvdc.py	/^    decentral_obs_shape=input_size,$/;"	variable	line:170
action_size	/home/neale/ctf_RL/run_cvdc.py	/^    action_size=action_space,$/;"	variable	line:171
agent_type	/home/neale/ctf_RL/run_cvdc.py	/^    agent_type=agent_type,$/;"	variable	line:172
atoms	/home/neale/ctf_RL/run_cvdc.py	/^    atoms=atoms,$/;"	variable	line:173
save_path	/home/neale/ctf_RL/run_cvdc.py	/^    save_path=MODEL_PATH,$/;"	variable	line:174
global_episodes	/home/neale/ctf_RL/run_cvdc.py	/^global_episodes = network.initiate()$/;"	variable	line:178
writer	/home/neale/ctf_RL/run_cvdc.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:182
train_central	/home/neale/ctf_RL/run_cvdc.py	/^def train_central($/;"	function	line:185
traj_buffer	/home/neale/ctf_RL/run_cvdc.py	/^    traj_buffer = defaultdict(list)$/;"	variable	line:195
states	/home/neale/ctf_RL/run_cvdc.py	/^        states = np.array(traj[0])$/;"	variable	line:198
last_state	/home/neale/ctf_RL/run_cvdc.py	/^        last_state = np.array(traj[1])[-1:, :, :, :]$/;"	variable	line:199
reward	/home/neale/ctf_RL/run_cvdc.py	/^        reward = traj[2]$/;"	variable	line:200
critic	/home/neale/ctf_RL/run_cvdc.py	/^        critic = env_critic["critic"].numpy()[:, 0].tolist()$/;"	variable	line:204
_critic	/home/neale/ctf_RL/run_cvdc.py	/^        _critic = _env_critic["critic"].numpy()[0, 0]$/;"	variable	line:205
train_dataset	/home/neale/ctf_RL/run_cvdc.py	/^    train_dataset = ($/;"	variable	line:213
train_decentral	/home/neale/ctf_RL/run_cvdc.py	/^def train_decentral($/;"	function	line:230
train_datasets	/home/neale/ctf_RL/run_cvdc.py	/^    train_datasets = []$/;"	variable	line:238
traj_buffer_list	/home/neale/ctf_RL/run_cvdc.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:241
advantage_lists	/home/neale/ctf_RL/run_cvdc.py	/^    advantage_lists = [[] for _ in range(num_type)]$/;"	variable	line:242
atype	/home/neale/ctf_RL/run_cvdc.py	/^            atype = agent_type_index[idx]$/;"	variable	line:245
reward	/home/neale/ctf_RL/run_cvdc.py	/^            reward = traj[2]$/;"	variable	line:247
mask	/home/neale/ctf_RL/run_cvdc.py	/^            mask = traj[3]$/;"	variable	line:248
critic	/home/neale/ctf_RL/run_cvdc.py	/^            critic = traj[5]$/;"	variable	line:249
phi	/home/neale/ctf_RL/run_cvdc.py	/^            phi = np.array(traj[7]).tolist()$/;"	variable	line:250
psi	/home/neale/ctf_RL/run_cvdc.py	/^            psi = np.array(traj[8]).tolist()$/;"	variable	line:251
_critic	/home/neale/ctf_RL/run_cvdc.py	/^            _critic = traj[9][-1]$/;"	variable	line:252
_psi	/home/neale/ctf_RL/run_cvdc.py	/^            _psi = np.array(traj[10][-1])$/;"	variable	line:253
normalize	/home/neale/ctf_RL/run_cvdc.py	/^                normalize=False$/;"	variable	line:259
normalize	/home/neale/ctf_RL/run_cvdc.py	/^                normalize=False,$/;"	variable	line:268
discount_adv	/home/neale/ctf_RL/run_cvdc.py	/^                discount_adv=False,$/;"	variable	line:277
normalize	/home/neale/ctf_RL/run_cvdc.py	/^                normalize=False,$/;"	variable	line:278
beta	/home/neale/ctf_RL/run_cvdc.py	/^            beta = (-0.9\/40000)*step + 1$/;"	variable	line:280
advantages	/home/neale/ctf_RL/run_cvdc.py	/^            advantages = [beta*a1+(1-beta)*a2 for a1, a2 in zip(advantages_global, advantages)]$/;"	variable	line:281
traj_buffer	/home/neale/ctf_RL/run_cvdc.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:284
traj_buffer	/home/neale/ctf_RL/run_cvdc.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:295
train_dataset	/home/neale/ctf_RL/run_cvdc.py	/^        train_dataset = ($/;"	variable	line:296
tag	/home/neale/ctf_RL/run_cvdc.py	/^            tag = "advantages\/"$/;"	variable	line:321
run_network	/home/neale/ctf_RL/run_cvdc.py	/^def run_network(states):$/;"	function	line:329
batch	/home/neale/ctf_RL/run_cvdc.py	/^batch = []$/;"	variable	line:368
dec_batch	/home/neale/ctf_RL/run_cvdc.py	/^dec_batch = []$/;"	variable	line:369
num_batch	/home/neale/ctf_RL/run_cvdc.py	/^num_batch = 0$/;"	variable	line:370
dec_batch_size	/home/neale/ctf_RL/run_cvdc.py	/^dec_batch_size = 0$/;"	variable	line:371
log_save_analysis	/home/neale/ctf_RL/run_cvdc.py	/^    log_save_analysis = False  #interval_flag(global_episodes, 1024 * 4, "save_log")$/;"	variable	line:375
episode_rew	/home/neale/ctf_RL/run_cvdc.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:378
is_alive	/home/neale/ctf_RL/run_cvdc.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:379
is_done	/home/neale/ctf_RL/run_cvdc.py	/^    is_done = [False for env in range(NENV * num_agent)]$/;"	variable	line:380
trajs	/home/neale/ctf_RL/run_cvdc.py	/^    trajs = [[Trajectory(depth=14) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:382
cent_trajs	/home/neale/ctf_RL/run_cvdc.py	/^    cent_trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:383
s1	/home/neale/ctf_RL/run_cvdc.py	/^    s1 = envs.reset(config_path=game_config, policy_red=policy.Roomba,)$/;"	variable	line:390
s1	/home/neale/ctf_RL/run_cvdc.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:391
reward_pred_list	/home/neale/ctf_RL/run_cvdc.py	/^    reward_pred_list = []$/;"	variable	line:396
stime_roll	/home/neale/ctf_RL/run_cvdc.py	/^    stime_roll = time.time()$/;"	variable	line:399
_states	/home/neale/ctf_RL/run_cvdc.py	/^    _states = [[] for _ in range(NENV)]$/;"	variable	line:400
_agent1_r	/home/neale/ctf_RL/run_cvdc.py	/^    _agent1_r = [[] for _ in range(NENV)]$/;"	variable	line:401
_agent2_r	/home/neale/ctf_RL/run_cvdc.py	/^    _agent2_r = [[] for _ in range(NENV)]$/;"	variable	line:402
_agent3_r	/home/neale/ctf_RL/run_cvdc.py	/^    _agent3_r = [[] for _ in range(NENV)]$/;"	variable	line:403
_agent1_o	/home/neale/ctf_RL/run_cvdc.py	/^    _agent1_o = [[] for _ in range(NENV)]$/;"	variable	line:404
_agent2_o	/home/neale/ctf_RL/run_cvdc.py	/^    _agent2_o = [[] for _ in range(NENV)]$/;"	variable	line:405
_agent3_o	/home/neale/ctf_RL/run_cvdc.py	/^    _agent3_o = [[] for _ in range(NENV)]$/;"	variable	line:406
s0	/home/neale/ctf_RL/run_cvdc.py	/^        s0 = s1$/;"	variable	line:409
a0	/home/neale/ctf_RL/run_cvdc.py	/^        a0 = a1$/;"	variable	line:410
vg0	/home/neale/ctf_RL/run_cvdc.py	/^        vg0 = vg1$/;"	variable	line:411
vc0	/home/neale/ctf_RL/run_cvdc.py	/^        vc0 = vc1$/;"	variable	line:412
psi0	/home/neale/ctf_RL/run_cvdc.py	/^        psi0 = psi1$/;"	variable	line:413
phi0	/home/neale/ctf_RL/run_cvdc.py	/^        phi0 = phi1$/;"	variable	line:414
log_logits0	/home/neale/ctf_RL/run_cvdc.py	/^        log_logits0 = log_logits1$/;"	variable	line:415
was_alive	/home/neale/ctf_RL/run_cvdc.py	/^        was_alive = is_alive$/;"	variable	line:416
was_done	/home/neale/ctf_RL/run_cvdc.py	/^        was_done = is_done$/;"	variable	line:417
is_alive	/home/neale/ctf_RL/run_cvdc.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:422
s1	/home/neale/ctf_RL/run_cvdc.py	/^        s1 = s1.astype(np.float32)  # Decentralize observation$/;"	variable	line:423
idx	/home/neale/ctf_RL/run_cvdc.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:435
etime_roll	/home/neale/ctf_RL/run_cvdc.py	/^    etime_roll = time.time()$/;"	variable	line:476
dec_batch_size	/home/neale/ctf_RL/run_cvdc.py	/^    dec_batch_size = len(dec_batch) * 200 * num_agent$/;"	variable	line:480
stime_train	/home/neale/ctf_RL/run_cvdc.py	/^        stime_train = time.time()$/;"	variable	line:482
log_image_on	/home/neale/ctf_RL/run_cvdc.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:483
epoch	/home/neale/ctf_RL/run_cvdc.py	/^            epoch=epoch,$/;"	variable	line:486
batch_size	/home/neale/ctf_RL/run_cvdc.py	/^            batch_size=minibatch_size,$/;"	variable	line:487
writer	/home/neale/ctf_RL/run_cvdc.py	/^            writer=writer,$/;"	variable	line:488
log	/home/neale/ctf_RL/run_cvdc.py	/^            log=log_image_on,$/;"	variable	line:489
step	/home/neale/ctf_RL/run_cvdc.py	/^            step=global_episodes,$/;"	variable	line:490
etime_train	/home/neale/ctf_RL/run_cvdc.py	/^        etime_train = time.time()$/;"	variable	line:492
dec_batch	/home/neale/ctf_RL/run_cvdc.py	/^        dec_batch = []$/;"	variable	line:493
log_on	/home/neale/ctf_RL/run_cvdc.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:515
tag	/home/neale/ctf_RL/run_cvdc.py	/^            tag = "baseline_training\/"$/;"	variable	line:518
step	/home/neale/ctf_RL/run_cvdc.py	/^                step=global_episodes,$/;"	variable	line:533
save_on	/home/neale/ctf_RL/run_cvdc.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:537
fig	/home/neale/ctf_RL/run_cvdc.py	/^        fig = plt.figure(figsize=(8, 9))$/;"	variable	line:543
widths	/home/neale/ctf_RL/run_cvdc.py	/^        widths = [1.2, 0.5, 1.5, 1.5]$/;"	variable	line:544
heights	/home/neale/ctf_RL/run_cvdc.py	/^        heights = [2, 2, 4, 4, 4]$/;"	variable	line:545
gs	/home/neale/ctf_RL/run_cvdc.py	/^        gs = fig.add_gridspec($/;"	variable	line:546
ax_env	/home/neale/ctf_RL/run_cvdc.py	/^        ax_env = fig.add_subplot(gs[:2, :2])$/;"	variable	line:549
ax_value	/home/neale/ctf_RL/run_cvdc.py	/^        ax_value = fig.add_subplot(gs[:2, 2:])$/;"	variable	line:553
ax_agent1	/home/neale/ctf_RL/run_cvdc.py	/^        ax_agent1 = fig.add_subplot(gs[2, 0])$/;"	variable	line:556
ax_agent2	/home/neale/ctf_RL/run_cvdc.py	/^        ax_agent2 = fig.add_subplot(gs[3, 0])$/;"	variable	line:560
ax_agent3	/home/neale/ctf_RL/run_cvdc.py	/^        ax_agent3 = fig.add_subplot(gs[4, 0])$/;"	variable	line:564
ax_reward3	/home/neale/ctf_RL/run_cvdc.py	/^        ax_reward3 = fig.add_subplot(gs[4, 1:])$/;"	variable	line:568
ax_reward2	/home/neale/ctf_RL/run_cvdc.py	/^        ax_reward2 = fig.add_subplot(gs[3, 1:], sharex=ax_reward3)$/;"	variable	line:570
ax_reward1	/home/neale/ctf_RL/run_cvdc.py	/^        ax_reward1 = fig.add_subplot(gs[2, 1:], sharex=ax_reward3)$/;"	variable	line:573
env_image	/home/neale/ctf_RL/run_cvdc.py	/^        env_image = ax_env.imshow(np.ones((map_size, map_size, 3)), vmin=0, vmax=1)$/;"	variable	line:577
agent_obs1	/home/neale/ctf_RL/run_cvdc.py	/^        agent_obs1 = ax_agent1.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:578
agent_obs2	/home/neale/ctf_RL/run_cvdc.py	/^        agent_obs2 = ax_agent2.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:579
agent_obs3	/home/neale/ctf_RL/run_cvdc.py	/^        agent_obs3 = ax_agent3.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:580
animate	/home/neale/ctf_RL/run_cvdc.py	/^        def animate(i, info, critic, env_idx):$/;"	function	line:590
_states	/home/neale/ctf_RL/run_cvdc.py	/^        _states = None$/;"	variable	line:641
_agent1_r	/home/neale/ctf_RL/run_cvdc.py	/^        _agent1_r = None$/;"	variable	line:642
_agent2_r	/home/neale/ctf_RL/run_cvdc.py	/^        _agent2_r = None$/;"	variable	line:643
_agent3_r	/home/neale/ctf_RL/run_cvdc.py	/^        _agent3_r = None$/;"	variable	line:644
_agent1_o	/home/neale/ctf_RL/run_cvdc.py	/^        _agent1_o = None$/;"	variable	line:645
_agent2_o	/home/neale/ctf_RL/run_cvdc.py	/^        _agent2_o = None$/;"	variable	line:646
_agent3_o	/home/neale/ctf_RL/run_cvdc.py	/^        _agent3_o = None$/;"	variable	line:647
anim	/home/neale/ctf_RL/run_cvdc.py	/^        anim = None$/;"	variable	line:648
physical_devices	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:18
parser	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^parser = argparse.ArgumentParser(description="PPO trainer for convoy")$/;"	variable	line:50
args	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^args = parser.parse_args()$/;"	variable	line:58
PROGBAR	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^PROGBAR = args.silence$/;"	variable	line:60
TRAIN_NAME	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^TRAIN_NAME = "CVDC_{}_{:02d}_map_{}".format($/;"	variable	line:63
TRAIN_TAG	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^TRAIN_TAG = "PPO e2e model w Stacked Frames: " + TRAIN_NAME$/;"	variable	line:68
LOG_PATH	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:69
MODEL_PATH	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:70
GPU_CAPACITY	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:71
NENV	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^NENV = 1$/;"	variable	line:73
total_episodes	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^total_episodes = 50000$/;"	variable	line:82
max_ep	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^max_ep = 200$/;"	variable	line:83
gamma	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:84
lambd	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:85
save_network_frequency	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^save_network_frequency = 1024$/;"	variable	line:87
save_stat_frequency	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^save_stat_frequency = 128$/;"	variable	line:88
save_image_frequency	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^save_image_frequency = 128$/;"	variable	line:89
moving_average_step	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:90
keep_frame	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^keep_frame = 1$/;"	variable	line:92
minibatch_size	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^minibatch_size = 128$/;"	variable	line:94
epoch	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^epoch = 2$/;"	variable	line:95
minimum_batch_size	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^minimum_batch_size = 1024$/;"	variable	line:96
log_episodic_reward	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:99
log_winrate	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:100
log_looptime	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:101
log_traintime	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:102
make_env	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^def make_env():$/;"	function	line:106
envs	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^envs = StarCraft2Env(args.map)$/;"	variable	line:109
envs	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^envs = SMACWrapper(envs)$/;"	variable	line:111
envs	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^envs = FrameStacking(envs)$/;"	variable	line:112
action_space	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^action_space = len(envs.env.get_avail_agent_actions(0))$/;"	variable	line:114
input_size	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^input_size = [None,envs.observation_space.shape[0]]$/;"	variable	line:115
cent_input_size	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^cent_input_size = [None,envs.state_shape.shape[0]]$/;"	variable	line:116
agent_type	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^agent_type = []$/;"	variable	line:128
num_agent	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^num_agent = sum(agent_type)$/;"	variable	line:133
num_type	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^num_type = len(agent_type)$/;"	variable	line:136
agent_type_masking	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^agent_type_masking = np.zeros([num_type, num_agent], dtype=bool)$/;"	variable	line:137
agent_type_index	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^agent_type_index = np.zeros([num_agent], dtype=int)$/;"	variable	line:138
prev_i	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^prev_i = 0$/;"	variable	line:139
prev_i	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    prev_i = i$/;"	variable	line:143
agent_type_masking	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:144
atoms	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^atoms = 256$/;"	variable	line:147
network	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^network = Network($/;"	variable	line:148
central_obs_shape	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    central_obs_shape=cent_input_size,$/;"	variable	line:149
decentral_obs_shape	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    decentral_obs_shape=input_size,$/;"	variable	line:150
action_size	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    action_size=action_space,$/;"	variable	line:151
agent_type	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    agent_type=agent_type,$/;"	variable	line:152
atoms	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    atoms=atoms,$/;"	variable	line:153
save_path	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    save_path=MODEL_PATH,$/;"	variable	line:154
entropy	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    entropy=args.entropy,$/;"	variable	line:155
global_episodes	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^global_episodes = network.initiate()$/;"	variable	line:159
writer	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:163
train_central	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^def train_central($/;"	function	line:166
traj_buffer	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    traj_buffer = defaultdict(list)$/;"	variable	line:176
states	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        states = np.array(traj[0])$/;"	variable	line:179
last_state	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        last_state = np.array(traj[3])[-1:, :]$/;"	variable	line:180
reward	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        reward = traj[2]$/;"	variable	line:181
critic	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        critic = env_critic["critic"].numpy()[:, 0].tolist()$/;"	variable	line:185
_critic	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _critic = _env_critic["critic"].numpy()[0, 0]$/;"	variable	line:186
train_dataset	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    train_dataset = ($/;"	variable	line:194
train_decentral	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^def train_decentral($/;"	function	line:211
train_datasets	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    train_datasets = []$/;"	variable	line:220
traj_buffer_list	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:223
advantage_lists	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    advantage_lists = [[] for _ in range(num_type)]$/;"	variable	line:224
f1_list	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    f1_list = []$/;"	variable	line:225
f2_list	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    f2_list = []$/;"	variable	line:226
fc_list	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    fc_list = []$/;"	variable	line:227
atype	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            atype = agent_type_index[idx]$/;"	variable	line:230
reward	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            reward = traj[2]$/;"	variable	line:232
mask	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            mask = traj[3]$/;"	variable	line:233
critic	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            critic = traj[5]$/;"	variable	line:234
phi	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            phi = np.array(traj[7]).tolist()$/;"	variable	line:235
psi	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            psi = np.array(traj[8]).tolist()$/;"	variable	line:236
_critic	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            _critic = traj[9][-1]$/;"	variable	line:237
_psi	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            _psi = np.array(traj[10][-1])$/;"	variable	line:238
cent_state	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            cent_state = np.array(traj[14])$/;"	variable	line:240
env_critic	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            env_critic = env_critic["critic"].numpy()[:, 0].tolist()$/;"	variable	line:242
icritic	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            icritic = traj[12]$/;"	variable	line:247
dc	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            dc = np.array(critic[1:])-np.array(critic[:-1])$/;"	variable	line:249
dc1	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            dc1 = np.array(env_critic[1:])-np.array(env_critic[:-1])$/;"	variable	line:250
dc2	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            dc2 = np.array(icritic[1:])-np.array(icritic[:-1])$/;"	variable	line:251
normalize	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^                normalize=False$/;"	variable	line:260
normalize	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^                normalize=False,$/;"	variable	line:269
discount_adv	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^                discount_adv=False,$/;"	variable	line:288
normalize	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^                normalize=False,$/;"	variable	line:289
beta	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            beta = max(min((-0.9\/30000)*step + 1, 1.0),0.1)$/;"	variable	line:291
traj_buffer	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:293
traj_buffer	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:304
train_dataset	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        train_dataset = ($/;"	variable	line:305
tag	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            tag = "advantages\/"$/;"	variable	line:330
run_network	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^def run_network(states,validActions):$/;"	function	line:342
batch	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^batch = []$/;"	variable	line:386
dec_batch	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^dec_batch = []$/;"	variable	line:387
log_save_analysis	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    log_save_analysis = False  #interval_flag(global_episodes, 1024 * 4, "save_log")$/;"	variable	line:391
episode_rew	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:394
is_alive	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    is_alive = [not va[0] for va in envs.get_avail_actions()]$/;"	variable	line:395
is_done	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    is_done = [False for env in range(NENV * num_agent)]$/;"	variable	line:396
trajs	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    trajs = [[Trajectory(depth=16) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:398
cent_trajs	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    cent_trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:399
s1	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    s1 = np.stack(s1).astype(np.float32)$/;"	variable	line:405
validActions	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    validActions = envs.get_avail_actions()$/;"	variable	line:406
validActions	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    validActions = envs.get_avail_actions()$/;"	variable	line:407
reward_pred_list	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    reward_pred_list = []$/;"	variable	line:416
stime_roll	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    stime_roll = time.time()$/;"	variable	line:419
_states	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    _states = [[] for _ in range(NENV)]$/;"	variable	line:420
_agent1_r	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    _agent1_r = [[] for _ in range(NENV)]$/;"	variable	line:421
_agent2_r	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    _agent2_r = [[] for _ in range(NENV)]$/;"	variable	line:422
_agent3_r	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    _agent3_r = [[] for _ in range(NENV)]$/;"	variable	line:423
_agent1_o	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    _agent1_o = [[] for _ in range(NENV)]$/;"	variable	line:424
_agent2_o	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    _agent2_o = [[] for _ in range(NENV)]$/;"	variable	line:425
_agent3_o	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    _agent3_o = [[] for _ in range(NENV)]$/;"	variable	line:426
s0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        s0 = s1$/;"	variable	line:429
a0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        a0 = a1$/;"	variable	line:430
vg0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        vg0 = vg1$/;"	variable	line:431
vc0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        vc0 = vc1$/;"	variable	line:432
psi0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        psi0 = psi1$/;"	variable	line:433
phi0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        phi0 = phi1$/;"	variable	line:434
log_logits0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        log_logits0 = log_logits1$/;"	variable	line:435
was_alive	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        was_alive = is_alive$/;"	variable	line:436
was_done	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        was_done = is_done$/;"	variable	line:437
cent_s0	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        cent_s0 = cent_s1$/;"	variable	line:438
is_alive	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        is_alive = [not va[0] for va in envs.get_avail_actions()]$/;"	variable	line:443
s1	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        s1 = s1.astype(np.float32)  # Decentralize observation$/;"	variable	line:444
validActions	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        validActions = envs.get_avail_actions()$/;"	variable	line:449
validActions	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        validActions = envs.get_avail_actions()$/;"	variable	line:450
idx	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:458
etime_roll	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    etime_roll = time.time()$/;"	variable	line:503
stime_train	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        stime_train = time.time()$/;"	variable	line:508
log	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        log = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:509
log_image	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        log_image = interval_flag(global_episodes, 1024, "ima_log")$/;"	variable	line:510
epoch	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            epoch=epoch,$/;"	variable	line:513
batch_size	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            batch_size=minibatch_size,$/;"	variable	line:514
writer	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            writer=writer,$/;"	variable	line:515
log	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            log=log,$/;"	variable	line:516
step	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            step=global_episodes,$/;"	variable	line:517
log_image	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            log_image=log_image,$/;"	variable	line:518
etime_train	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        etime_train = time.time()$/;"	variable	line:520
dec_batch	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        dec_batch = []$/;"	variable	line:521
log_tc_on	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        log_tc_on = interval_flag(global_episodes, save_image_frequency, 'tc_log')$/;"	variable	line:526
batch	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        batch = []$/;"	variable	line:528
log_on	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:539
tag	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^            tag = "baseline_training\/"$/;"	variable	line:542
step	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^                step=global_episodes,$/;"	variable	line:557
save_on	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:561
fig	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        fig = plt.figure(figsize=(8, 9))$/;"	variable	line:567
widths	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        widths = [1.2, 0.5, 1.5, 1.5]$/;"	variable	line:568
heights	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        heights = [2, 2, 4, 4, 4]$/;"	variable	line:569
gs	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        gs = fig.add_gridspec($/;"	variable	line:570
ax_env	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_env = fig.add_subplot(gs[:2, :2])$/;"	variable	line:573
ax_value	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_value = fig.add_subplot(gs[:2, 2:])$/;"	variable	line:577
ax_agent1	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_agent1 = fig.add_subplot(gs[2, 0])$/;"	variable	line:580
ax_agent2	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_agent2 = fig.add_subplot(gs[3, 0])$/;"	variable	line:584
ax_agent3	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_agent3 = fig.add_subplot(gs[4, 0])$/;"	variable	line:588
ax_reward3	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_reward3 = fig.add_subplot(gs[4, 1:])$/;"	variable	line:592
ax_reward2	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_reward2 = fig.add_subplot(gs[3, 1:], sharex=ax_reward3)$/;"	variable	line:594
ax_reward1	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        ax_reward1 = fig.add_subplot(gs[2, 1:], sharex=ax_reward3)$/;"	variable	line:597
env_image	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        env_image = ax_env.imshow(np.ones((map_size, map_size, 3)), vmin=0, vmax=1)$/;"	variable	line:601
agent_obs1	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        agent_obs1 = ax_agent1.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:602
agent_obs2	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        agent_obs2 = ax_agent2.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:603
agent_obs3	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        agent_obs3 = ax_agent3.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:604
animate	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        def animate(i, info, critic, env_idx):$/;"	function	line:614
_states	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _states = None$/;"	variable	line:665
_agent1_r	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _agent1_r = None$/;"	variable	line:666
_agent2_r	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _agent2_r = None$/;"	variable	line:667
_agent3_r	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _agent3_r = None$/;"	variable	line:668
_agent1_o	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _agent1_o = None$/;"	variable	line:669
_agent2_o	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _agent2_o = None$/;"	variable	line:670
_agent3_o	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        _agent3_o = None$/;"	variable	line:671
anim	/home/neale/ctf_RL/run_cvdc2_sc2.py	/^        anim = None$/;"	variable	line:672
physical_devices	/home/neale/ctf_RL/run_COMA.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:18
parser	/home/neale/ctf_RL/run_COMA.py	/^parser = argparse.ArgumentParser(description="CVDC(learnability) trainer for convoy")$/;"	variable	line:48
args	/home/neale/ctf_RL/run_COMA.py	/^args = parser.parse_args()$/;"	variable	line:59
PROGBAR	/home/neale/ctf_RL/run_COMA.py	/^PROGBAR = args.silence$/;"	variable	line:61
TRAIN_NAME	/home/neale/ctf_RL/run_COMA.py	/^TRAIN_NAME = "COMA_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:64
TRAIN_TAG	/home/neale/ctf_RL/run_COMA.py	/^TRAIN_TAG = "Central value decentralized control(COMA), " + TRAIN_NAME$/;"	variable	line:73
LOG_PATH	/home/neale/ctf_RL/run_COMA.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:74
MODEL_PATH	/home/neale/ctf_RL/run_COMA.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:75
MAP_PATH	/home/neale/ctf_RL/run_COMA.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:76
GPU_CAPACITY	/home/neale/ctf_RL/run_COMA.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:77
NENV	/home/neale/ctf_RL/run_COMA.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:81
env_setting_path	/home/neale/ctf_RL/run_COMA.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:84
game_config	/home/neale/ctf_RL/run_COMA.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:85
config_path	/home/neale/ctf_RL/run_COMA.py	/^config_path = "config.ini"$/;"	variable	line:97
config	/home/neale/ctf_RL/run_COMA.py	/^config = configparser.ConfigParser()$/;"	variable	line:98
total_episodes	/home/neale/ctf_RL/run_COMA.py	/^total_episodes = 100000$/;"	variable	line:102
max_ep	/home/neale/ctf_RL/run_COMA.py	/^max_ep = 200$/;"	variable	line:103
gamma	/home/neale/ctf_RL/run_COMA.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:104
lambd	/home/neale/ctf_RL/run_COMA.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:105
save_network_frequency	/home/neale/ctf_RL/run_COMA.py	/^save_network_frequency = 1024$/;"	variable	line:107
save_stat_frequency	/home/neale/ctf_RL/run_COMA.py	/^save_stat_frequency = 128$/;"	variable	line:108
save_image_frequency	/home/neale/ctf_RL/run_COMA.py	/^save_image_frequency = 128$/;"	variable	line:109
moving_average_step	/home/neale/ctf_RL/run_COMA.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:110
action_space	/home/neale/ctf_RL/run_COMA.py	/^action_space = 5$/;"	variable	line:112
keep_frame	/home/neale/ctf_RL/run_COMA.py	/^keep_frame = 1$/;"	variable	line:113
map_size	/home/neale/ctf_RL/run_COMA.py	/^map_size = args.map_size$/;"	variable	line:114
vision_range	/home/neale/ctf_RL/run_COMA.py	/^vision_range = map_size - 1$/;"	variable	line:115
nchannel	/home/neale/ctf_RL/run_COMA.py	/^nchannel = 6 * keep_frame$/;"	variable	line:117
input_size	/home/neale/ctf_RL/run_COMA.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:118
cent_input_size	/home/neale/ctf_RL/run_COMA.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:119
minibatch_size	/home/neale/ctf_RL/run_COMA.py	/^minibatch_size = 128$/;"	variable	line:121
epoch	/home/neale/ctf_RL/run_COMA.py	/^epoch = 1$/;"	variable	line:122
minimum_batch_size	/home/neale/ctf_RL/run_COMA.py	/^minimum_batch_size = 1024$/;"	variable	line:123
log_episodic_reward	/home/neale/ctf_RL/run_COMA.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:126
log_winrate	/home/neale/ctf_RL/run_COMA.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:127
log_redwinrate	/home/neale/ctf_RL/run_COMA.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:128
log_looptime	/home/neale/ctf_RL/run_COMA.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:129
log_traintime	/home/neale/ctf_RL/run_COMA.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:130
make_env	/home/neale/ctf_RL/run_COMA.py	/^def make_env(map_size):$/;"	function	line:135
envs	/home/neale/ctf_RL/run_COMA.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:139
envs	/home/neale/ctf_RL/run_COMA.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:140
num_blue	/home/neale/ctf_RL/run_COMA.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:141
num_red	/home/neale/ctf_RL/run_COMA.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:142
num_agent	/home/neale/ctf_RL/run_COMA.py	/^num_agent = num_blue  # +num_red$/;"	variable	line:143
agent_type	/home/neale/ctf_RL/run_COMA.py	/^agent_type = []$/;"	variable	line:149
num_type	/home/neale/ctf_RL/run_COMA.py	/^num_type = len(agent_type)$/;"	variable	line:154
agent_type_masking	/home/neale/ctf_RL/run_COMA.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:155
agent_type_index	/home/neale/ctf_RL/run_COMA.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:156
prev_i	/home/neale/ctf_RL/run_COMA.py	/^prev_i = 0$/;"	variable	line:157
prev_i	/home/neale/ctf_RL/run_COMA.py	/^    prev_i = i$/;"	variable	line:161
agent_type_masking	/home/neale/ctf_RL/run_COMA.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:162
atoms	/home/neale/ctf_RL/run_COMA.py	/^atoms = 128$/;"	variable	line:165
network	/home/neale/ctf_RL/run_COMA.py	/^network = Network($/;"	variable	line:166
state_shape	/home/neale/ctf_RL/run_COMA.py	/^    state_shape=input_size,$/;"	variable	line:167
cent_state_shape	/home/neale/ctf_RL/run_COMA.py	/^    cent_state_shape=cent_input_size,$/;"	variable	line:168
action_size	/home/neale/ctf_RL/run_COMA.py	/^    action_size=action_space,$/;"	variable	line:169
num_agent	/home/neale/ctf_RL/run_COMA.py	/^    num_agent=num_agent,$/;"	variable	line:170
agent_type	/home/neale/ctf_RL/run_COMA.py	/^    agent_type=agent_type,$/;"	variable	line:171
agent_type_index	/home/neale/ctf_RL/run_COMA.py	/^    agent_type_index=agent_type_index,$/;"	variable	line:172
atoms	/home/neale/ctf_RL/run_COMA.py	/^    atoms=atoms,$/;"	variable	line:173
save_path	/home/neale/ctf_RL/run_COMA.py	/^    save_path=MODEL_PATH,$/;"	variable	line:174
global_episodes	/home/neale/ctf_RL/run_COMA.py	/^global_episodes = network.initiate()$/;"	variable	line:178
writer	/home/neale/ctf_RL/run_COMA.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:182
train	/home/neale/ctf_RL/run_COMA.py	/^def train($/;"	function	line:185
rep_buffer	/home/neale/ctf_RL/run_COMA.py	/^    rep_buffer = defaultdict(list)$/;"	variable	line:195
rewards	/home/neale/ctf_RL/run_COMA.py	/^        rewards = discount(traj[2], 0.98)$/;"	variable	line:197
dataset	/home/neale/ctf_RL/run_COMA.py	/^    dataset = ($/;"	variable	line:203
run_network	/home/neale/ctf_RL/run_COMA.py	/^def run_network(states):$/;"	function	line:220
dec_batch	/home/neale/ctf_RL/run_COMA.py	/^dec_batch = []$/;"	variable	line:244
dec_batch_size	/home/neale/ctf_RL/run_COMA.py	/^dec_batch_size = 0$/;"	variable	line:245
episode_rew	/home/neale/ctf_RL/run_COMA.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:249
is_alive	/home/neale/ctf_RL/run_COMA.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:250
is_done	/home/neale/ctf_RL/run_COMA.py	/^    is_done = [False for env in range(NENV * num_agent)]$/;"	variable	line:251
trajs	/home/neale/ctf_RL/run_COMA.py	/^    trajs = [Trajectory(depth=5) for _ in range(NENV)]$/;"	variable	line:253
s1	/home/neale/ctf_RL/run_COMA.py	/^    s1 = envs.reset(config_path=game_config, policy_red=policy.Roomba,)$/;"	variable	line:256
s1	/home/neale/ctf_RL/run_COMA.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:257
cent_s1	/home/neale/ctf_RL/run_COMA.py	/^    cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:258
stime_roll	/home/neale/ctf_RL/run_COMA.py	/^    stime_roll = time.time()$/;"	variable	line:263
s0	/home/neale/ctf_RL/run_COMA.py	/^        s0 = s1$/;"	variable	line:266
a0	/home/neale/ctf_RL/run_COMA.py	/^        a0 = a1$/;"	variable	line:267
logits0	/home/neale/ctf_RL/run_COMA.py	/^        logits0 = logits1$/;"	variable	line:268
was_alive	/home/neale/ctf_RL/run_COMA.py	/^        was_alive = is_alive$/;"	variable	line:269
was_done	/home/neale/ctf_RL/run_COMA.py	/^        was_done = is_done$/;"	variable	line:270
cent_s0	/home/neale/ctf_RL/run_COMA.py	/^        cent_s0 = cent_s1$/;"	variable	line:271
is_alive	/home/neale/ctf_RL/run_COMA.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:275
s1	/home/neale/ctf_RL/run_COMA.py	/^        s1 = s1.astype(np.float32)  # Decentralize observation$/;"	variable	line:276
cent_s1	/home/neale/ctf_RL/run_COMA.py	/^        cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:277
idx	/home/neale/ctf_RL/run_COMA.py	/^            idx = env_idx * num_agent$/;"	variable	line:285
etime_roll	/home/neale/ctf_RL/run_COMA.py	/^    etime_roll = time.time()$/;"	variable	line:296
dec_batch_size	/home/neale/ctf_RL/run_COMA.py	/^    dec_batch_size = len(dec_batch) * 200 * num_agent$/;"	variable	line:300
stime_train	/home/neale/ctf_RL/run_COMA.py	/^        stime_train = time.time()$/;"	variable	line:302
log_image_on	/home/neale/ctf_RL/run_COMA.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:303
epoch	/home/neale/ctf_RL/run_COMA.py	/^            epoch=epoch,$/;"	variable	line:306
batch_size	/home/neale/ctf_RL/run_COMA.py	/^            batch_size=minibatch_size,$/;"	variable	line:307
writer	/home/neale/ctf_RL/run_COMA.py	/^            writer=writer,$/;"	variable	line:308
log	/home/neale/ctf_RL/run_COMA.py	/^            log=log_image_on,$/;"	variable	line:309
step	/home/neale/ctf_RL/run_COMA.py	/^            step=global_episodes,$/;"	variable	line:310
etime_train	/home/neale/ctf_RL/run_COMA.py	/^        etime_train = time.time()$/;"	variable	line:312
dec_batch	/home/neale/ctf_RL/run_COMA.py	/^        dec_batch = []$/;"	variable	line:313
dec_batch_size	/home/neale/ctf_RL/run_COMA.py	/^        dec_batch_size = 0$/;"	variable	line:314
log_on	/home/neale/ctf_RL/run_COMA.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:326
tag	/home/neale/ctf_RL/run_COMA.py	/^            tag = "baseline_training\/"$/;"	variable	line:329
save_on	/home/neale/ctf_RL/run_COMA.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:337
Policy	/home/neale/ctf_RL/policy/policy.py	/^class Policy:$/;"	class	line:16
__init__	/home/neale/ctf_RL/policy/policy.py	/^    def __init__(self):$/;"	member	line:28	class:Policy
gen_action	/home/neale/ctf_RL/policy/policy.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:43	class:Policy
initiate	/home/neale/ctf_RL/policy/policy.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:59	class:Policy
move_toward	/home/neale/ctf_RL/policy/policy.py	/^    def move_toward(self, start, target):$/;"	member	line:86	class:Policy
next_loc	/home/neale/ctf_RL/policy/policy.py	/^    def next_loc(self, position, move):$/;"	member	line:110	class:Policy
can_move	/home/neale/ctf_RL/policy/policy.py	/^    def can_move(self, position, move):$/;"	member	line:125	class:Policy
distance	/home/neale/ctf_RL/policy/policy.py	/^    def distance(self, start, goal, euc=False):$/;"	member	line:143	class:Policy
get_flag_loc	/home/neale/ctf_RL/policy/policy.py	/^    def get_flag_loc(self, team, friendly_flag=False):$/;"	member	line:160	class:Policy
route_astar	/home/neale/ctf_RL/policy/policy.py	/^    def route_astar(self, start, goal):$/;"	member	line:184	class:Policy
PPO	/home/neale/ctf_RL/policy/policy_RL.py	/^class PPO(Policy):$/;"	class	line:18
unstack_frame	/home/neale/ctf_RL/policy/policy_RL.py	/^    def unstack_frame(frames):$/;"	member	line:34	class:PPO
append_frame	/home/neale/ctf_RL/policy/policy_RL.py	/^    def append_frame(l:list, obj):$/;"	member	line:38	class:PPO
__init__	/home/neale/ctf_RL/policy/policy_RL.py	/^    def __init__(self):$/;"	member	line:43	class:PPO
initiate	/home/neale/ctf_RL/policy/policy_RL.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:52	class:PPO
gen_action	/home/neale/ctf_RL/policy/policy_RL.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:67	class:PPO
center_pad	/home/neale/ctf_RL/policy/policy_RL.py	/^    def center_pad(self, m, width, padder=[1,0,0,1,0,0]):$/;"	member	line:103	class:PPO
PPO_multimodes	/home/neale/ctf_RL/policy/policy_RL.py	/^class PPO_multimodes(Policy):$/;"	class	line:111
unstack_frame	/home/neale/ctf_RL/policy/policy_RL.py	/^    def unstack_frame(frames):$/;"	member	line:112	class:PPO_multimodes
append_frame	/home/neale/ctf_RL/policy/policy_RL.py	/^    def append_frame(l:list, obj):$/;"	member	line:116	class:PPO_multimodes
__init__	/home/neale/ctf_RL/policy/policy_RL.py	/^    def __init__(self):$/;"	member	line:121	class:PPO_multimodes
initiate	/home/neale/ctf_RL/policy/policy_RL.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:135	class:PPO_multimodes
gen_action	/home/neale/ctf_RL/policy/policy_RL.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:142	class:PPO_multimodes
center_pad	/home/neale/ctf_RL/policy/policy_RL.py	/^    def center_pad(self, m, width, padder=[1,0,0,1,0,0]):$/;"	member	line:172	class:PPO_multimodes
AStar	/home/neale/ctf_RL/policy/astar_flag.py	/^class AStar(Policy):$/;"	class	line:12
__init__	/home/neale/ctf_RL/policy/astar_flag.py	/^    def __init__(self):$/;"	member	line:22	class:AStar
initiate	/home/neale/ctf_RL/policy/astar_flag.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:35	class:AStar
gen_action	/home/neale/ctf_RL/policy/astar_flag.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:48	class:AStar
Fighter	/home/neale/ctf_RL/policy/fighter.py	/^class Fighter(Policy):$/;"	class	line:6
__init__	/home/neale/ctf_RL/policy/fighter.py	/^    def __init__(self):$/;"	member	line:16	class:Fighter
initiate	/home/neale/ctf_RL/policy/fighter.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:20	class:Fighter
gen_action	/home/neale/ctf_RL/policy/fighter.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:24	class:Fighter
search_nearest_enemy	/home/neale/ctf_RL/policy/fighter.py	/^    def search_nearest_enemy(self, agent, obs):$/;"	member	line:60	class:Fighter
aggr_policy	/home/neale/ctf_RL/policy/fighter.py	/^    def aggr_policy(self, agent, obs, goal, idx):$/;"	member	line:76	class:Fighter
def_policy	/home/neale/ctf_RL/policy/fighter.py	/^    def def_policy(self, agent, obs, idx):$/;"	member	line:88	class:Fighter
Random	/home/neale/ctf_RL/policy/random.py	/^class Random(Policy):$/;"	class	line:16
__init__	/home/neale/ctf_RL/policy/random.py	/^    def __init__(self):$/;"	member	line:26	class:Random
gen_action	/home/neale/ctf_RL/policy/random.py	/^    def gen_action(self, agent_list, observation, free_map=None):$/;"	member	line:30	class:Random
PolicyGen	/home/neale/ctf_RL/policy/policy_RL_indv.py	/^class PolicyGen:$/;"	class	line:24
__init__	/home/neale/ctf_RL/policy/policy_RL_indv.py	/^    def __init__(self, free_map, agent_list, model_dir='.\/model\/VANILLA', color='blue', input_name='state:0', output_name='action:0', import_scope=None):$/;"	member	line:36	class:PolicyGen
gen_action	/home/neale/ctf_RL/policy/policy_RL_indv.py	/^    def gen_action(self, agent_list, observation, free_map=None):$/;"	member	line:58	class:PolicyGen
reset_network_weight	/home/neale/ctf_RL/policy/policy_RL_indv.py	/^    def reset_network_weight(self, input_name='state:0', output_name='action:0'):$/;"	member	line:90	class:PolicyGen
reset_network	/home/neale/ctf_RL/policy/policy_RL_indv.py	/^    def reset_network(self, input_name = 'state:0', output_name = 'action:0', im_scope=None):$/;"	member	line:98	class:PolicyGen
set_directory	/home/neale/ctf_RL/policy/policy_RL_indv.py	/^    def set_directory(self, model_dir):$/;"	member	line:123	class:PolicyGen
set_deterministic	/home/neale/ctf_RL/policy/policy_RL_indv.py	/^    def set_deterministic(self, b):$/;"	member	line:126	class:PolicyGen
UAV	/home/neale/ctf_RL/policy/policy_UAV.py	/^class UAV(Policy):$/;"	class	line:17
unstack_frame	/home/neale/ctf_RL/policy/policy_UAV.py	/^    def unstack_frame(frames):$/;"	member	line:33	class:UAV
append_frame	/home/neale/ctf_RL/policy/policy_UAV.py	/^    def append_frame(l:list, obj):$/;"	member	line:37	class:UAV
__init__	/home/neale/ctf_RL/policy/policy_UAV.py	/^    def __init__(self):$/;"	member	line:42	class:UAV
reload_network	/home/neale/ctf_RL/policy/policy_UAV.py	/^    def reload_network(self, path, step=None):$/;"	member	line:58	class:UAV
initiate	/home/neale/ctf_RL/policy/policy_UAV.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:61	class:UAV
gen_action	/home/neale/ctf_RL/policy/policy_UAV.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:74	class:UAV
center_pad	/home/neale/ctf_RL/policy/policy_UAV.py	/^    def center_pad(self, m, width, padder=[0,0,0,1,0,0]):$/;"	member	line:118	class:UAV
Zeros	/home/neale/ctf_RL/policy/zeros.py	/^class Zeros(Policy):$/;"	class	line:17
gen_action	/home/neale/ctf_RL/policy/zeros.py	/^    def gen_action(self, agent_list, observation, free_map=None):$/;"	member	line:27	class:Zeros
PolicyGen	/home/neale/ctf_RL/policy/policy_grad.py	/^class PolicyGen:$/;"	class	line:20
__init__	/home/neale/ctf_RL/policy/policy_grad.py	/^    def __init__(self, free_map, agent_list):$/;"	member	line:31	class:PolicyGen
gen_action	/home/neale/ctf_RL/policy/policy_grad.py	/^    def gen_action(self, agent_list, observation, free_map=None):$/;"	member	line:66	class:PolicyGen
sigmoid	/home/neale/ctf_RL/policy/policy_grad.py	/^    def sigmoid(x):$/;"	member	line:86	class:PolicyGen
prepro	/home/neale/ctf_RL/policy/policy_grad.py	/^    def prepro(I):$/;"	member	line:89	class:PolicyGen
discount_rewards	/home/neale/ctf_RL/policy/policy_grad.py	/^    def discount_rewards(r):$/;"	member	line:93	class:PolicyGen
policy_forward	/home/neale/ctf_RL/policy/policy_grad.py	/^    def policy_forward(self,x):$/;"	member	line:103	class:PolicyGen
policy_backward	/home/neale/ctf_RL/policy/policy_grad.py	/^    def policy_backward(self, eph, epdlogp, epx):$/;"	member	line:111	class:PolicyGen
train_model	/home/neale/ctf_RL/policy/policy_grad.py	/^    def train_model(self, l, episode_number, running_reward, pos=0, first_time=True):$/;"	member	line:120	class:PolicyGen
Defense	/home/neale/ctf_RL/policy/defense.py	/^class Defense(Policy):$/;"	class	line:16
__init__	/home/neale/ctf_RL/policy/defense.py	/^    def __init__(self):$/;"	member	line:27	class:Defense
initiate	/home/neale/ctf_RL/policy/defense.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:30	class:Defense
gen_action	/home/neale/ctf_RL/policy/defense.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:40	class:Defense
flag_approach	/home/neale/ctf_RL/policy/defense.py	/^    def flag_approach(self, agent):$/;"	member	line:72	class:Defense
PolicyGen	/home/neale/ctf_RL/policy/policy_ensemble.py	/^class PolicyGen:$/;"	class	line:9
__init__	/home/neale/ctf_RL/policy/policy_ensemble.py	/^    def __init__($/;"	member	line:11	class:PolicyGen
gen_action	/home/neale/ctf_RL/policy/policy_ensemble.py	/^    def gen_action(self, agent_list, observation, free_map=None):$/;"	member	line:27	class:PolicyGen
PolicyGen	/home/neale/ctf_RL/policy/policy_A3C.py	/^class PolicyGen:$/;"	class	line:27
__init__	/home/neale/ctf_RL/policy/policy_A3C.py	/^    def __init__(self,$/;"	member	line:39	class:PolicyGen
gen_action	/home/neale/ctf_RL/policy/policy_A3C.py	/^    def gen_action(self, agent_list, observation, free_map=None, centered_obs=False):$/;"	member	line:81	class:PolicyGen
reset_network_weight	/home/neale/ctf_RL/policy/policy_A3C.py	/^    def reset_network_weight(self):$/;"	member	line:108	class:PolicyGen
Spiral	/home/neale/ctf_RL/policy/spiral.py	/^class Spiral(Policy):$/;"	class	line:6
__init__	/home/neale/ctf_RL/policy/spiral.py	/^    def __init__(self):$/;"	member	line:8	class:Spiral
initiate	/home/neale/ctf_RL/policy/spiral.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:11	class:Spiral
gen_action	/home/neale/ctf_RL/policy/spiral.py	/^    def gen_action(self, agent_list, observation, free_map=None):$/;"	member	line:27	class:Spiral
spiral	/home/neale/ctf_RL/policy/spiral.py	/^    def spiral(self, loc):$/;"	member	line:48	class:Spiral
blocking	/home/neale/ctf_RL/policy/spiral.py	/^        def blocking(position,d):$/;"	function	line:56	function:Spiral.spiral
Clone	/home/neale/ctf_RL/policy/policy_clone.py	/^class Clone:$/;"	class	line:21
__init__	/home/neale/ctf_RL/policy/policy_clone.py	/^    def __init__(self, clone=None):$/;"	member	line:32	class:Clone
PolicyGen	/home/neale/ctf_RL/policy/policy_clone.py	/^    def PolicyGen(self, free_map, agent_list):$/;"	member	line:44	class:Clone
PolicyGen	/home/neale/ctf_RL/policy/policy_scheduler.py	/^class PolicyGen:$/;"	class	line:30
__init__	/home/neale/ctf_RL/policy/policy_scheduler.py	/^    def __init__(self,$/;"	member	line:42	class:PolicyGen
gen_action	/home/neale/ctf_RL/policy/policy_scheduler.py	/^    def gen_action(self, agent_list, observation, free_map=None, centered_obs=False):$/;"	member	line:106	class:PolicyGen
reset_network_weight	/home/neale/ctf_RL/policy/policy_scheduler.py	/^    def reset_network_weight(self, input_name=None, output_name=None):$/;"	member	line:138	class:PolicyGen
Patrol	/home/neale/ctf_RL/policy/patrol.py	/^class Patrol(Policy):$/;"	class	line:19
__init__	/home/neale/ctf_RL/policy/patrol.py	/^    def __init__(self):$/;"	member	line:30	class:Patrol
initiate	/home/neale/ctf_RL/policy/patrol.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:34	class:Patrol
gen_action	/home/neale/ctf_RL/policy/patrol.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:93	class:Patrol
patrol	/home/neale/ctf_RL/policy/patrol.py	/^    def patrol(self, loc, boarder, obs):$/;"	member	line:129	class:Patrol
Roomba	/home/neale/ctf_RL/policy/roomba.py	/^class Roomba(Policy):$/;"	class	line:16
initiate	/home/neale/ctf_RL/policy/roomba.py	/^    def initiate(self, free_map, agent_list):$/;"	member	line:32	class:Roomba
gen_action	/home/neale/ctf_RL/policy/roomba.py	/^    def gen_action(self, agent_list, observation):$/;"	member	line:51	class:Roomba
policy	/home/neale/ctf_RL/policy/roomba.py	/^    def policy(self, agent, obs, agent_id):$/;"	member	line:75	class:Roomba
blocking	/home/neale/ctf_RL/policy/roomba.py	/^        def blocking(x,y,d):$/;"	function	line:98	function:Roomba.policy
obj_in_range	/home/neale/ctf_RL/policy/roomba.py	/^    def obj_in_range(self, x, y, r, obs, chn, elem=-1):$/;"	member	line:166	class:Roomba
center_pad	/home/neale/ctf_RL/policy/roomba.py	/^    def center_pad(self, m, width, padder=8):$/;"	member	line:178	class:Roomba
sc2	/home/neale/ctf_RL/Makefile	/^define sc2$/;"	macro	line:2
SMACWrapper	/home/neale/ctf_RL/SC2Wrappers.py	/^class SMACWrapper(gym.core.Wrapper):$/;"	class	line:4
__init__	/home/neale/ctf_RL/SC2Wrappers.py	/^    def __init__(self,env,**kwargs):$/;"	member	line:9	class:SMACWrapper
reset	/home/neale/ctf_RL/SC2Wrappers.py	/^    def reset(self,*args,**kwargs):$/;"	member	line:17	class:SMACWrapper
step	/home/neale/ctf_RL/SC2Wrappers.py	/^    def step(self,action,*args,**kwargs):$/;"	member	line:20	class:SMACWrapper
get_avail_actions	/home/neale/ctf_RL/SC2Wrappers.py	/^    def get_avail_actions(self):$/;"	member	line:41	class:SMACWrapper
convert_observation_to_space	/home/neale/ctf_RL/SC2Wrappers.py	/^def convert_observation_to_space(observation):$/;"	function	line:54
ActionFiltering	/home/neale/ctf_RL/SC2Wrappers.py	/^class ActionFiltering(gym.core.Wrapper):$/;"	class	line:70
__init__	/home/neale/ctf_RL/SC2Wrappers.py	/^    def __init__(self,env, **kwargs):$/;"	member	line:71	class:ActionFiltering
reset	/home/neale/ctf_RL/SC2Wrappers.py	/^    def reset(self, **kwargs):$/;"	member	line:74	class:ActionFiltering
step	/home/neale/ctf_RL/SC2Wrappers.py	/^    def step(self, action):$/;"	member	line:78	class:ActionFiltering
ActionFiltering	/home/neale/ctf_RL/SC2Wrappers.py	/^class ActionFiltering(gym.core.Wrapper):$/;"	class	line:83
__init__	/home/neale/ctf_RL/SC2Wrappers.py	/^    def __init__(self,env, **kwargs):$/;"	member	line:84	class:ActionFiltering
reset	/home/neale/ctf_RL/SC2Wrappers.py	/^    def reset(self, **kwargs):$/;"	member	line:87	class:ActionFiltering
step	/home/neale/ctf_RL/SC2Wrappers.py	/^    def step(self, action):$/;"	member	line:91	class:ActionFiltering
FrameStacking	/home/neale/ctf_RL/SC2Wrappers.py	/^class FrameStacking(gym.core.Wrapper):$/;"	class	line:96
__init__	/home/neale/ctf_RL/SC2Wrappers.py	/^    def __init__(self,env,numFrames=3,lstm=False,**kwargs):$/;"	member	line:98	class:FrameStacking
reset	/home/neale/ctf_RL/SC2Wrappers.py	/^    def reset(self,*args,**kwargs):$/;"	member	line:111	class:FrameStacking
step	/home/neale/ctf_RL/SC2Wrappers.py	/^    def step(self,action,*args,**kwargs):$/;"	member	line:119	class:FrameStacking
Stacked_state	/home/neale/ctf_RL/SC2Wrappers.py	/^class Stacked_state:$/;"	class	line:125
__init__	/home/neale/ctf_RL/SC2Wrappers.py	/^    def __init__(self, keep_frame, axis,lstm=False):$/;"	member	line:126	class:Stacked_state
initiate	/home/neale/ctf_RL/SC2Wrappers.py	/^    def initiate(self, obj):$/;"	member	line:132	class:Stacked_state
__call__	/home/neale/ctf_RL/SC2Wrappers.py	/^    def __call__(self, obj=None):$/;"	member	line:135	class:Stacked_state	file:
physical_devices	/home/neale/ctf_RL/run_multiagent_ppo.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:15
parser	/home/neale/ctf_RL/run_multiagent_ppo.py	/^parser = argparse.ArgumentParser(description="PPO trainer for convoy")$/;"	variable	line:37
args	/home/neale/ctf_RL/run_multiagent_ppo.py	/^args = parser.parse_args()$/;"	variable	line:48
PROGBAR	/home/neale/ctf_RL/run_multiagent_ppo.py	/^PROGBAR = args.silence$/;"	variable	line:50
TRAIN_NAME	/home/neale/ctf_RL/run_multiagent_ppo.py	/^TRAIN_NAME = "PPO_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:53
TRAIN_TAG	/home/neale/ctf_RL/run_multiagent_ppo.py	/^TRAIN_TAG = "PPO e2e model w Stacked Frames: " + TRAIN_NAME$/;"	variable	line:62
LOG_PATH	/home/neale/ctf_RL/run_multiagent_ppo.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:63
MODEL_PATH	/home/neale/ctf_RL/run_multiagent_ppo.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:64
MAP_PATH	/home/neale/ctf_RL/run_multiagent_ppo.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:65
GPU_CAPACITY	/home/neale/ctf_RL/run_multiagent_ppo.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:66
NENV	/home/neale/ctf_RL/run_multiagent_ppo.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:68
env_setting_path	/home/neale/ctf_RL/run_multiagent_ppo.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:71
game_config	/home/neale/ctf_RL/run_multiagent_ppo.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:72
total_episodes	/home/neale/ctf_RL/run_multiagent_ppo.py	/^total_episodes = 100000$/;"	variable	line:85
max_ep	/home/neale/ctf_RL/run_multiagent_ppo.py	/^max_ep = 200$/;"	variable	line:86
gamma	/home/neale/ctf_RL/run_multiagent_ppo.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:87
lambd	/home/neale/ctf_RL/run_multiagent_ppo.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:88
save_network_frequency	/home/neale/ctf_RL/run_multiagent_ppo.py	/^save_network_frequency = 1024$/;"	variable	line:90
save_stat_frequency	/home/neale/ctf_RL/run_multiagent_ppo.py	/^save_stat_frequency = 128$/;"	variable	line:91
save_image_frequency	/home/neale/ctf_RL/run_multiagent_ppo.py	/^save_image_frequency = 128$/;"	variable	line:92
moving_average_step	/home/neale/ctf_RL/run_multiagent_ppo.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:93
action_space	/home/neale/ctf_RL/run_multiagent_ppo.py	/^action_space = 5$/;"	variable	line:95
keep_frame	/home/neale/ctf_RL/run_multiagent_ppo.py	/^keep_frame = 1$/;"	variable	line:96
map_size	/home/neale/ctf_RL/run_multiagent_ppo.py	/^map_size = args.map_size$/;"	variable	line:97
vision_range	/home/neale/ctf_RL/run_multiagent_ppo.py	/^vision_range = map_size - 1$/;"	variable	line:98
nchannel	/home/neale/ctf_RL/run_multiagent_ppo.py	/^nchannel = 6 * keep_frame$/;"	variable	line:100
input_size	/home/neale/ctf_RL/run_multiagent_ppo.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:101
minibatch_size	/home/neale/ctf_RL/run_multiagent_ppo.py	/^minibatch_size = 128$/;"	variable	line:103
epoch	/home/neale/ctf_RL/run_multiagent_ppo.py	/^epoch = 2$/;"	variable	line:104
minimum_batch_size	/home/neale/ctf_RL/run_multiagent_ppo.py	/^minimum_batch_size = 4096$/;"	variable	line:105
log_episodic_reward	/home/neale/ctf_RL/run_multiagent_ppo.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:108
log_winrate	/home/neale/ctf_RL/run_multiagent_ppo.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:109
log_redwinrate	/home/neale/ctf_RL/run_multiagent_ppo.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:110
log_looptime	/home/neale/ctf_RL/run_multiagent_ppo.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:111
log_traintime	/home/neale/ctf_RL/run_multiagent_ppo.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:112
make_env	/home/neale/ctf_RL/run_multiagent_ppo.py	/^def make_env(map_size):$/;"	function	line:116
envs	/home/neale/ctf_RL/run_multiagent_ppo.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:120
envs	/home/neale/ctf_RL/run_multiagent_ppo.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:121
num_blue	/home/neale/ctf_RL/run_multiagent_ppo.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:122
num_red	/home/neale/ctf_RL/run_multiagent_ppo.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:123
num_agent	/home/neale/ctf_RL/run_multiagent_ppo.py	/^num_agent = num_blue  # + num_red$/;"	variable	line:124
agent_type	/home/neale/ctf_RL/run_multiagent_ppo.py	/^agent_type = []$/;"	variable	line:130
num_type	/home/neale/ctf_RL/run_multiagent_ppo.py	/^num_type = len(agent_type)$/;"	variable	line:135
agent_type_masking	/home/neale/ctf_RL/run_multiagent_ppo.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:136
agent_type_index	/home/neale/ctf_RL/run_multiagent_ppo.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:137
prev_i	/home/neale/ctf_RL/run_multiagent_ppo.py	/^prev_i = 0$/;"	variable	line:138
prev_i	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    prev_i = i$/;"	variable	line:142
agent_type_masking	/home/neale/ctf_RL/run_multiagent_ppo.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:143
network	/home/neale/ctf_RL/run_multiagent_ppo.py	/^network = Network($/;"	variable	line:146
input_shape	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    input_shape=input_size,$/;"	variable	line:147
action_size	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    action_size=action_space,$/;"	variable	line:148
agent_type	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    agent_type=agent_type,$/;"	variable	line:149
save_path	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    save_path=MODEL_PATH,$/;"	variable	line:150
global_episodes	/home/neale/ctf_RL/run_multiagent_ppo.py	/^global_episodes = network.initiate()$/;"	variable	line:154
writer	/home/neale/ctf_RL/run_multiagent_ppo.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:157
train	/home/neale/ctf_RL/run_multiagent_ppo.py	/^def train($/;"	function	line:161
train_datasets	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    train_datasets = []$/;"	variable	line:171
advantage_list	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    advantage_list = []$/;"	variable	line:173
traj_buffer_list	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:174
atype	/home/neale/ctf_RL/run_multiagent_ppo.py	/^            atype = agent_type_index[idx]$/;"	variable	line:177
reward	/home/neale/ctf_RL/run_multiagent_ppo.py	/^            reward = traj[2]$/;"	variable	line:179
critic	/home/neale/ctf_RL/run_multiagent_ppo.py	/^            critic = traj[3]$/;"	variable	line:180
_critic	/home/neale/ctf_RL/run_multiagent_ppo.py	/^            _critic = traj[5][-1]$/;"	variable	line:181
traj_buffer	/home/neale/ctf_RL/run_multiagent_ppo.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:189
traj_buffer	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:198
train_dataset	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        train_dataset = ($/;"	variable	line:199
tag	/home/neale/ctf_RL/run_multiagent_ppo.py	/^            tag = "debug\/"$/;"	variable	line:221
get_action	/home/neale/ctf_RL/run_multiagent_ppo.py	/^def get_action(states):$/;"	function	line:228
batch	/home/neale/ctf_RL/run_multiagent_ppo.py	/^batch = []$/;"	variable	line:252
num_batch	/home/neale/ctf_RL/run_multiagent_ppo.py	/^num_batch = 0$/;"	variable	line:253
episode_rew	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:258
was_alive	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:259
was_done	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:260
trajs	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    trajs = [[Trajectory(depth=6) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:263
s1	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    s1 = envs.reset($/;"	variable	line:266
map_size	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        map_size=map_size,$/;"	variable	line:267
config_path	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        config_path=game_config,$/;"	variable	line:269
policy_red	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        policy_red=policy.Roomba,$/;"	variable	line:270
s1	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:272
stime_roll	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    stime_roll = time.time()$/;"	variable	line:276
s0	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        s0 = s1$/;"	variable	line:278
p0	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        p0 = p1$/;"	variable	line:280
s1	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        s1 = s1.astype(np.float32)$/;"	variable	line:283
is_alive	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:284
idx	/home/neale/ctf_RL/run_multiagent_ppo.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:292
was_alive	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        was_alive = is_alive$/;"	variable	line:297
was_done	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        was_done = done$/;"	variable	line:298
etime_roll	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    etime_roll = time.time()$/;"	variable	line:302
num_batch	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    num_batch = len(batch) * 200 * num_agent$/;"	variable	line:305
stime_train	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        stime_train = time.time()$/;"	variable	line:307
log_image_on	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:308
etime_train	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        etime_train = time.time()$/;"	variable	line:319
batch	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        batch = []$/;"	variable	line:320
num_batch	/home/neale/ctf_RL/run_multiagent_ppo.py	/^        num_batch = 0$/;"	variable	line:321
log_on	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:333
tag	/home/neale/ctf_RL/run_multiagent_ppo.py	/^            tag = "baseline_training\/"$/;"	variable	line:336
save_on	/home/neale/ctf_RL/run_multiagent_ppo.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:350
V4PPO	/home/neale/ctf_RL/network/SC2.py	/^class V4PPO(tf.keras.Model):$/;"	class	line:19
__init__	/home/neale/ctf_RL/network/SC2.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:21	class:V4PPO
print_summary	/home/neale/ctf_RL/network/SC2.py	/^    def print_summary(self):$/;"	member	line:40	class:V4PPO
call	/home/neale/ctf_RL/network/SC2.py	/^    def call(self, inputs):$/;"	member	line:43	class:V4PPO
loss	/home/neale/ctf_RL/network/SC2.py	/^def loss(model, state, old_log_logit, action, advantage, td_target, old_value,$/;"	function	line:61
get_gradient	/home/neale/ctf_RL/network/SC2.py	/^def get_gradient(model, inputs, **hyperparameters):$/;"	function	line:110
train	/home/neale/ctf_RL/network/SC2.py	/^def train(model, optimizer, inputs, **hyperparameters):$/;"	function	line:117
V4PPO	/home/neale/ctf_RL/network/PPO.py	/^class V4PPO(tf.keras.Model):$/;"	class	line:18
__init__	/home/neale/ctf_RL/network/PPO.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:20	class:V4PPO
print_summary	/home/neale/ctf_RL/network/PPO.py	/^    def print_summary(self):$/;"	member	line:39	class:V4PPO
call	/home/neale/ctf_RL/network/PPO.py	/^    def call(self, inputs):$/;"	member	line:42	class:V4PPO
loss	/home/neale/ctf_RL/network/PPO.py	/^def loss(model, state, old_log_logit, action, advantage, td_target, old_value,$/;"	function	line:58
get_gradient	/home/neale/ctf_RL/network/PPO.py	/^def get_gradient(model, inputs, **hyperparameters):$/;"	function	line:107
train	/home/neale/ctf_RL/network/PPO.py	/^def train(model, optimizer, inputs, **hyperparameters):$/;"	function	line:114
Spatial_VAE	/home/neale/ctf_RL/network/model_V3.py	/^class Spatial_VAE(tf.keras.Model):$/;"	class	line:30
__init__	/home/neale/ctf_RL/network/model_V3.py	/^    def __init__(self, input_shape, input_placeholder, latent_dim=128, lr=1e-4, num_stack=4, scope=None):$/;"	member	line:32	class:Spatial_VAE
build_pipeline	/home/neale/ctf_RL/network/model_V3.py	/^    def build_pipeline(self, input_placeholder, pipe_name='pipe'):$/;"	member	line:113	class:Spatial_VAE
build_external_loss	/home/neale/ctf_RL/network/model_V3.py	/^    def build_external_loss(self, loss, pipe_name='extern_loss'):$/;"	member	line:119	class:Spatial_VAE
sample	/home/neale/ctf_RL/network/model_V3.py	/^    def sample(self, eps=None):$/;"	member	line:125	class:Spatial_VAE
encode	/home/neale/ctf_RL/network/model_V3.py	/^    def encode(self, x):$/;"	member	line:130	class:Spatial_VAE
reparameterize	/home/neale/ctf_RL/network/model_V3.py	/^    def reparameterize(self, mean, logvar):$/;"	member	line:134	class:Spatial_VAE
decode	/home/neale/ctf_RL/network/model_V3.py	/^    def decode(self, z, apply_tanh=False):$/;"	member	line:139	class:Spatial_VAE
log_normal_pdf	/home/neale/ctf_RL/network/model_V3.py	/^    def log_normal_pdf(self, sample, mean, logvar, raxis=1):$/;"	member	line:147	class:Spatial_VAE
Spatial_VQVAE	/home/neale/ctf_RL/network/model_V3.py	/^class Spatial_VQVAE(tf.keras.Model):$/;"	class	line:154
__init__	/home/neale/ctf_RL/network/model_V3.py	/^    def __init__(self, input_shape, input_placeholder, latent_dim=256, lr=1e-4, num_stack=4, scope=None, beta=0.1):$/;"	member	line:156	class:Spatial_VQVAE
build_pipeline	/home/neale/ctf_RL/network/model_V3.py	/^    def build_pipeline(self, input_placeholder, pipe_name='pipe'):$/;"	member	line:249	class:Spatial_VQVAE
build_external_loss	/home/neale/ctf_RL/network/model_V3.py	/^    def build_external_loss(self, loss, pipe_name='extern_loss'):$/;"	member	line:253	class:Spatial_VQVAE
sample	/home/neale/ctf_RL/network/model_V3.py	/^    def sample(self, eps=None):$/;"	member	line:259	class:Spatial_VQVAE
decode	/home/neale/ctf_RL/network/model_V3.py	/^    def decode(self, z):$/;"	member	line:264	class:Spatial_VQVAE
Temporal_VAE	/home/neale/ctf_RL/network/model_V3.py	/^class Temporal_VAE(tf.keras.Model):$/;"	class	line:268
__init__	/home/neale/ctf_RL/network/model_V3.py	/^    def __init__(self, input_shape, input_placeholder,$/;"	member	line:270	class:Temporal_VAE
build_pipeline	/home/neale/ctf_RL/network/model_V3.py	/^    def build_pipeline(self, input_placeholder, pipe_name='pipe'):$/;"	member	line:341	class:Temporal_VAE
build_external_loss	/home/neale/ctf_RL/network/model_V3.py	/^    def build_external_loss(self, loss, pipe_name='extern_loss'):$/;"	member	line:347	class:Temporal_VAE
sample	/home/neale/ctf_RL/network/model_V3.py	/^    def sample(self, eps=None):$/;"	member	line:353	class:Temporal_VAE
encode	/home/neale/ctf_RL/network/model_V3.py	/^    def encode(self, x):$/;"	member	line:358	class:Temporal_VAE
reparameterize	/home/neale/ctf_RL/network/model_V3.py	/^    def reparameterize(self, mean, logvar):$/;"	member	line:362	class:Temporal_VAE
decode	/home/neale/ctf_RL/network/model_V3.py	/^    def decode(self, z, apply_tanh=False):$/;"	member	line:367	class:Temporal_VAE
log_normal_pdf	/home/neale/ctf_RL/network/model_V3.py	/^    def log_normal_pdf(self, sample, mean, logvar, raxis=1):$/;"	member	line:375	class:Temporal_VAE
build_network	/home/neale/ctf_RL/network/model_V3.py	/^def build_network(input_hold, keep_dim=4):$/;"	function	line:381
input_hold	/home/neale/ctf_RL/network/model_V3.py	/^    input_hold = tf.placeholder(dtype=tf.float32, shape=[None, 39, 39, 24])$/;"	variable	line:417
svae	/home/neale/ctf_RL/network/model_V3.py	/^    svae = Spatial_VAE((39,39,6), input_hold, scope='spatial_vae', lr=1e-4, num_stack=4)$/;"	variable	line:418
spatial_matrix	/home/neale/ctf_RL/network/model_V3.py	/^    spatial_matrix = tf.stack(svae.z_list, axis=-1)  # (None, 128, 4)$/;"	variable	line:420
spatial_matrix	/home/neale/ctf_RL/network/model_V3.py	/^    spatial_matrix = tf.stop_gradient(spatial_matrix)$/;"	variable	line:421
tvae	/home/neale/ctf_RL/network/model_V3.py	/^    tvae = Temporal_VAE((128, 4), spatial_matrix, scope='temporal_vae', lr=1e-4)$/;"	variable	line:422
vq	/home/neale/ctf_RL/network/model_V3.py	/^    vq = Spatial_VQVAE((39,39,6), input_hold, scope='vqvae')$/;"	variable	line:463
build_network	/home/neale/ctf_RL/network/attention_ctf.py	/^def build_network(input_hold, output_size=128, return_layers=False):$/;"	function	line:17
CVAE	/home/neale/ctf_RL/network/attention_ctf.py	/^class CVAE(tf.keras.Model):$/;"	class	line:59
__init__	/home/neale/ctf_RL/network/attention_ctf.py	/^    def __init__(self, latent_dim, sess=None):$/;"	member	line:60	class:CVAE
sample	/home/neale/ctf_RL/network/attention_ctf.py	/^    def sample(self, eps=None):$/;"	member	line:102	class:CVAE
encode	/home/neale/ctf_RL/network/attention_ctf.py	/^    def encode(self, x):$/;"	member	line:107	class:CVAE
reparameterize	/home/neale/ctf_RL/network/attention_ctf.py	/^    def reparameterize(self, mean, logvar):$/;"	member	line:111	class:CVAE
decode	/home/neale/ctf_RL/network/attention_ctf.py	/^    def decode(self, z, apply_sigmoid=False):$/;"	member	line:115	class:CVAE
PPO_SF	/home/neale/ctf_RL/network/model_SF.py	/^class PPO_SF(tf.keras.Model):$/;"	class	line:17
__init__	/home/neale/ctf_RL/network/model_SF.py	/^    def __init__(self, action_size=5, phi_n=16, trainable=True, lr=1e-4, eps=0.2, entropy_beta=0.01, name='PPO'):$/;"	member	line:19	class:PPO_SF
call	/home/neale/ctf_RL/network/model_SF.py	/^    def call(self, inputs):$/;"	member	line:51	class:PPO_SF
get_feature_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_feature_variables(self):$/;"	member	line:77	class:PPO_SF
get_actor_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_actor_variables(self):$/;"	member	line:81	class:PPO_SF
get_phi_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_phi_variables(self):$/;"	member	line:85	class:PPO_SF
get_psi_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_psi_variables(self):$/;"	member	line:89	class:PPO_SF
build_loss	/home/neale/ctf_RL/network/model_SF.py	/^    def build_loss(self, old_log_logit, action, advantage, td_target, result):$/;"	member	line:92	class:PPO_SF
_log	/home/neale/ctf_RL/network/model_SF.py	/^        def _log(val):$/;"	function	line:93	function:PPO_SF.build_loss
PPO_SF_softmax	/home/neale/ctf_RL/network/model_SF.py	/^class PPO_SF_softmax(tf.keras.Model):$/;"	class	line:136
__init__	/home/neale/ctf_RL/network/model_SF.py	/^    def __init__(self, action_size=5, trainable=True, lr=1e-4, eps=0.2, entropy_beta=0.01, name='PPO'):$/;"	member	line:138	class:PPO_SF_softmax
call	/home/neale/ctf_RL/network/model_SF.py	/^    def call(self, inputs):$/;"	member	line:172	class:PPO_SF_softmax
get_feature_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_feature_variables(self):$/;"	member	line:216	class:PPO_SF_softmax
get_actor_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_actor_variables(self):$/;"	member	line:220	class:PPO_SF_softmax
get_phi_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_phi_variables(self):$/;"	member	line:224	class:PPO_SF_softmax
get_psi_variables	/home/neale/ctf_RL/network/model_SF.py	/^    def get_psi_variables(self):$/;"	member	line:228	class:PPO_SF_softmax
build_loss	/home/neale/ctf_RL/network/model_SF.py	/^    def build_loss(self, old_log_logit, action, advantage, td_target, result):$/;"	member	line:231	class:PPO_SF_softmax
_log	/home/neale/ctf_RL/network/model_SF.py	/^        def _log(val):$/;"	function	line:232	function:PPO_SF_softmax.build_loss
network	/home/neale/ctf_RL/network/model_SF.py	/^    network = PPO_SF()$/;"	variable	line:278	class:PPO_SF_softmax
first_batch	/home/neale/ctf_RL/network/model_SF.py	/^    first_batch = tf.zeros((1,39,39,12))$/;"	variable	line:280	class:PPO_SF_softmax
z	/home/neale/ctf_RL/network/model_SF.py	/^    z = network(first_batch)$/;"	variable	line:281	class:PPO_SF_softmax
V4TD	/home/neale/ctf_RL/network/TD.py	/^class V4TD(tf.keras.Model):$/;"	class	line:20
__init__	/home/neale/ctf_RL/network/TD.py	/^    def __init__(self, input_shape, action_size, atoms,$/;"	member	line:22	class:V4TD
print_summary	/home/neale/ctf_RL/network/TD.py	/^    def print_summary(self):$/;"	member	line:38	class:V4TD
inference_network	/home/neale/ctf_RL/network/TD.py	/^    def inference_network(self, inputs):$/;"	member	line:41	class:V4TD
generative_network	/home/neale/ctf_RL/network/TD.py	/^    def generative_network(self, q_z):$/;"	member	line:53	class:V4TD
call	/home/neale/ctf_RL/network/TD.py	/^    def call(self, inputs):$/;"	member	line:59	class:V4TD
loss_decoder	/home/neale/ctf_RL/network/TD.py	/^def loss_decoder(model, state, beta_kl=1e-4, training=True):$/;"	function	line:74
loss	/home/neale/ctf_RL/network/TD.py	/^def loss(model, state, reward, done, next_state,$/;"	function	line:85
train	/home/neale/ctf_RL/network/TD.py	/^def train(model, loss, optimizer, inputs, **hyperparameters):$/;"	function	line:100
VDNNet	/home/neale/ctf_RL/network/VDN.py	/^class VDNNet(tf.keras.Model):$/;"	class	line:18
__init__	/home/neale/ctf_RL/network/VDN.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:20	class:VDNNet
call	/home/neale/ctf_RL/network/VDN.py	/^    def call(self, inputs):$/;"	member	line:35	class:VDNNet
loss_critic	/home/neale/ctf_RL/network/VDN.py	/^def loss_critic(models, state, td_target, agent_type_index, gamma=0.98):$/;"	function	line:53
train_critic	/home/neale/ctf_RL/network/VDN.py	/^def train_critic(models, optimizer, inputs):$/;"	function	line:71
layer_normalization	/home/neale/ctf_RL/network/core.py	/^def layer_normalization(x):$/;"	function	line:3
V4PG	/home/neale/ctf_RL/network/PG.py	/^class V4PG(tf.keras.Model):$/;"	class	line:18
__init__	/home/neale/ctf_RL/network/PG.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:20	class:V4PG
print_summary	/home/neale/ctf_RL/network/PG.py	/^    def print_summary(self):$/;"	member	line:34	class:V4PG
call	/home/neale/ctf_RL/network/PG.py	/^    def call(self, inputs):$/;"	member	line:37	class:V4PG
loss	/home/neale/ctf_RL/network/PG.py	/^def loss(model, state, old_log_logit, action, advantage,$/;"	function	line:47
get_gradient	/home/neale/ctf_RL/network/PG.py	/^def get_gradient(model, inputs, **hyperparameters):$/;"	function	line:81
train	/home/neale/ctf_RL/network/PG.py	/^def train(model, optimizer, inputs, **hyperparameters):$/;"	function	line:88
V4	/home/neale/ctf_RL/network/model_V4_40.py	/^class V4(tf.keras.Model):$/;"	class	line:18
STATIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_40.py	/^    STATIC_CHANNEL = [0,1,3]$/;"	variable	line:19	class:V4
DYNAMIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_40.py	/^    DYNAMIC_CHANNEL = [2,4,5]$/;"	variable	line:20	class:V4
LATENT_DIM	/home/neale/ctf_RL/network/model_V4_40.py	/^    LATENT_DIM = 128$/;"	variable	line:21	class:V4
__init__	/home/neale/ctf_RL/network/model_V4_40.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:24	class:V4
print_summary	/home/neale/ctf_RL/network/model_V4_40.py	/^    def print_summary(self):$/;"	member	line:53	class:V4
call	/home/neale/ctf_RL/network/model_V4_40.py	/^    def call(self, inputs):$/;"	member	line:57	class:V4
V4INV	/home/neale/ctf_RL/network/model_V4_40.py	/^class V4INV(tf.keras.Model):$/;"	class	line:69
__init__	/home/neale/ctf_RL/network/model_V4_40.py	/^    def __init__(self, trainable=True, name='FeatureNN_Inverse'):$/;"	member	line:71	class:V4INV
print_summary	/home/neale/ctf_RL/network/model_V4_40.py	/^    def print_summary(self):$/;"	member	line:91	class:V4INV
call	/home/neale/ctf_RL/network/model_V4_40.py	/^    def call(self, inputs):$/;"	member	line:95	class:V4INV
V4Decentral	/home/neale/ctf_RL/network/model_V4_40.py	/^class V4Decentral(tf.keras.Model):$/;"	class	line:106
STATIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_40.py	/^    STATIC_CHANNEL = [0,1,3]$/;"	variable	line:107	class:V4Decentral
DYNAMIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_40.py	/^    DYNAMIC_CHANNEL = [2,4,5]$/;"	variable	line:108	class:V4Decentral
LATENT_DIM	/home/neale/ctf_RL/network/model_V4_40.py	/^    LATENT_DIM = 128$/;"	variable	line:109	class:V4Decentral
__init__	/home/neale/ctf_RL/network/model_V4_40.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:112	class:V4Decentral
print_summary	/home/neale/ctf_RL/network/model_V4_40.py	/^    def print_summary(self):$/;"	member	line:140	class:V4Decentral
call	/home/neale/ctf_RL/network/model_V4_40.py	/^    def call(self, inputs):$/;"	member	line:144	class:V4Decentral
V4INVDecentral	/home/neale/ctf_RL/network/model_V4_40.py	/^class V4INVDecentral(tf.keras.Model):$/;"	class	line:157
__init__	/home/neale/ctf_RL/network/model_V4_40.py	/^    def __init__(self, trainable=True, name='FeatureNN_Inverse'):$/;"	member	line:159	class:V4INVDecentral
print_summary	/home/neale/ctf_RL/network/model_V4_40.py	/^    def print_summary(self):$/;"	member	line:178	class:V4INVDecentral
call	/home/neale/ctf_RL/network/model_V4_40.py	/^    def call(self, inputs):$/;"	member	line:182	class:V4INVDecentral
physical_devices	/home/neale/ctf_RL/network/model_V4_40.py	/^    physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:194	class:V4INVDecentral
config	/home/neale/ctf_RL/network/model_V4_40.py	/^    config = tf.config.experimental.set_memory_growth(physical_devices[0], True)$/;"	variable	line:196	class:V4INVDecentral
sample_size	/home/neale/ctf_RL/network/model_V4_40.py	/^    sample_size = 32$/;"	variable	line:198	class:V4INVDecentral
image_shape	/home/neale/ctf_RL/network/model_V4_40.py	/^    image_shape = [40,40,6]$/;"	variable	line:199	class:V4INVDecentral
sample_shape	/home/neale/ctf_RL/network/model_V4_40.py	/^    sample_shape = [sample_size]+image_shape$/;"	variable	line:200	class:V4INVDecentral
latent_size	/home/neale/ctf_RL/network/model_V4_40.py	/^    latent_size = 128$/;"	variable	line:201	class:V4INVDecentral
latent_shape	/home/neale/ctf_RL/network/model_V4_40.py	/^    latent_shape = [sample_size]+[latent_size]$/;"	variable	line:202	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4_40.py	/^    model = V4(image_shape, 5)$/;"	variable	line:205	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4_40.py	/^    sample = np.random.random(sample_shape).astype(np.float32)$/;"	variable	line:208	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4_40.py	/^    output = model(sample)$/;"	variable	line:209	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4_40.py	/^    model = V4INV()$/;"	variable	line:214	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4_40.py	/^    sample = np.random.random(latent_shape).astype(np.float32)$/;"	variable	line:217	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4_40.py	/^    output = model(sample)$/;"	variable	line:218	class:V4INVDecentral
grouped_conv	/home/neale/ctf_RL/network/custom_layers.py	/^def grouped_conv(input_layer, n_group, axis=-1, **kwargs):$/;"	function	line:4
grouped_sep_conv	/home/neale/ctf_RL/network/custom_layers.py	/^def grouped_sep_conv(input_layer, n_group, axis=-1, **kwargs):$/;"	function	line:14
PPO_LSTM_V1	/home/neale/ctf_RL/network/model_lstm.py	/^class PPO_LSTM_V1(tf.keras.Model):$/;"	class	line:18
__init__	/home/neale/ctf_RL/network/model_lstm.py	/^    def __init__(self, action_size=5, trainable=True, lr=1e-4, eps=0.2, entropy_beta=0.01, critic_beta=0.5, name='PPO'):$/;"	member	line:20	class:PPO_LSTM_V1
call	/home/neale/ctf_RL/network/model_lstm.py	/^    def call(self, inputs):$/;"	member	line:53	class:PPO_LSTM_V1
reset_lstm	/home/neale/ctf_RL/network/model_lstm.py	/^    def reset_lstm(self):$/;"	member	line:87	class:PPO_LSTM_V1
build_loss	/home/neale/ctf_RL/network/model_lstm.py	/^    def build_loss(self, old_log_logit, action, advantage, td_target):$/;"	member	line:91	class:PPO_LSTM_V1
_log	/home/neale/ctf_RL/network/model_lstm.py	/^        def _log(val):$/;"	function	line:92	function:PPO_LSTM_V1.build_loss
network	/home/neale/ctf_RL/network/model_lstm.py	/^    network = PPO_LSTM_V1()$/;"	variable	line:128	class:PPO_LSTM_V1
first_batch	/home/neale/ctf_RL/network/model_lstm.py	/^    first_batch = tf.zeros((1,4,39,39,6))$/;"	variable	line:130	class:PPO_LSTM_V1
z	/home/neale/ctf_RL/network/model_lstm.py	/^    z = network(first_batch)$/;"	variable	line:131	class:PPO_LSTM_V1
soft_attention	/home/neale/ctf_RL/network/attention.py	/^def soft_attention(h_prev, a, num_input, hidden_size):$/;"	function	line:7
weight_variable	/home/neale/ctf_RL/network/attention.py	/^    def weight_variable(name, shape):$/;"	function	line:8	function:soft_attention
self_attention	/home/neale/ctf_RL/network/attention.py	/^def self_attention(data, hidden_dim, output_dim, residual=True):$/;"	function	line:25
scaled_dot_product	/home/neale/ctf_RL/network/attention.py	/^    def scaled_dot_product(Q, K, scaled_=True, masked_=False):$/;"	function	line:30	function:self_attention
non_local_nn_2d	/home/neale/ctf_RL/network/attention.py	/^def non_local_nn_2d(data, hidden_dim=None, pool=False, use_dense=False, normalize=False, name='non_local', return_layers=False):$/;"	function	line:57
scaled_dot_product	/home/neale/ctf_RL/network/attention.py	/^        def scaled_dot_product(Q, K, scaled_=True, masked_=False):$/;"	function	line:62	function:non_local_nn_2d
non_local_nn_3d	/home/neale/ctf_RL/network/attention.py	/^def non_local_nn_3d(data, hidden_dim, pool=False, name='non_local'):$/;"	function	line:121
scaled_dot_product	/home/neale/ctf_RL/network/attention.py	/^        def scaled_dot_product(Q, K, scaled_=True, masked_=False):$/;"	function	line:125	function:non_local_nn_3d
multiheaded_attention	/home/neale/ctf_RL/network/attention.py	/^def multiheaded_attention(data, hidden_dim, att_output_dim, output_dim, num_attention_layer=8):$/;"	function	line:166
Non_local_nn	/home/neale/ctf_RL/network/attention.py	/^class Non_local_nn(tf.keras.layers.Layer):$/;"	class	line:176
__init__	/home/neale/ctf_RL/network/attention.py	/^    def __init__(self, channels, pool=False, residual=True, train_gamma=False, name='non_local'):$/;"	member	line:177	class:Non_local_nn
build	/home/neale/ctf_RL/network/attention.py	/^    def build(self, input_shape):$/;"	member	line:185	class:Non_local_nn
call	/home/neale/ctf_RL/network/attention.py	/^    def call(self, input_layer, normalize=True):$/;"	member	line:195	class:Non_local_nn
V4COMA_d	/home/neale/ctf_RL/network/counterfactual.py	/^class V4COMA_d(tf.keras.Model):$/;"	class	line:21
__init__	/home/neale/ctf_RL/network/counterfactual.py	/^    def __init__(self, input_shape, action_size=5, atoms=128,$/;"	member	line:23	class:V4COMA_d
print_summary	/home/neale/ctf_RL/network/counterfactual.py	/^    def print_summary(self):$/;"	member	line:41	class:V4COMA_d
call	/home/neale/ctf_RL/network/counterfactual.py	/^    def call(self, inputs):$/;"	member	line:44	class:V4COMA_d
V4COMA_c	/home/neale/ctf_RL/network/counterfactual.py	/^class V4COMA_c(tf.keras.Model):$/;"	class	line:58
__init__	/home/neale/ctf_RL/network/counterfactual.py	/^    def __init__(self, input_shape, indiv_models, agent_type_index, num_agent_type, action_size=5, atoms=128,$/;"	member	line:60	class:V4COMA_c
call	/home/neale/ctf_RL/network/counterfactual.py	/^    def call(self, env_state, indiv_states, indiv_actions):$/;"	member	line:79	class:V4COMA_c
loss	/home/neale/ctf_RL/network/counterfactual.py	/^def loss(cent_model, dec_models, env_states, metastates, metaactions, rewards):$/;"	function	line:107
train	/home/neale/ctf_RL/network/counterfactual.py	/^def train(cent_model, dec_models, optimizer, inputs):$/;"	function	line:144
V2Dist	/home/neale/ctf_RL/network/C51.py	/^class V2Dist(tf.keras.Model):$/;"	class	line:16
__init__	/home/neale/ctf_RL/network/C51.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:18	class:V2Dist
call	/home/neale/ctf_RL/network/C51.py	/^    def call(self, inputs):$/;"	member	line:52	class:V2Dist
_log	/home/neale/ctf_RL/network/C51.py	/^def _log(val):$/;"	function	line:64
get_action	/home/neale/ctf_RL/network/C51.py	/^def get_action(model, state):$/;"	function	line:67
loss	/home/neale/ctf_RL/network/C51.py	/^def loss(model, state, action, td_target, advantage, old_log_logit, reward, done, next_state,$/;"	function	line:74
train	/home/neale/ctf_RL/network/C51.py	/^def train(model, optimizer, inputs, **hyperparameters):$/;"	function	line:119
Decentral	/home/neale/ctf_RL/network/CVDC_model2.py	/^class Decentral(tf.keras.Model):$/;"	class	line:22
__init__	/home/neale/ctf_RL/network/CVDC_model2.py	/^    def __init__(self, input_shape, action_size=5, atoms=512,$/;"	member	line:24	class:Decentral
print_summary	/home/neale/ctf_RL/network/CVDC_model2.py	/^    def print_summary(self):$/;"	member	line:91	class:Decentral
action_call	/home/neale/ctf_RL/network/CVDC_model2.py	/^    def action_call(self, obs):  # Short operation for forward pass$/;"	member	line:95	class:Decentral
call	/home/neale/ctf_RL/network/CVDC_model2.py	/^    def call(self, inputs): # Full Operation of the method$/;"	member	line:138	class:Decentral
Central	/home/neale/ctf_RL/network/CVDC_model2.py	/^class Central(tf.keras.Model):$/;"	class	line:213
__init__	/home/neale/ctf_RL/network/CVDC_model2.py	/^    def __init__(self, input_shape, atoms,$/;"	member	line:215	class:Central
print_summary	/home/neale/ctf_RL/network/CVDC_model2.py	/^    def print_summary(self):$/;"	member	line:231	class:Central
call	/home/neale/ctf_RL/network/CVDC_model2.py	/^    def call(self, inputs):$/;"	member	line:234	class:Central
loss_central	/home/neale/ctf_RL/network/CVDC_model2.py	/^def loss_central(model, state, td_target_c):$/;"	function	line:251
loss_decentral_critic_only	/home/neale/ctf_RL/network/CVDC_model2.py	/^def loss_decentral_critic_only(model, state, action, td_target_psi, td_target_c):$/;"	function	line:264
loss_ppo	/home/neale/ctf_RL/network/CVDC_model2.py	/^def loss_ppo(model, state, old_log_logit, action, old_value, td_target, advantage, td_target_c, rewards, next_state,$/;"	function	line:276
get_gradient	/home/neale/ctf_RL/network/CVDC_model2.py	/^def get_gradient(model, loss, inputs, hyperparameters={}):$/;"	function	line:364
train	/home/neale/ctf_RL/network/CVDC_model2.py	/^def train(model, loss, optimizer, inputs, hyperparameters={}):$/;"	function	line:371
V4	/home/neale/ctf_RL/network/model_V4.py	/^class V4(tf.keras.Model):$/;"	class	line:18
STATIC_CHANNEL	/home/neale/ctf_RL/network/model_V4.py	/^    STATIC_CHANNEL = [0,1,3]$/;"	variable	line:19	class:V4
DYNAMIC_CHANNEL	/home/neale/ctf_RL/network/model_V4.py	/^    DYNAMIC_CHANNEL = [2,4,5]$/;"	variable	line:20	class:V4
LATENT_DIM	/home/neale/ctf_RL/network/model_V4.py	/^    LATENT_DIM = 128$/;"	variable	line:21	class:V4
__init__	/home/neale/ctf_RL/network/model_V4.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:24	class:V4
print_summary	/home/neale/ctf_RL/network/model_V4.py	/^    def print_summary(self):$/;"	member	line:50	class:V4
call	/home/neale/ctf_RL/network/model_V4.py	/^    def call(self, inputs):$/;"	member	line:54	class:V4
V4INV	/home/neale/ctf_RL/network/model_V4.py	/^class V4INV(tf.keras.Model):$/;"	class	line:66
__init__	/home/neale/ctf_RL/network/model_V4.py	/^    def __init__(self, trainable=True, name='FeatureNN_Inverse'):$/;"	member	line:68	class:V4INV
print_summary	/home/neale/ctf_RL/network/model_V4.py	/^    def print_summary(self):$/;"	member	line:88	class:V4INV
call	/home/neale/ctf_RL/network/model_V4.py	/^    def call(self, inputs):$/;"	member	line:92	class:V4INV
V4Decentral	/home/neale/ctf_RL/network/model_V4.py	/^class V4Decentral(tf.keras.Model):$/;"	class	line:103
STATIC_CHANNEL	/home/neale/ctf_RL/network/model_V4.py	/^    STATIC_CHANNEL = [0,1,3]$/;"	variable	line:104	class:V4Decentral
DYNAMIC_CHANNEL	/home/neale/ctf_RL/network/model_V4.py	/^    DYNAMIC_CHANNEL = [2,4,5]$/;"	variable	line:105	class:V4Decentral
LATENT_DIM	/home/neale/ctf_RL/network/model_V4.py	/^    LATENT_DIM = 128$/;"	variable	line:106	class:V4Decentral
__init__	/home/neale/ctf_RL/network/model_V4.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:109	class:V4Decentral
print_summary	/home/neale/ctf_RL/network/model_V4.py	/^    def print_summary(self):$/;"	member	line:135	class:V4Decentral
call	/home/neale/ctf_RL/network/model_V4.py	/^    def call(self, inputs):$/;"	member	line:139	class:V4Decentral
V4INVDecentral	/home/neale/ctf_RL/network/model_V4.py	/^class V4INVDecentral(tf.keras.Model):$/;"	class	line:152
__init__	/home/neale/ctf_RL/network/model_V4.py	/^    def __init__(self, trainable=True, name='FeatureNN_Inverse', **kwargs):$/;"	member	line:154	class:V4INVDecentral
print_summary	/home/neale/ctf_RL/network/model_V4.py	/^    def print_summary(self):$/;"	member	line:174	class:V4INVDecentral
call	/home/neale/ctf_RL/network/model_V4.py	/^    def call(self, inputs):$/;"	member	line:178	class:V4INVDecentral
physical_devices	/home/neale/ctf_RL/network/model_V4.py	/^    physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:190	class:V4INVDecentral
config	/home/neale/ctf_RL/network/model_V4.py	/^    config = tf.config.experimental.set_memory_growth(physical_devices[0], True)$/;"	variable	line:192	class:V4INVDecentral
sample_size	/home/neale/ctf_RL/network/model_V4.py	/^    sample_size = 32$/;"	variable	line:195	class:V4INVDecentral
map_size	/home/neale/ctf_RL/network/model_V4.py	/^    map_size = 20$/;"	variable	line:196	class:V4INVDecentral
image_shape	/home/neale/ctf_RL/network/model_V4.py	/^    image_shape = [map_size,map_size,6]$/;"	variable	line:197	class:V4INVDecentral
sample_shape	/home/neale/ctf_RL/network/model_V4.py	/^    sample_shape = [sample_size]+image_shape$/;"	variable	line:198	class:V4INVDecentral
latent_size	/home/neale/ctf_RL/network/model_V4.py	/^    latent_size = 128$/;"	variable	line:199	class:V4INVDecentral
latent_shape	/home/neale/ctf_RL/network/model_V4.py	/^    latent_shape = [sample_size]+[latent_size]$/;"	variable	line:200	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4.py	/^    model = V4(image_shape, 5)$/;"	variable	line:203	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4.py	/^    sample = np.random.random(sample_shape).astype(np.float32)$/;"	variable	line:206	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4.py	/^    output = model(sample)$/;"	variable	line:207	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4.py	/^    model = V4INV()$/;"	variable	line:212	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4.py	/^    sample = np.random.random(latent_shape).astype(np.float32)$/;"	variable	line:215	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4.py	/^    output = model(sample)$/;"	variable	line:216	class:V4INVDecentral
sample_size	/home/neale/ctf_RL/network/model_V4.py	/^    sample_size = 32$/;"	variable	line:221	class:V4INVDecentral
map_size	/home/neale/ctf_RL/network/model_V4.py	/^    map_size = map_size*2-1$/;"	variable	line:222	class:V4INVDecentral
image_shape	/home/neale/ctf_RL/network/model_V4.py	/^    image_shape = [map_size,map_size,6]$/;"	variable	line:223	class:V4INVDecentral
sample_shape	/home/neale/ctf_RL/network/model_V4.py	/^    sample_shape = [sample_size]+image_shape$/;"	variable	line:224	class:V4INVDecentral
latent_size	/home/neale/ctf_RL/network/model_V4.py	/^    latent_size = 128$/;"	variable	line:225	class:V4INVDecentral
latent_shape	/home/neale/ctf_RL/network/model_V4.py	/^    latent_shape = [sample_size]+[latent_size]$/;"	variable	line:226	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4.py	/^    model = V4Decentral(image_shape, 5)$/;"	variable	line:229	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4.py	/^    sample = np.random.random(sample_shape).astype(np.float32)$/;"	variable	line:232	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4.py	/^    output = model(sample)$/;"	variable	line:233	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4.py	/^    model = V4INVDecentral()$/;"	variable	line:238	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4.py	/^    sample = np.random.random(latent_shape).astype(np.float32)$/;"	variable	line:241	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4.py	/^    output = model(sample)$/;"	variable	line:242	class:V4INVDecentral
Decentral	/home/neale/ctf_RL/network/CVDC_model1.py	/^class Decentral(tf.keras.Model):$/;"	class	line:22
__init__	/home/neale/ctf_RL/network/CVDC_model1.py	/^    def __init__(self, input_shape, action_size=5, atoms=128,$/;"	member	line:24	class:Decentral
print_summary	/home/neale/ctf_RL/network/CVDC_model1.py	/^    def print_summary(self):$/;"	member	line:68	class:Decentral
call	/home/neale/ctf_RL/network/CVDC_model1.py	/^    def call(self, inputs):$/;"	member	line:71	class:Decentral
Central	/home/neale/ctf_RL/network/CVDC_model1.py	/^class Central(tf.keras.Model):$/;"	class	line:144
__init__	/home/neale/ctf_RL/network/CVDC_model1.py	/^    def __init__(self, input_shape, atoms,$/;"	member	line:146	class:Central
print_summary	/home/neale/ctf_RL/network/CVDC_model1.py	/^    def print_summary(self):$/;"	member	line:162	class:Central
call	/home/neale/ctf_RL/network/CVDC_model1.py	/^    def call(self, inputs):$/;"	member	line:165	class:Central
loss_central	/home/neale/ctf_RL/network/CVDC_model1.py	/^def loss_central(model, state, td_target_c,$/;"	function	line:182
loss_ppo	/home/neale/ctf_RL/network/CVDC_model1.py	/^def loss_ppo(model, state, old_log_logit, action, old_value, td_target, advantage, td_target_c, rewards, next_state,$/;"	function	line:196
get_gradient	/home/neale/ctf_RL/network/CVDC_model1.py	/^def get_gradient(model, loss, inputs, hyperparameters={}):$/;"	function	line:284
train	/home/neale/ctf_RL/network/CVDC_model1.py	/^def train(model, loss, optimizer, inputs, hyperparameters={}):$/;"	function	line:291
V2	/home/neale/ctf_RL/network/model_V2.py	/^class V2(tf.keras.Model):$/;"	class	line:26
__init__	/home/neale/ctf_RL/network/model_V2.py	/^    def __init__(self, trainable=True, name='V2'):$/;"	member	line:29	class:V2
call	/home/neale/ctf_RL/network/model_V2.py	/^    def call(self, inputs):$/;"	member	line:46	class:V2
V2_PPO	/home/neale/ctf_RL/network/model_V2.py	/^class V2_PPO(tf.keras.Model):$/;"	class	line:81
__init__	/home/neale/ctf_RL/network/model_V2.py	/^    def __init__(self, action_size=5, trainable=True, lr=1e-4, eps=0.2, entropy_beta=0.05, critic_beta=0.5, name='PPO'):$/;"	member	line:83	class:V2_PPO
call	/home/neale/ctf_RL/network/model_V2.py	/^    def call(self, inputs):$/;"	member	line:96	class:V2_PPO
build_loss	/home/neale/ctf_RL/network/model_V2.py	/^    def build_loss(self, old_log_logit, action, advantage, td_target):$/;"	member	line:113	class:V2_PPO
_log	/home/neale/ctf_RL/network/model_V2.py	/^        def _log(val):$/;"	function	line:114	function:V2_PPO.build_loss
_kl_entropy	/home/neale/ctf_RL/network/model_V2.py	/^    def _kl_entropy(self):$/;"	member	line:150	class:V2_PPO
build_network	/home/neale/ctf_RL/network/model_V2.py	/^def build_network(input_hold):$/;"	function	line:164
network	/home/neale/ctf_RL/network/model_V2.py	/^    network = V2()$/;"	variable	line:203
z	/home/neale/ctf_RL/network/model_V2.py	/^    z = network(tf.placeholder(tf.float32, [None, 39, 39, 24]))$/;"	variable	line:204
V2Dist	/home/neale/ctf_RL/network/C51_Central.py	/^class V2Dist(tf.keras.Model):$/;"	class	line:16
__init__	/home/neale/ctf_RL/network/C51_Central.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:18	class:V2Dist
print_summary	/home/neale/ctf_RL/network/C51_Central.py	/^    def print_summary(self):$/;"	member	line:67	class:V2Dist
call	/home/neale/ctf_RL/network/C51_Central.py	/^    def call(self, inputs):$/;"	member	line:71	class:V2Dist
loss	/home/neale/ctf_RL/network/C51_Central.py	/^def loss(model, target_model, state, reward, done, next_state,$/;"	function	line:85
train	/home/neale/ctf_RL/network/C51_Central.py	/^def train(model, target_model, optimizer, inputs, **hyperparameters):$/;"	function	line:119
V4	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^class V4(tf.keras.Model):$/;"	class	line:18
STATIC_CHANNEL	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    STATIC_CHANNEL = [0,1,3]$/;"	variable	line:19	class:V4
DYNAMIC_CHANNEL	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    DYNAMIC_CHANNEL = [2,4,5]$/;"	variable	line:20	class:V4
LATENT_DIM	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    LATENT_DIM = 256$/;"	variable	line:21	class:V4
__init__	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:24	class:V4
print_summary	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def print_summary(self):$/;"	member	line:36	class:V4
call	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def call(self, inputs):$/;"	member	line:39	class:V4
V4INV	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^class V4INV(tf.keras.Model):$/;"	class	line:46
__init__	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def __init__(self, input_shape, trainable=True, name='FeatureNN_Inverse'):$/;"	member	line:48	class:V4INV
print_summary	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def print_summary(self):$/;"	member	line:62	class:V4INV
call	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def call(self, inputs):$/;"	member	line:65	class:V4INV
Decentral	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^class Decentral(tf.keras.Model):$/;"	class	line:69
__init__	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def __init__(self, input_shape, action_size=5, atoms=512,$/;"	member	line:71	class:Decentral
print_summary	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def print_summary(self):$/;"	member	line:139	class:Decentral
action_call	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def action_call(self, obs):  # Short operation for forward pass$/;"	member	line:143	class:Decentral
call	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def call(self, inputs): # Full Operation of the method$/;"	member	line:187	class:Decentral
Central	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^class Central(tf.keras.Model):$/;"	class	line:263
__init__	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def __init__(self, input_shape, atoms,$/;"	member	line:265	class:Central
print_summary	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def print_summary(self):$/;"	member	line:281	class:Central
call	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^    def call(self, inputs):$/;"	member	line:284	class:Central
loss_central	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^def loss_central(model, state, td_target_c):$/;"	function	line:301
loss_decentral_critic_only	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^def loss_decentral_critic_only(model, state, action, td_target_psi, td_target_c):$/;"	function	line:314
loss_ppo	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^def loss_ppo(model, state, old_log_logit, action, old_value, td_target, advantage, td_target_c, rewards, next_state,$/;"	function	line:326
get_gradient	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^def get_gradient(model, loss, inputs, hyperparameters={}):$/;"	function	line:415
train	/home/neale/ctf_RL/network/CVDC_model2_sc2.py	/^def train(model, loss, optimizer, inputs, hyperparameters={}):$/;"	function	line:422
V4	/home/neale/ctf_RL/network/model_V4_30.py	/^class V4(tf.keras.Model):$/;"	class	line:18
STATIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_30.py	/^    STATIC_CHANNEL = [0,1,3]$/;"	variable	line:19	class:V4
DYNAMIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_30.py	/^    DYNAMIC_CHANNEL = [2,4,5]$/;"	variable	line:20	class:V4
LATENT_DIM	/home/neale/ctf_RL/network/model_V4_30.py	/^    LATENT_DIM = 128$/;"	variable	line:21	class:V4
__init__	/home/neale/ctf_RL/network/model_V4_30.py	/^    def __init__(self, input_shape, action_size=5,$/;"	member	line:24	class:V4
print_summary	/home/neale/ctf_RL/network/model_V4_30.py	/^    def print_summary(self):$/;"	member	line:51	class:V4
call	/home/neale/ctf_RL/network/model_V4_30.py	/^    def call(self, inputs):$/;"	member	line:55	class:V4
V4INV	/home/neale/ctf_RL/network/model_V4_30.py	/^class V4INV(tf.keras.Model):$/;"	class	line:67
__init__	/home/neale/ctf_RL/network/model_V4_30.py	/^    def __init__(self, trainable=True, name='FeatureNN_Inverse'):$/;"	member	line:69	class:V4INV
print_summary	/home/neale/ctf_RL/network/model_V4_30.py	/^    def print_summary(self):$/;"	member	line:91	class:V4INV
call	/home/neale/ctf_RL/network/model_V4_30.py	/^    def call(self, inputs):$/;"	member	line:95	class:V4INV
V4Decentral	/home/neale/ctf_RL/network/model_V4_30.py	/^class V4Decentral(tf.keras.Model):$/;"	class	line:106
STATIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_30.py	/^    STATIC_CHANNEL = [0,1,3]$/;"	variable	line:107	class:V4Decentral
DYNAMIC_CHANNEL	/home/neale/ctf_RL/network/model_V4_30.py	/^    DYNAMIC_CHANNEL = [2,4,5]$/;"	variable	line:108	class:V4Decentral
LATENT_DIM	/home/neale/ctf_RL/network/model_V4_30.py	/^    LATENT_DIM = 128$/;"	variable	line:109	class:V4Decentral
__init__	/home/neale/ctf_RL/network/model_V4_30.py	/^    def __init__(self, input_shape, action_size=5, $/;"	member	line:112	class:V4Decentral
print_summary	/home/neale/ctf_RL/network/model_V4_30.py	/^    def print_summary(self):$/;"	member	line:137	class:V4Decentral
call	/home/neale/ctf_RL/network/model_V4_30.py	/^    def call(self, inputs):$/;"	member	line:141	class:V4Decentral
V4INVDecentral	/home/neale/ctf_RL/network/model_V4_30.py	/^class V4INVDecentral(tf.keras.Model):$/;"	class	line:154
__init__	/home/neale/ctf_RL/network/model_V4_30.py	/^    def __init__(self, trainable=True, name='FeatureNN_Inverse', **kwargs):$/;"	member	line:156	class:V4INVDecentral
print_summary	/home/neale/ctf_RL/network/model_V4_30.py	/^    def print_summary(self):$/;"	member	line:178	class:V4INVDecentral
call	/home/neale/ctf_RL/network/model_V4_30.py	/^    def call(self, inputs):$/;"	member	line:182	class:V4INVDecentral
physical_devices	/home/neale/ctf_RL/network/model_V4_30.py	/^    physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:194	class:V4INVDecentral
config	/home/neale/ctf_RL/network/model_V4_30.py	/^    config = tf.config.experimental.set_memory_growth(physical_devices[0], True)$/;"	variable	line:196	class:V4INVDecentral
sample_size	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample_size = 32$/;"	variable	line:199	class:V4INVDecentral
map_size	/home/neale/ctf_RL/network/model_V4_30.py	/^    map_size = 30$/;"	variable	line:200	class:V4INVDecentral
map_size	/home/neale/ctf_RL/network/model_V4_30.py	/^    map_size = map_size*2-1$/;"	variable	line:201	class:V4INVDecentral
image_shape	/home/neale/ctf_RL/network/model_V4_30.py	/^    image_shape = [map_size,map_size,6]$/;"	variable	line:202	class:V4INVDecentral
sample_shape	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample_shape = [sample_size]+image_shape$/;"	variable	line:203	class:V4INVDecentral
latent_size	/home/neale/ctf_RL/network/model_V4_30.py	/^    latent_size = 128$/;"	variable	line:204	class:V4INVDecentral
latent_shape	/home/neale/ctf_RL/network/model_V4_30.py	/^    latent_shape = [sample_size]+[latent_size]$/;"	variable	line:205	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4_30.py	/^    model = V4(image_shape, 5)$/;"	variable	line:208	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample = np.random.random(sample_shape).astype(np.float32)$/;"	variable	line:211	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4_30.py	/^    output = model(sample)$/;"	variable	line:212	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4_30.py	/^    model = V4INV()$/;"	variable	line:217	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample = np.random.random(latent_shape).astype(np.float32)$/;"	variable	line:220	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4_30.py	/^    output = model(sample)$/;"	variable	line:221	class:V4INVDecentral
sample_size	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample_size = 32$/;"	variable	line:226	class:V4INVDecentral
image_shape	/home/neale/ctf_RL/network/model_V4_30.py	/^    image_shape = [map_size,map_size,6]$/;"	variable	line:228	class:V4INVDecentral
sample_shape	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample_shape = [sample_size]+image_shape$/;"	variable	line:229	class:V4INVDecentral
latent_size	/home/neale/ctf_RL/network/model_V4_30.py	/^    latent_size = 128$/;"	variable	line:230	class:V4INVDecentral
latent_shape	/home/neale/ctf_RL/network/model_V4_30.py	/^    latent_shape = [sample_size]+[latent_size]$/;"	variable	line:231	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4_30.py	/^    model = V4Decentral(image_shape, 5)$/;"	variable	line:234	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample = np.random.random(sample_shape).astype(np.float32)$/;"	variable	line:237	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4_30.py	/^    output = model(sample)$/;"	variable	line:238	class:V4INVDecentral
model	/home/neale/ctf_RL/network/model_V4_30.py	/^    model = V4INVDecentral()$/;"	variable	line:243	class:V4INVDecentral
sample	/home/neale/ctf_RL/network/model_V4_30.py	/^    sample = np.random.random(latent_shape).astype(np.float32)$/;"	variable	line:246	class:V4INVDecentral
output	/home/neale/ctf_RL/network/model_V4_30.py	/^    output = model(sample)$/;"	variable	line:247	class:V4INVDecentral
physical_devices	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:15
parser	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^parser = argparse.ArgumentParser(description="PPO trainer for convoy")$/;"	variable	line:39
args	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^args = parser.parse_args()$/;"	variable	line:46
PROGBAR	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^PROGBAR = args.silence$/;"	variable	line:48
TRAIN_NAME	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^TRAIN_NAME = "PPO_{}_{:02d}_map_{}".format($/;"	variable	line:51
TRAIN_TAG	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^TRAIN_TAG = "PPO e2e model w Stacked Frames: " + TRAIN_NAME$/;"	variable	line:56
LOG_PATH	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:57
MODEL_PATH	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:58
GPU_CAPACITY	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:59
NENV	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^NENV = 1$/;"	variable	line:61
total_episodes	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^total_episodes = 250000$/;"	variable	line:70
max_ep	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^max_ep = 200$/;"	variable	line:71
gamma	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:72
lambd	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:73
save_network_frequency	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^save_network_frequency = 1024$/;"	variable	line:75
save_stat_frequency	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^save_stat_frequency = 128$/;"	variable	line:76
save_image_frequency	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^save_image_frequency = 128$/;"	variable	line:77
moving_average_step	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:78
keep_frame	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^keep_frame = 1$/;"	variable	line:80
minibatch_size	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^minibatch_size = 128$/;"	variable	line:82
epoch	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^epoch = 2$/;"	variable	line:83
minimum_batch_size	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^minimum_batch_size = 4096$/;"	variable	line:84
log_episodic_reward	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:87
log_winrate	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:88
log_looptime	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:89
log_traintime	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:90
make_env	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^def make_env():$/;"	function	line:94
envs	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^envs = StarCraft2Env(args.map)$/;"	variable	line:97
envs	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^envs = SMACWrapper(envs)$/;"	variable	line:99
action_space	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^action_space = len(envs.env.get_avail_agent_actions(0))$/;"	variable	line:101
input_size	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^input_size = [None,envs.env.get_obs_size()]$/;"	variable	line:102
agent_type_sc	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^agent_type_sc = []$/;"	variable	line:108
agent_type_list	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^agent_type_list = agent_type_sc.copy()$/;"	variable	line:111
agent_type	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^agent_type = [0]*len(set(agent_type_list))$/;"	variable	line:112
num_agent	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^num_agent = sum(agent_type)$/;"	variable	line:118
num_type	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^num_type = len(agent_type)$/;"	variable	line:120
agent_type_masking	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^agent_type_masking = np.zeros([num_type, num_agent], dtype=bool)$/;"	variable	line:121
agent_type_index	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^agent_type_index = np.zeros([num_agent], dtype=int)$/;"	variable	line:122
prev_i	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^prev_i = 0$/;"	variable	line:123
prev_i	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    prev_i = i$/;"	variable	line:127
agent_type_masking	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:128
network	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^network = Network($/;"	variable	line:131
input_shape	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    input_shape=input_size,$/;"	variable	line:132
action_size	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    action_size=action_space,$/;"	variable	line:133
agent_type	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    agent_type=agent_type,$/;"	variable	line:134
save_path	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    save_path=MODEL_PATH,$/;"	variable	line:135
global_episodes	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^global_episodes = network.initiate()$/;"	variable	line:139
writer	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:142
train	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^def train($/;"	function	line:146
train_datasets	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    train_datasets = []$/;"	variable	line:156
advantage_list	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    advantage_list = []$/;"	variable	line:158
traj_buffer_list	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:159
atype	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^            atype = agent_type_index[idx]$/;"	variable	line:162
reward	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^            reward = traj[2]$/;"	variable	line:164
critic	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^            critic = traj[3]$/;"	variable	line:165
_critic	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^            _critic = traj[5][-1]$/;"	variable	line:166
traj_buffer	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:174
traj_buffer	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:183
train_dataset	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        train_dataset = ($/;"	variable	line:184
tag	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^            tag = "debug\/"$/;"	variable	line:206
get_action	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^def get_action(states,validActions):$/;"	function	line:213
batch	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^batch = []$/;"	variable	line:240
num_batch	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^num_batch = 0$/;"	variable	line:241
episode_rew	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:246
trajs	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    trajs = [[Trajectory(depth=6) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:249
s1	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    s1 = envs.reset()$/;"	variable	line:252
s1	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    s1 = np.stack(s1).astype(np.float32)$/;"	variable	line:253
validActions	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    validActions = envs.get_avail_actions()$/;"	variable	line:254
validActions	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    validActions = envs.get_avail_actions()$/;"	variable	line:255
stime_roll	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    stime_roll = time.time()$/;"	variable	line:260
s0	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        s0 = s1$/;"	variable	line:262
p0	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        p0 = p1$/;"	variable	line:264
s1	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        s1 = np.stack(s1).astype(np.float32)$/;"	variable	line:267
is_alive	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        is_alive = [not va[0] for va in envs.get_avail_actions()]$/;"	variable	line:268
validActions	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        validActions = envs.get_avail_actions()$/;"	variable	line:271
idx	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:277
was_alive	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        was_alive = is_alive$/;"	variable	line:282
was_done	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        was_done = done$/;"	variable	line:283
etime_roll	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    etime_roll = time.time()$/;"	variable	line:287
num_batch	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    num_batch = len(batch) * 200 * num_agent$/;"	variable	line:290
stime_train	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        stime_train = time.time()$/;"	variable	line:292
log_image_on	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:293
etime_train	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        etime_train = time.time()$/;"	variable	line:304
batch	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        batch = []$/;"	variable	line:305
num_batch	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^        num_batch = 0$/;"	variable	line:306
log_on	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:318
tag	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^            tag = "baseline_training\/"$/;"	variable	line:321
save_on	/home/neale/ctf_RL/run_multiagent_ppo_sc2.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:335
physical_devices	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:20
parser	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^parser = argparse.ArgumentParser(description="CVDC(learnability) trainer for convoy")$/;"	variable	line:50
args	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^args = parser.parse_args()$/;"	variable	line:61
PROGBAR	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^PROGBAR = args.silence$/;"	variable	line:63
TRAIN_NAME	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^TRAIN_NAME = "CVDC_sharedSF_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:66
TRAIN_TAG	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^TRAIN_TAG = "Central value decentralized control(learnability) with sharedSF, " + TRAIN_NAME$/;"	variable	line:75
LOG_PATH	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:76
MODEL_PATH	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:77
SAVE_PATH	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^SAVE_PATH = ".\/save\/" + TRAIN_NAME$/;"	variable	line:78
MAP_PATH	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:79
GPU_CAPACITY	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:80
NENV	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:84
env_setting_path	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:87
game_config	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:88
config_path	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^config_path = "config.ini"$/;"	variable	line:101
config	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^config = configparser.ConfigParser()$/;"	variable	line:102
total_episodes	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^total_episodes = 100000$/;"	variable	line:106
max_ep	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^max_ep = 200$/;"	variable	line:107
gamma	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:108
lambd	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:109
save_network_frequency	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^save_network_frequency = 1024$/;"	variable	line:111
save_stat_frequency	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^save_stat_frequency = 128$/;"	variable	line:112
save_image_frequency	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^save_image_frequency = 128$/;"	variable	line:113
moving_average_step	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:114
action_space	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^action_space = 5$/;"	variable	line:116
keep_frame	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^keep_frame = 1$/;"	variable	line:117
map_size	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^map_size = args.map_size$/;"	variable	line:118
vision_range	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^vision_range = map_size - 1$/;"	variable	line:119
nchannel	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^nchannel = 6 * keep_frame$/;"	variable	line:121
input_size	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:122
cent_input_size	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:123
minibatch_size	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^minibatch_size = 128$/;"	variable	line:125
epoch	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^epoch = 1$/;"	variable	line:126
minimum_batch_size	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^minimum_batch_size = 1024 * 4$/;"	variable	line:127
log_episodic_reward	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:130
log_winrate	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:131
log_redwinrate	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:132
log_looptime	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:133
log_traintime	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:134
_qenv	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^_qenv = gym.make("cap-v0", map_size=map_size, config_path=game_config)$/;"	variable	line:138
make_env	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^def make_env(map_size):$/;"	function	line:139
envs	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:143
envs	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:144
num_blue	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:145
num_red	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:146
num_agent	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^num_agent = num_blue  # +num_red$/;"	variable	line:147
agent_type	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^agent_type = []$/;"	variable	line:153
num_type	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^num_type = len(agent_type)$/;"	variable	line:158
agent_type_masking	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:159
agent_type_index	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:160
prev_i	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^prev_i = 0$/;"	variable	line:161
prev_i	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    prev_i = i$/;"	variable	line:165
agent_type_masking	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:166
atoms	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^atoms = 256$/;"	variable	line:169
network	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^network = Network($/;"	variable	line:170
central_obs_shape	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    central_obs_shape=cent_input_size,$/;"	variable	line:171
decentral_obs_shape	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    decentral_obs_shape=input_size,$/;"	variable	line:172
action_size	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    action_size=action_space,$/;"	variable	line:173
agent_type	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    agent_type=agent_type,$/;"	variable	line:174
atoms	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    atoms=atoms,$/;"	variable	line:175
save_path	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    save_path=MODEL_PATH,$/;"	variable	line:176
global_episodes	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^global_episodes = network.initiate()$/;"	variable	line:180
writer	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:184
train_central	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^def train_central($/;"	function	line:187
traj_buffer	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    traj_buffer = defaultdict(list)$/;"	variable	line:197
states	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        states = np.array(traj[0])$/;"	variable	line:200
last_state	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        last_state = np.array(traj[3])[-1:, :, :, :]$/;"	variable	line:201
reward	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        reward = traj[2]$/;"	variable	line:202
critic	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        critic = env_critic["critic"].numpy()[:, 0].tolist()$/;"	variable	line:206
_critic	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        _critic = _env_critic["critic"].numpy()[0, 0]$/;"	variable	line:207
train_dataset	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    train_dataset = ($/;"	variable	line:215
train_decentral	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^def train_decentral($/;"	function	line:232
train_datasets	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    train_datasets = []$/;"	variable	line:241
traj_buffer_list	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:244
advantage_lists	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    advantage_lists = [[] for _ in range(num_type)]$/;"	variable	line:245
atype	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            atype = agent_type_index[idx]$/;"	variable	line:248
reward	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            reward = traj[2]$/;"	variable	line:250
mask	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            mask = traj[3]$/;"	variable	line:251
critic	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            critic = traj[5]$/;"	variable	line:252
phi	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            phi = np.array(traj[7]).tolist()$/;"	variable	line:253
psi	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            psi = np.array(traj[8]).tolist()$/;"	variable	line:254
_critic	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            _critic = traj[9][-1]$/;"	variable	line:255
_psi	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            _psi = np.array(traj[10][-1])$/;"	variable	line:256
normalize	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^                normalize=False$/;"	variable	line:269
normalize	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^                normalize=False,$/;"	variable	line:278
discount_adv	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^                discount_adv=False,$/;"	variable	line:297
normalize	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^                normalize=False,$/;"	variable	line:298
beta	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            beta = max(min((-0.9\/30000)*step + 1, 1.0),0.1)$/;"	variable	line:300
traj_buffer	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:302
traj_buffer	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:313
train_dataset	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        train_dataset = ($/;"	variable	line:314
tag	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            tag = "advantages\/"$/;"	variable	line:339
run_network	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^def run_network(states):$/;"	function	line:348
batch	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^batch = []$/;"	variable	line:387
dec_batch	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^dec_batch = []$/;"	variable	line:388
log_save_analysis	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    log_save_analysis = False  #interval_flag(global_episodes, 1024 * 4, "save_log")$/;"	variable	line:392
episode_rew	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:395
is_alive	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:396
is_done	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    is_done = [False for env in range(NENV * num_agent)]$/;"	variable	line:397
trajs	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    trajs = [[Trajectory(depth=16) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:399
cent_trajs	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    cent_trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:400
s1	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    s1 = envs.reset(config_path=game_config, policy_red=policy.Roomba,)$/;"	variable	line:404
s1	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:405
cent_s1	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:406
reward_pred_list	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    reward_pred_list = []$/;"	variable	line:410
stime_roll	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    stime_roll = time.time()$/;"	variable	line:413
_states	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    _states = [[] for _ in range(NENV)]$/;"	variable	line:414
_agent1_r	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    _agent1_r = [[] for _ in range(NENV)]$/;"	variable	line:415
_agent2_r	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    _agent2_r = [[] for _ in range(NENV)]$/;"	variable	line:416
_agent3_r	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    _agent3_r = [[] for _ in range(NENV)]$/;"	variable	line:417
_agent1_o	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    _agent1_o = [[] for _ in range(NENV)]$/;"	variable	line:418
_agent2_o	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    _agent2_o = [[] for _ in range(NENV)]$/;"	variable	line:419
_agent3_o	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    _agent3_o = [[] for _ in range(NENV)]$/;"	variable	line:420
s0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        s0 = s1$/;"	variable	line:423
a0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        a0 = a1$/;"	variable	line:424
vg0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        vg0 = vg1$/;"	variable	line:425
vc0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        vc0 = vc1$/;"	variable	line:426
psi0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        psi0 = psi1$/;"	variable	line:427
phi0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        phi0 = phi1$/;"	variable	line:428
log_logits0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        log_logits0 = log_logits1$/;"	variable	line:429
was_alive	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        was_alive = is_alive$/;"	variable	line:430
was_done	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        was_done = is_done$/;"	variable	line:431
cent_s0	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        cent_s0 = cent_s1$/;"	variable	line:432
is_alive	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:436
s1	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        s1 = s1.astype(np.float32)  # Decentralize observation$/;"	variable	line:437
cent_s1	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:438
idx	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:449
etime_roll	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    etime_roll = time.time()$/;"	variable	line:491
stime_train	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        stime_train = time.time()$/;"	variable	line:496
log	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        log = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:497
log_image	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        log_image = interval_flag(global_episodes, 1024, "ima_log")$/;"	variable	line:498
epoch	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            epoch=epoch,$/;"	variable	line:501
batch_size	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            batch_size=minibatch_size,$/;"	variable	line:502
writer	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            writer=writer,$/;"	variable	line:503
log	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            log=log,$/;"	variable	line:504
step	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            step=global_episodes,$/;"	variable	line:505
log_image	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            log_image=log_image,$/;"	variable	line:506
etime_train	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        etime_train = time.time()$/;"	variable	line:508
dec_batch	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        dec_batch = []$/;"	variable	line:509
log_tc_on	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        log_tc_on = interval_flag(global_episodes, save_image_frequency, 'tc_log')$/;"	variable	line:514
batch	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^        batch = []$/;"	variable	line:516
log_on	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:527
tag	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^            tag = "baseline_training\/"$/;"	variable	line:530
step	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^                step=global_episodes,$/;"	variable	line:545
save_on	/home/neale/ctf_RL/run_cvdc_sharedSF.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:549
RUNNING_SCRIPT	/home/neale/ctf_RL/runner.py	/^RUNNING_SCRIPT = [$/;"	variable	line:6
parser	/home/neale/ctf_RL/runner.py	/^parser = argparse.ArgumentParser(description="PPO trainer for convoy")$/;"	variable	line:20
args	/home/neale/ctf_RL/runner.py	/^args = parser.parse_args()$/;"	variable	line:28
device	/home/neale/ctf_RL/runner.py	/^    device = ','.join(args.device)$/;"	variable	line:31
make_command	/home/neale/ctf_RL/runner.py	/^def make_command(run_file_name):$/;"	function	line:35
command	/home/neale/ctf_RL/runner.py	/^    command = make_command(script_name)$/;"	variable	line:52
physical_devices	/home/neale/ctf_RL/run_multiagent_pg.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:15
parser	/home/neale/ctf_RL/run_multiagent_pg.py	/^parser = argparse.ArgumentParser(description="PPO trainer for convoy")$/;"	variable	line:37
args	/home/neale/ctf_RL/run_multiagent_pg.py	/^args = parser.parse_args()$/;"	variable	line:48
PROGBAR	/home/neale/ctf_RL/run_multiagent_pg.py	/^PROGBAR = args.silence$/;"	variable	line:50
TRAIN_NAME	/home/neale/ctf_RL/run_multiagent_pg.py	/^TRAIN_NAME = "IPG_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:53
TRAIN_TAG	/home/neale/ctf_RL/run_multiagent_pg.py	/^TRAIN_TAG = "PG e2e model w Stacked Frames: " + TRAIN_NAME$/;"	variable	line:62
LOG_PATH	/home/neale/ctf_RL/run_multiagent_pg.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:63
MODEL_PATH	/home/neale/ctf_RL/run_multiagent_pg.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:64
MAP_PATH	/home/neale/ctf_RL/run_multiagent_pg.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:65
GPU_CAPACITY	/home/neale/ctf_RL/run_multiagent_pg.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:66
NENV	/home/neale/ctf_RL/run_multiagent_pg.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:68
env_setting_path	/home/neale/ctf_RL/run_multiagent_pg.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:71
game_config	/home/neale/ctf_RL/run_multiagent_pg.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:72
total_episodes	/home/neale/ctf_RL/run_multiagent_pg.py	/^total_episodes = 100000$/;"	variable	line:85
max_ep	/home/neale/ctf_RL/run_multiagent_pg.py	/^max_ep = 200$/;"	variable	line:86
gamma	/home/neale/ctf_RL/run_multiagent_pg.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:87
lambd	/home/neale/ctf_RL/run_multiagent_pg.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:88
save_network_frequency	/home/neale/ctf_RL/run_multiagent_pg.py	/^save_network_frequency = 1024$/;"	variable	line:90
save_stat_frequency	/home/neale/ctf_RL/run_multiagent_pg.py	/^save_stat_frequency = 128$/;"	variable	line:91
save_image_frequency	/home/neale/ctf_RL/run_multiagent_pg.py	/^save_image_frequency = 128$/;"	variable	line:92
moving_average_step	/home/neale/ctf_RL/run_multiagent_pg.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:93
action_space	/home/neale/ctf_RL/run_multiagent_pg.py	/^action_space = 5$/;"	variable	line:95
keep_frame	/home/neale/ctf_RL/run_multiagent_pg.py	/^keep_frame = 1$/;"	variable	line:96
map_size	/home/neale/ctf_RL/run_multiagent_pg.py	/^map_size = args.map_size$/;"	variable	line:97
vision_range	/home/neale/ctf_RL/run_multiagent_pg.py	/^vision_range = map_size - 1$/;"	variable	line:98
nchannel	/home/neale/ctf_RL/run_multiagent_pg.py	/^nchannel = 6 * keep_frame$/;"	variable	line:100
input_size	/home/neale/ctf_RL/run_multiagent_pg.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:101
minibatch_size	/home/neale/ctf_RL/run_multiagent_pg.py	/^minibatch_size = 128$/;"	variable	line:103
epoch	/home/neale/ctf_RL/run_multiagent_pg.py	/^epoch = 1$/;"	variable	line:104
minimum_batch_size	/home/neale/ctf_RL/run_multiagent_pg.py	/^minimum_batch_size = 2048$/;"	variable	line:105
log_episodic_reward	/home/neale/ctf_RL/run_multiagent_pg.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:108
log_winrate	/home/neale/ctf_RL/run_multiagent_pg.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:109
log_redwinrate	/home/neale/ctf_RL/run_multiagent_pg.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:110
log_looptime	/home/neale/ctf_RL/run_multiagent_pg.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:111
log_traintime	/home/neale/ctf_RL/run_multiagent_pg.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:112
make_env	/home/neale/ctf_RL/run_multiagent_pg.py	/^def make_env(map_size):$/;"	function	line:116
envs	/home/neale/ctf_RL/run_multiagent_pg.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:120
envs	/home/neale/ctf_RL/run_multiagent_pg.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:121
num_blue	/home/neale/ctf_RL/run_multiagent_pg.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:122
num_red	/home/neale/ctf_RL/run_multiagent_pg.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:123
num_agent	/home/neale/ctf_RL/run_multiagent_pg.py	/^num_agent = num_blue  # + num_red$/;"	variable	line:124
agent_type	/home/neale/ctf_RL/run_multiagent_pg.py	/^agent_type = []$/;"	variable	line:130
num_type	/home/neale/ctf_RL/run_multiagent_pg.py	/^num_type = len(agent_type)$/;"	variable	line:135
agent_type_masking	/home/neale/ctf_RL/run_multiagent_pg.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:136
agent_type_index	/home/neale/ctf_RL/run_multiagent_pg.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:137
prev_i	/home/neale/ctf_RL/run_multiagent_pg.py	/^prev_i = 0$/;"	variable	line:138
prev_i	/home/neale/ctf_RL/run_multiagent_pg.py	/^    prev_i = i$/;"	variable	line:142
agent_type_masking	/home/neale/ctf_RL/run_multiagent_pg.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:143
network	/home/neale/ctf_RL/run_multiagent_pg.py	/^network = Network($/;"	variable	line:146
input_shape	/home/neale/ctf_RL/run_multiagent_pg.py	/^    input_shape=input_size,$/;"	variable	line:147
action_size	/home/neale/ctf_RL/run_multiagent_pg.py	/^    action_size=action_space,$/;"	variable	line:148
agent_type	/home/neale/ctf_RL/run_multiagent_pg.py	/^    agent_type=agent_type,$/;"	variable	line:149
save_path	/home/neale/ctf_RL/run_multiagent_pg.py	/^    save_path=MODEL_PATH,$/;"	variable	line:150
global_episodes	/home/neale/ctf_RL/run_multiagent_pg.py	/^global_episodes = network.initiate()$/;"	variable	line:154
writer	/home/neale/ctf_RL/run_multiagent_pg.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:157
train	/home/neale/ctf_RL/run_multiagent_pg.py	/^def train($/;"	function	line:161
train_datasets	/home/neale/ctf_RL/run_multiagent_pg.py	/^    train_datasets = []$/;"	variable	line:171
advantage_list	/home/neale/ctf_RL/run_multiagent_pg.py	/^    advantage_list = []$/;"	variable	line:173
traj_buffer_list	/home/neale/ctf_RL/run_multiagent_pg.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:174
atype	/home/neale/ctf_RL/run_multiagent_pg.py	/^            atype = agent_type_index[idx]$/;"	variable	line:177
reward	/home/neale/ctf_RL/run_multiagent_pg.py	/^            reward = traj[2]$/;"	variable	line:179
discounted_reward	/home/neale/ctf_RL/run_multiagent_pg.py	/^            discounted_reward = discount(traj[2], gamma)$/;"	variable	line:180
advantages	/home/neale/ctf_RL/run_multiagent_pg.py	/^            advantages = discounted_reward$/;"	variable	line:181
traj_buffer	/home/neale/ctf_RL/run_multiagent_pg.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:185
traj_buffer	/home/neale/ctf_RL/run_multiagent_pg.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:192
train_dataset	/home/neale/ctf_RL/run_multiagent_pg.py	/^        train_dataset = ($/;"	variable	line:193
tag	/home/neale/ctf_RL/run_multiagent_pg.py	/^            tag = "debug\/"$/;"	variable	line:213
get_action	/home/neale/ctf_RL/run_multiagent_pg.py	/^def get_action(states):$/;"	function	line:220
batch	/home/neale/ctf_RL/run_multiagent_pg.py	/^batch = []$/;"	variable	line:242
num_batch	/home/neale/ctf_RL/run_multiagent_pg.py	/^num_batch = 0$/;"	variable	line:243
episode_rew	/home/neale/ctf_RL/run_multiagent_pg.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:248
was_alive	/home/neale/ctf_RL/run_multiagent_pg.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:249
was_done	/home/neale/ctf_RL/run_multiagent_pg.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:250
trajs	/home/neale/ctf_RL/run_multiagent_pg.py	/^    trajs = [[Trajectory(depth=6) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:253
s1	/home/neale/ctf_RL/run_multiagent_pg.py	/^    s1 = envs.reset($/;"	variable	line:256
map_size	/home/neale/ctf_RL/run_multiagent_pg.py	/^        map_size=map_size,$/;"	variable	line:257
config_path	/home/neale/ctf_RL/run_multiagent_pg.py	/^        config_path=game_config,$/;"	variable	line:259
policy_red	/home/neale/ctf_RL/run_multiagent_pg.py	/^        policy_red=policy.Roomba,$/;"	variable	line:260
s1	/home/neale/ctf_RL/run_multiagent_pg.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:262
stime_roll	/home/neale/ctf_RL/run_multiagent_pg.py	/^    stime_roll = time.time()$/;"	variable	line:266
s0	/home/neale/ctf_RL/run_multiagent_pg.py	/^        s0 = s1$/;"	variable	line:268
a0	/home/neale/ctf_RL/run_multiagent_pg.py	/^        a0 = a1$/;"	variable	line:269
p0	/home/neale/ctf_RL/run_multiagent_pg.py	/^        p0 = p1$/;"	variable	line:270
s1	/home/neale/ctf_RL/run_multiagent_pg.py	/^        s1 = s1.astype(np.float32)$/;"	variable	line:273
is_alive	/home/neale/ctf_RL/run_multiagent_pg.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:274
idx	/home/neale/ctf_RL/run_multiagent_pg.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:282
was_alive	/home/neale/ctf_RL/run_multiagent_pg.py	/^        was_alive = is_alive$/;"	variable	line:287
was_done	/home/neale/ctf_RL/run_multiagent_pg.py	/^        was_done = done$/;"	variable	line:288
etime_roll	/home/neale/ctf_RL/run_multiagent_pg.py	/^    etime_roll = time.time()$/;"	variable	line:292
num_batch	/home/neale/ctf_RL/run_multiagent_pg.py	/^    num_batch = len(batch) * 200 * num_agent$/;"	variable	line:295
stime_train	/home/neale/ctf_RL/run_multiagent_pg.py	/^        stime_train = time.time()$/;"	variable	line:297
log_image_on	/home/neale/ctf_RL/run_multiagent_pg.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:298
etime_train	/home/neale/ctf_RL/run_multiagent_pg.py	/^        etime_train = time.time()$/;"	variable	line:309
batch	/home/neale/ctf_RL/run_multiagent_pg.py	/^        batch = []$/;"	variable	line:310
num_batch	/home/neale/ctf_RL/run_multiagent_pg.py	/^        num_batch = 0$/;"	variable	line:311
log_on	/home/neale/ctf_RL/run_multiagent_pg.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:323
tag	/home/neale/ctf_RL/run_multiagent_pg.py	/^            tag = "baseline_training\/"$/;"	variable	line:326
save_on	/home/neale/ctf_RL/run_multiagent_pg.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:340
env	/home/neale/ctf_RL/competition.py	/^env = gym.make("cap-v0")$/;"	variable	line:15
map_path	/home/neale/ctf_RL/competition.py	/^map_path = 'fair_uav'$/;"	variable	line:18
map_paths	/home/neale/ctf_RL/competition.py	/^map_paths = [join(map_path,f) for f in os.listdir(map_path) if isfile(join(map_path, f))]$/;"	variable	line:19
elo	/home/neale/ctf_RL/competition.py	/^elo = Elo()$/;"	variable	line:22
players	/home/neale/ctf_RL/competition.py	/^players = []$/;"	variable	line:26
read_player	/home/neale/ctf_RL/competition.py	/^def read_player(fpath):$/;"	function	line:27
players	/home/neale/ctf_RL/competition.py	/^players = read_player('competition_player.txt')$/;"	variable	line:35
N	/home/neale/ctf_RL/competition.py	/^N = 5 # Length of set$/;"	variable	line:45
win_cutoff	/home/neale/ctf_RL/competition.py	/^win_cutoff = 0.5$/;"	variable	line:46
nn_policy_1	/home/neale/ctf_RL/competition.py	/^    nn_policy_1 = policy.UAV()$/;"	variable	line:49
nn_policy_2	/home/neale/ctf_RL/competition.py	/^    nn_policy_2 = policy.UAV()$/;"	variable	line:50
basic_policies	/home/neale/ctf_RL/competition.py	/^    basic_policies = {'Roomba': policy.Roomba(), 'Zeros': policy.Zeros(), 'Random': policy.Random()}$/;"	variable	line:51
basic_policies_name	/home/neale/ctf_RL/competition.py	/^    basic_policies_name = ['Roomba', 'Zeros', 'Random']$/;"	variable	line:52
episode	/home/neale/ctf_RL/competition.py	/^    episode = 0$/;"	variable	line:53
resetCount	/home/neale/ctf_RL/competition.py	/^    resetCount = 0$/;"	variable	line:57
p1_name	/home/neale/ctf_RL/competition.py	/^        p1_name = p1 if p1 in basic_policies_name else p1[2]$/;"	variable	line:68
p2_name	/home/neale/ctf_RL/competition.py	/^        p2_name = p2 if p2 in basic_policies_name else p2[2]$/;"	variable	line:69
resetCount	/home/neale/ctf_RL/competition.py	/^            resetCount = 0$/;"	variable	line:74
player1	/home/neale/ctf_RL/competition.py	/^            player1 = p1$/;"	variable	line:76
policy1	/home/neale/ctf_RL/competition.py	/^            policy1 = basic_policies[player1]$/;"	variable	line:77
player1	/home/neale/ctf_RL/competition.py	/^            player1 = p1[2]$/;"	variable	line:79
policy1	/home/neale/ctf_RL/competition.py	/^            policy1 = nn_policy_1$/;"	variable	line:80
player2	/home/neale/ctf_RL/competition.py	/^            player2 = p2$/;"	variable	line:83
policy2	/home/neale/ctf_RL/competition.py	/^            policy2 = basic_policies[player2]$/;"	variable	line:84
player2	/home/neale/ctf_RL/competition.py	/^            player2 = p2[2]$/;"	variable	line:86
policy2	/home/neale/ctf_RL/competition.py	/^            policy2 = nn_policy_2$/;"	variable	line:87
match_results	/home/neale/ctf_RL/competition.py	/^        match_results = []$/;"	variable	line:91
observation	/home/neale/ctf_RL/competition.py	/^            observation = env.reset($/;"	variable	line:93
map_size	/home/neale/ctf_RL/competition.py	/^                    map_size=20,$/;"	variable	line:94
config_path	/home/neale/ctf_RL/competition.py	/^                    config_path='uav_settings.ini',$/;"	variable	line:95
custom_board	/home/neale/ctf_RL/competition.py	/^                    custom_board=random.choice(map_paths),$/;"	variable	line:96
policy_blue	/home/neale/ctf_RL/competition.py	/^                    policy_blue=policy1,$/;"	variable	line:97
policy_red	/home/neale/ctf_RL/competition.py	/^                    policy_red=policy2,$/;"	variable	line:98
t	/home/neale/ctf_RL/competition.py	/^            t = 0$/;"	variable	line:101
done	/home/neale/ctf_RL/competition.py	/^            done = False$/;"	variable	line:102
result	/home/neale/ctf_RL/competition.py	/^        result = sum(match_results)$/;"	variable	line:119
elo_string	/home/neale/ctf_RL/competition.py	/^    elo_string = str(elo)$/;"	variable	line:130
elo_string	/home/neale/ctf_RL/competition.py	/^    elo_string = str(elo)$/;"	variable	line:140
physical_devices	/home/neale/ctf_RL/run_VDN.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:15
parser	/home/neale/ctf_RL/run_VDN.py	/^parser = argparse.ArgumentParser(description="PPO trainer for convoy")$/;"	variable	line:38
args	/home/neale/ctf_RL/run_VDN.py	/^args = parser.parse_args()$/;"	variable	line:49
PROGBAR	/home/neale/ctf_RL/run_VDN.py	/^PROGBAR = args.silence$/;"	variable	line:51
TRAIN_NAME	/home/neale/ctf_RL/run_VDN.py	/^TRAIN_NAME = "VDN_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:54
TRAIN_TAG	/home/neale/ctf_RL/run_VDN.py	/^TRAIN_TAG = "IV e2e model w Stacked Frames: " + TRAIN_NAME$/;"	variable	line:63
LOG_PATH	/home/neale/ctf_RL/run_VDN.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:64
MODEL_PATH	/home/neale/ctf_RL/run_VDN.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:65
MAP_PATH	/home/neale/ctf_RL/run_VDN.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:66
GPU_CAPACITY	/home/neale/ctf_RL/run_VDN.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:67
NENV	/home/neale/ctf_RL/run_VDN.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:69
env_setting_path	/home/neale/ctf_RL/run_VDN.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:72
game_config	/home/neale/ctf_RL/run_VDN.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:73
total_episodes	/home/neale/ctf_RL/run_VDN.py	/^total_episodes = 100000$/;"	variable	line:86
max_ep	/home/neale/ctf_RL/run_VDN.py	/^max_ep = 200$/;"	variable	line:87
gamma	/home/neale/ctf_RL/run_VDN.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:88
lambd	/home/neale/ctf_RL/run_VDN.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:89
save_network_frequency	/home/neale/ctf_RL/run_VDN.py	/^save_network_frequency = 1024$/;"	variable	line:91
save_stat_frequency	/home/neale/ctf_RL/run_VDN.py	/^save_stat_frequency = 128$/;"	variable	line:92
save_image_frequency	/home/neale/ctf_RL/run_VDN.py	/^save_image_frequency = 128$/;"	variable	line:93
moving_average_step	/home/neale/ctf_RL/run_VDN.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:94
action_space	/home/neale/ctf_RL/run_VDN.py	/^action_space = 5$/;"	variable	line:96
keep_frame	/home/neale/ctf_RL/run_VDN.py	/^keep_frame = 4$/;"	variable	line:97
map_size	/home/neale/ctf_RL/run_VDN.py	/^map_size = args.map_size$/;"	variable	line:98
vision_range	/home/neale/ctf_RL/run_VDN.py	/^vision_range = map_size - 1$/;"	variable	line:99
nchannel	/home/neale/ctf_RL/run_VDN.py	/^nchannel = 6 * keep_frame$/;"	variable	line:101
input_size	/home/neale/ctf_RL/run_VDN.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:102
minibatch_size	/home/neale/ctf_RL/run_VDN.py	/^minibatch_size = 256$/;"	variable	line:104
epoch	/home/neale/ctf_RL/run_VDN.py	/^epoch = 1$/;"	variable	line:105
minimum_batch_size	/home/neale/ctf_RL/run_VDN.py	/^minimum_batch_size = 2048$/;"	variable	line:106
log_episodic_reward	/home/neale/ctf_RL/run_VDN.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:109
log_winrate	/home/neale/ctf_RL/run_VDN.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:110
log_redwinrate	/home/neale/ctf_RL/run_VDN.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:111
log_looptime	/home/neale/ctf_RL/run_VDN.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:112
log_traintime	/home/neale/ctf_RL/run_VDN.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:113
make_env	/home/neale/ctf_RL/run_VDN.py	/^def make_env(map_size):$/;"	function	line:117
envs	/home/neale/ctf_RL/run_VDN.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:121
envs	/home/neale/ctf_RL/run_VDN.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:122
num_blue	/home/neale/ctf_RL/run_VDN.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:123
num_red	/home/neale/ctf_RL/run_VDN.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:124
num_agent	/home/neale/ctf_RL/run_VDN.py	/^num_agent = num_blue  # + num_red$/;"	variable	line:125
agent_type	/home/neale/ctf_RL/run_VDN.py	/^agent_type = []$/;"	variable	line:131
num_type	/home/neale/ctf_RL/run_VDN.py	/^num_type = len(agent_type)$/;"	variable	line:136
agent_type_masking	/home/neale/ctf_RL/run_VDN.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:137
agent_type_index	/home/neale/ctf_RL/run_VDN.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:138
prev_i	/home/neale/ctf_RL/run_VDN.py	/^prev_i = 0$/;"	variable	line:139
prev_i	/home/neale/ctf_RL/run_VDN.py	/^    prev_i = i$/;"	variable	line:143
agent_type_masking	/home/neale/ctf_RL/run_VDN.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:144
network	/home/neale/ctf_RL/run_VDN.py	/^network = Network($/;"	variable	line:147
input_shape	/home/neale/ctf_RL/run_VDN.py	/^    input_shape=input_size,$/;"	variable	line:148
action_size	/home/neale/ctf_RL/run_VDN.py	/^    action_size=action_space,$/;"	variable	line:149
agent_type	/home/neale/ctf_RL/run_VDN.py	/^    agent_type=agent_type,$/;"	variable	line:150
save_path	/home/neale/ctf_RL/run_VDN.py	/^    save_path=MODEL_PATH,$/;"	variable	line:151
global_episodes	/home/neale/ctf_RL/run_VDN.py	/^global_episodes = network.initiate()$/;"	variable	line:155
writer	/home/neale/ctf_RL/run_VDN.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:158
train	/home/neale/ctf_RL/run_VDN.py	/^def train($/;"	function	line:162
datasets_critic	/home/neale/ctf_RL/run_VDN.py	/^    datasets_critic = []$/;"	variable	line:172
traj_buffer	/home/neale/ctf_RL/run_VDN.py	/^    traj_buffer = defaultdict(list)$/;"	variable	line:175
reward	/home/neale/ctf_RL/run_VDN.py	/^        reward = traj[1]$/;"	variable	line:177
critic	/home/neale/ctf_RL/run_VDN.py	/^        critic = traj[2]$/;"	variable	line:178
_critic	/home/neale/ctf_RL/run_VDN.py	/^        _critic = traj[3][-1]$/;"	variable	line:179
train_dataset	/home/neale/ctf_RL/run_VDN.py	/^    train_dataset = ($/;"	variable	line:189
datasets_critic	/home/neale/ctf_RL/run_VDN.py	/^    datasets_critic = train_dataset$/;"	variable	line:200
tag	/home/neale/ctf_RL/run_VDN.py	/^            tag = "debug\/"$/;"	variable	line:207
get_action	/home/neale/ctf_RL/run_VDN.py	/^def get_action(states):$/;"	function	line:214
batch2	/home/neale/ctf_RL/run_VDN.py	/^batch2 = []$/;"	variable	line:236
num_batch	/home/neale/ctf_RL/run_VDN.py	/^num_batch = 0$/;"	variable	line:237
episode_rew	/home/neale/ctf_RL/run_VDN.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:241
was_alive	/home/neale/ctf_RL/run_VDN.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:242
was_done	/home/neale/ctf_RL/run_VDN.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:243
trajs_team	/home/neale/ctf_RL/run_VDN.py	/^    trajs_team = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:245
s1	/home/neale/ctf_RL/run_VDN.py	/^    s1 = envs.reset($/;"	variable	line:248
map_size	/home/neale/ctf_RL/run_VDN.py	/^        map_size=map_size,$/;"	variable	line:249
config_path	/home/neale/ctf_RL/run_VDN.py	/^        config_path=game_config,$/;"	variable	line:251
policy_red	/home/neale/ctf_RL/run_VDN.py	/^        policy_red=policy.Roomba,$/;"	variable	line:252
s1	/home/neale/ctf_RL/run_VDN.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:254
stime_roll	/home/neale/ctf_RL/run_VDN.py	/^    stime_roll = time.time()$/;"	variable	line:258
s0	/home/neale/ctf_RL/run_VDN.py	/^        s0 = s1$/;"	variable	line:260
s1	/home/neale/ctf_RL/run_VDN.py	/^        s1 = s1.astype(np.float32)$/;"	variable	line:264
is_alive	/home/neale/ctf_RL/run_VDN.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:265
v0s	/home/neale/ctf_RL/run_VDN.py	/^        v0s = v0.reshape([NENV, num_agent]).sum(axis=1)$/;"	variable	line:271
v1s	/home/neale/ctf_RL/run_VDN.py	/^        v1s = v1.reshape([NENV, num_agent]).sum(axis=1)$/;"	variable	line:272
was_alive	/home/neale/ctf_RL/run_VDN.py	/^        was_alive = is_alive$/;"	variable	line:281
was_done	/home/neale/ctf_RL/run_VDN.py	/^        was_done = done$/;"	variable	line:282
etime_roll	/home/neale/ctf_RL/run_VDN.py	/^    etime_roll = time.time()$/;"	variable	line:283
num_batch	/home/neale/ctf_RL/run_VDN.py	/^    num_batch = len(batch2) * 200 * num_agent$/;"	variable	line:286
stime_train	/home/neale/ctf_RL/run_VDN.py	/^        stime_train = time.time()$/;"	variable	line:288
log_image_on	/home/neale/ctf_RL/run_VDN.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:289
etime_train	/home/neale/ctf_RL/run_VDN.py	/^        etime_train = time.time()$/;"	variable	line:300
batch2	/home/neale/ctf_RL/run_VDN.py	/^        batch2 = []$/;"	variable	line:301
num_batch	/home/neale/ctf_RL/run_VDN.py	/^        num_batch = 0$/;"	variable	line:302
log_on	/home/neale/ctf_RL/run_VDN.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:314
tag	/home/neale/ctf_RL/run_VDN.py	/^            tag = "baseline_training\/"$/;"	variable	line:317
save_on	/home/neale/ctf_RL/run_VDN.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:331
RL Policies	/home/neale/ctf_RL/README.md	/^# RL Policies$/;"	function	line:1
1. A3C (TensorFlow)	/home/neale/ctf_RL/README.md	/^## 1. A3C (TensorFlow)$/;"	function	line:5
Specification	/home/neale/ctf_RL/README.md	/^### Specification$/;"	function	line:7
Contents	/home/neale/ctf_RL/README.md	/^### Contents$/;"	function	line:11
Example usage	/home/neale/ctf_RL/README.md	/^### Example usage$/;"	function	line:16
Note:	/home/neale/ctf_RL/README.md	/^# Note:$/;"	function	line:24
physical_devices	/home/neale/ctf_RL/run_cvdc2.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:18
parser	/home/neale/ctf_RL/run_cvdc2.py	/^parser = argparse.ArgumentParser(description="CVDC(learnability) trainer for convoy")$/;"	variable	line:48
args	/home/neale/ctf_RL/run_cvdc2.py	/^args = parser.parse_args()$/;"	variable	line:59
PROGBAR	/home/neale/ctf_RL/run_cvdc2.py	/^PROGBAR = args.silence$/;"	variable	line:61
TRAIN_NAME	/home/neale/ctf_RL/run_cvdc2.py	/^TRAIN_NAME = "CVDC_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:64
TRAIN_TAG	/home/neale/ctf_RL/run_cvdc2.py	/^TRAIN_TAG = "Central value decentralized control(learnability), " + TRAIN_NAME$/;"	variable	line:73
LOG_PATH	/home/neale/ctf_RL/run_cvdc2.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:74
MODEL_PATH	/home/neale/ctf_RL/run_cvdc2.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:75
SAVE_PATH	/home/neale/ctf_RL/run_cvdc2.py	/^SAVE_PATH = ".\/save\/" + TRAIN_NAME$/;"	variable	line:76
MAP_PATH	/home/neale/ctf_RL/run_cvdc2.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:77
GPU_CAPACITY	/home/neale/ctf_RL/run_cvdc2.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:78
NENV	/home/neale/ctf_RL/run_cvdc2.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:82
env_setting_path	/home/neale/ctf_RL/run_cvdc2.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:85
game_config	/home/neale/ctf_RL/run_cvdc2.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:86
config_path	/home/neale/ctf_RL/run_cvdc2.py	/^config_path = "config.ini"$/;"	variable	line:99
config	/home/neale/ctf_RL/run_cvdc2.py	/^config = configparser.ConfigParser()$/;"	variable	line:100
total_episodes	/home/neale/ctf_RL/run_cvdc2.py	/^total_episodes = 100000$/;"	variable	line:104
max_ep	/home/neale/ctf_RL/run_cvdc2.py	/^max_ep = 200$/;"	variable	line:105
gamma	/home/neale/ctf_RL/run_cvdc2.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:106
lambd	/home/neale/ctf_RL/run_cvdc2.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:107
save_network_frequency	/home/neale/ctf_RL/run_cvdc2.py	/^save_network_frequency = 1024$/;"	variable	line:109
save_stat_frequency	/home/neale/ctf_RL/run_cvdc2.py	/^save_stat_frequency = 128$/;"	variable	line:110
save_image_frequency	/home/neale/ctf_RL/run_cvdc2.py	/^save_image_frequency = 128$/;"	variable	line:111
moving_average_step	/home/neale/ctf_RL/run_cvdc2.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:112
action_space	/home/neale/ctf_RL/run_cvdc2.py	/^action_space = 5$/;"	variable	line:114
keep_frame	/home/neale/ctf_RL/run_cvdc2.py	/^keep_frame = 1$/;"	variable	line:115
map_size	/home/neale/ctf_RL/run_cvdc2.py	/^map_size = args.map_size$/;"	variable	line:116
vision_range	/home/neale/ctf_RL/run_cvdc2.py	/^vision_range = map_size - 1$/;"	variable	line:117
nchannel	/home/neale/ctf_RL/run_cvdc2.py	/^nchannel = 6 * keep_frame$/;"	variable	line:119
input_size	/home/neale/ctf_RL/run_cvdc2.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:120
cent_input_size	/home/neale/ctf_RL/run_cvdc2.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:121
minibatch_size	/home/neale/ctf_RL/run_cvdc2.py	/^minibatch_size = 128$/;"	variable	line:123
epoch	/home/neale/ctf_RL/run_cvdc2.py	/^epoch = 1$/;"	variable	line:124
minimum_batch_size	/home/neale/ctf_RL/run_cvdc2.py	/^minimum_batch_size = 1024 * 4$/;"	variable	line:125
log_episodic_reward	/home/neale/ctf_RL/run_cvdc2.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:128
log_winrate	/home/neale/ctf_RL/run_cvdc2.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:129
log_redwinrate	/home/neale/ctf_RL/run_cvdc2.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:130
log_looptime	/home/neale/ctf_RL/run_cvdc2.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:131
log_traintime	/home/neale/ctf_RL/run_cvdc2.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:132
_qenv	/home/neale/ctf_RL/run_cvdc2.py	/^_qenv = gym.make("cap-v0", map_size=map_size, config_path=game_config)$/;"	variable	line:136
make_env	/home/neale/ctf_RL/run_cvdc2.py	/^def make_env(map_size):$/;"	function	line:137
envs	/home/neale/ctf_RL/run_cvdc2.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:141
envs	/home/neale/ctf_RL/run_cvdc2.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:142
num_blue	/home/neale/ctf_RL/run_cvdc2.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:143
num_red	/home/neale/ctf_RL/run_cvdc2.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:144
num_agent	/home/neale/ctf_RL/run_cvdc2.py	/^num_agent = num_blue  # +num_red$/;"	variable	line:145
agent_type	/home/neale/ctf_RL/run_cvdc2.py	/^agent_type = []$/;"	variable	line:151
num_type	/home/neale/ctf_RL/run_cvdc2.py	/^num_type = len(agent_type)$/;"	variable	line:156
agent_type_masking	/home/neale/ctf_RL/run_cvdc2.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:157
agent_type_index	/home/neale/ctf_RL/run_cvdc2.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:158
prev_i	/home/neale/ctf_RL/run_cvdc2.py	/^prev_i = 0$/;"	variable	line:159
prev_i	/home/neale/ctf_RL/run_cvdc2.py	/^    prev_i = i$/;"	variable	line:163
agent_type_masking	/home/neale/ctf_RL/run_cvdc2.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:164
atoms	/home/neale/ctf_RL/run_cvdc2.py	/^atoms = 256$/;"	variable	line:167
network	/home/neale/ctf_RL/run_cvdc2.py	/^network = Network($/;"	variable	line:168
central_obs_shape	/home/neale/ctf_RL/run_cvdc2.py	/^    central_obs_shape=cent_input_size,$/;"	variable	line:169
decentral_obs_shape	/home/neale/ctf_RL/run_cvdc2.py	/^    decentral_obs_shape=input_size,$/;"	variable	line:170
action_size	/home/neale/ctf_RL/run_cvdc2.py	/^    action_size=action_space,$/;"	variable	line:171
agent_type	/home/neale/ctf_RL/run_cvdc2.py	/^    agent_type=agent_type,$/;"	variable	line:172
atoms	/home/neale/ctf_RL/run_cvdc2.py	/^    atoms=atoms,$/;"	variable	line:173
save_path	/home/neale/ctf_RL/run_cvdc2.py	/^    save_path=MODEL_PATH,$/;"	variable	line:174
global_episodes	/home/neale/ctf_RL/run_cvdc2.py	/^global_episodes = network.initiate()$/;"	variable	line:178
writer	/home/neale/ctf_RL/run_cvdc2.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:182
train_central	/home/neale/ctf_RL/run_cvdc2.py	/^def train_central($/;"	function	line:185
traj_buffer	/home/neale/ctf_RL/run_cvdc2.py	/^    traj_buffer = defaultdict(list)$/;"	variable	line:195
states	/home/neale/ctf_RL/run_cvdc2.py	/^        states = np.array(traj[0])$/;"	variable	line:198
last_state	/home/neale/ctf_RL/run_cvdc2.py	/^        last_state = np.array(traj[3])[-1:, :, :, :]$/;"	variable	line:199
reward	/home/neale/ctf_RL/run_cvdc2.py	/^        reward = traj[2]$/;"	variable	line:200
critic	/home/neale/ctf_RL/run_cvdc2.py	/^        critic = env_critic["critic"].numpy()[:, 0].tolist()$/;"	variable	line:204
_critic	/home/neale/ctf_RL/run_cvdc2.py	/^        _critic = _env_critic["critic"].numpy()[0, 0]$/;"	variable	line:205
train_dataset	/home/neale/ctf_RL/run_cvdc2.py	/^    train_dataset = ($/;"	variable	line:213
train_decentral	/home/neale/ctf_RL/run_cvdc2.py	/^def train_decentral($/;"	function	line:230
train_datasets	/home/neale/ctf_RL/run_cvdc2.py	/^    train_datasets = []$/;"	variable	line:239
traj_buffer_list	/home/neale/ctf_RL/run_cvdc2.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:242
advantage_lists	/home/neale/ctf_RL/run_cvdc2.py	/^    advantage_lists = [[] for _ in range(num_type)]$/;"	variable	line:243
f1_list	/home/neale/ctf_RL/run_cvdc2.py	/^    f1_list = []$/;"	variable	line:244
f2_list	/home/neale/ctf_RL/run_cvdc2.py	/^    f2_list = []$/;"	variable	line:245
fc_list	/home/neale/ctf_RL/run_cvdc2.py	/^    fc_list = []$/;"	variable	line:246
atype	/home/neale/ctf_RL/run_cvdc2.py	/^            atype = agent_type_index[idx]$/;"	variable	line:249
reward	/home/neale/ctf_RL/run_cvdc2.py	/^            reward = traj[2]$/;"	variable	line:251
mask	/home/neale/ctf_RL/run_cvdc2.py	/^            mask = traj[3]$/;"	variable	line:252
critic	/home/neale/ctf_RL/run_cvdc2.py	/^            critic = traj[5]$/;"	variable	line:253
phi	/home/neale/ctf_RL/run_cvdc2.py	/^            phi = np.array(traj[7]).tolist()$/;"	variable	line:254
psi	/home/neale/ctf_RL/run_cvdc2.py	/^            psi = np.array(traj[8]).tolist()$/;"	variable	line:255
_critic	/home/neale/ctf_RL/run_cvdc2.py	/^            _critic = traj[9][-1]$/;"	variable	line:256
_psi	/home/neale/ctf_RL/run_cvdc2.py	/^            _psi = np.array(traj[10][-1])$/;"	variable	line:257
cent_state	/home/neale/ctf_RL/run_cvdc2.py	/^            cent_state = np.array(traj[14])$/;"	variable	line:259
env_critic	/home/neale/ctf_RL/run_cvdc2.py	/^            env_critic = env_critic["critic"].numpy()[:, 0].tolist()$/;"	variable	line:261
icritic	/home/neale/ctf_RL/run_cvdc2.py	/^            icritic = traj[12]$/;"	variable	line:266
dc	/home/neale/ctf_RL/run_cvdc2.py	/^            dc = np.array(critic[1:])-np.array(critic[:-1])$/;"	variable	line:268
dc1	/home/neale/ctf_RL/run_cvdc2.py	/^            dc1 = np.array(env_critic[1:])-np.array(env_critic[:-1])$/;"	variable	line:269
dc2	/home/neale/ctf_RL/run_cvdc2.py	/^            dc2 = np.array(icritic[1:])-np.array(icritic[:-1])$/;"	variable	line:270
normalize	/home/neale/ctf_RL/run_cvdc2.py	/^                normalize=False$/;"	variable	line:279
normalize	/home/neale/ctf_RL/run_cvdc2.py	/^                normalize=False,$/;"	variable	line:288
discount_adv	/home/neale/ctf_RL/run_cvdc2.py	/^                discount_adv=False,$/;"	variable	line:307
normalize	/home/neale/ctf_RL/run_cvdc2.py	/^                normalize=False,$/;"	variable	line:308
beta	/home/neale/ctf_RL/run_cvdc2.py	/^            beta = max(min((-0.9\/30000)*step + 1, 1.0),0.1)$/;"	variable	line:310
traj_buffer	/home/neale/ctf_RL/run_cvdc2.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:312
traj_buffer	/home/neale/ctf_RL/run_cvdc2.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:323
train_dataset	/home/neale/ctf_RL/run_cvdc2.py	/^        train_dataset = ($/;"	variable	line:324
tag	/home/neale/ctf_RL/run_cvdc2.py	/^            tag = "advantages\/"$/;"	variable	line:349
run_network	/home/neale/ctf_RL/run_cvdc2.py	/^def run_network(states):$/;"	function	line:361
batch	/home/neale/ctf_RL/run_cvdc2.py	/^batch = []$/;"	variable	line:400
dec_batch	/home/neale/ctf_RL/run_cvdc2.py	/^dec_batch = []$/;"	variable	line:401
log_save_analysis	/home/neale/ctf_RL/run_cvdc2.py	/^    log_save_analysis = False  #interval_flag(global_episodes, 1024 * 4, "save_log")$/;"	variable	line:405
episode_rew	/home/neale/ctf_RL/run_cvdc2.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:408
is_alive	/home/neale/ctf_RL/run_cvdc2.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:409
is_done	/home/neale/ctf_RL/run_cvdc2.py	/^    is_done = [False for env in range(NENV * num_agent)]$/;"	variable	line:410
trajs	/home/neale/ctf_RL/run_cvdc2.py	/^    trajs = [[Trajectory(depth=16) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:412
cent_trajs	/home/neale/ctf_RL/run_cvdc2.py	/^    cent_trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:413
s1	/home/neale/ctf_RL/run_cvdc2.py	/^    s1 = envs.reset(config_path=game_config, policy_red=policy.Roomba,)$/;"	variable	line:420
s1	/home/neale/ctf_RL/run_cvdc2.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:421
cent_s1	/home/neale/ctf_RL/run_cvdc2.py	/^    cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:422
reward_pred_list	/home/neale/ctf_RL/run_cvdc2.py	/^    reward_pred_list = []$/;"	variable	line:426
stime_roll	/home/neale/ctf_RL/run_cvdc2.py	/^    stime_roll = time.time()$/;"	variable	line:429
_states	/home/neale/ctf_RL/run_cvdc2.py	/^    _states = [[] for _ in range(NENV)]$/;"	variable	line:430
_agent1_r	/home/neale/ctf_RL/run_cvdc2.py	/^    _agent1_r = [[] for _ in range(NENV)]$/;"	variable	line:431
_agent2_r	/home/neale/ctf_RL/run_cvdc2.py	/^    _agent2_r = [[] for _ in range(NENV)]$/;"	variable	line:432
_agent3_r	/home/neale/ctf_RL/run_cvdc2.py	/^    _agent3_r = [[] for _ in range(NENV)]$/;"	variable	line:433
_agent1_o	/home/neale/ctf_RL/run_cvdc2.py	/^    _agent1_o = [[] for _ in range(NENV)]$/;"	variable	line:434
_agent2_o	/home/neale/ctf_RL/run_cvdc2.py	/^    _agent2_o = [[] for _ in range(NENV)]$/;"	variable	line:435
_agent3_o	/home/neale/ctf_RL/run_cvdc2.py	/^    _agent3_o = [[] for _ in range(NENV)]$/;"	variable	line:436
s0	/home/neale/ctf_RL/run_cvdc2.py	/^        s0 = s1$/;"	variable	line:439
a0	/home/neale/ctf_RL/run_cvdc2.py	/^        a0 = a1$/;"	variable	line:440
vg0	/home/neale/ctf_RL/run_cvdc2.py	/^        vg0 = vg1$/;"	variable	line:441
vc0	/home/neale/ctf_RL/run_cvdc2.py	/^        vc0 = vc1$/;"	variable	line:442
psi0	/home/neale/ctf_RL/run_cvdc2.py	/^        psi0 = psi1$/;"	variable	line:443
phi0	/home/neale/ctf_RL/run_cvdc2.py	/^        phi0 = phi1$/;"	variable	line:444
log_logits0	/home/neale/ctf_RL/run_cvdc2.py	/^        log_logits0 = log_logits1$/;"	variable	line:445
was_alive	/home/neale/ctf_RL/run_cvdc2.py	/^        was_alive = is_alive$/;"	variable	line:446
was_done	/home/neale/ctf_RL/run_cvdc2.py	/^        was_done = is_done$/;"	variable	line:447
cent_s0	/home/neale/ctf_RL/run_cvdc2.py	/^        cent_s0 = cent_s1$/;"	variable	line:448
is_alive	/home/neale/ctf_RL/run_cvdc2.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:452
s1	/home/neale/ctf_RL/run_cvdc2.py	/^        s1 = s1.astype(np.float32)  # Decentralize observation$/;"	variable	line:453
cent_s1	/home/neale/ctf_RL/run_cvdc2.py	/^        cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:454
idx	/home/neale/ctf_RL/run_cvdc2.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:465
etime_roll	/home/neale/ctf_RL/run_cvdc2.py	/^    etime_roll = time.time()$/;"	variable	line:507
stime_train	/home/neale/ctf_RL/run_cvdc2.py	/^        stime_train = time.time()$/;"	variable	line:512
log	/home/neale/ctf_RL/run_cvdc2.py	/^        log = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:513
log_image	/home/neale/ctf_RL/run_cvdc2.py	/^        log_image = interval_flag(global_episodes, 1024, "ima_log")$/;"	variable	line:514
epoch	/home/neale/ctf_RL/run_cvdc2.py	/^            epoch=epoch,$/;"	variable	line:517
batch_size	/home/neale/ctf_RL/run_cvdc2.py	/^            batch_size=minibatch_size,$/;"	variable	line:518
writer	/home/neale/ctf_RL/run_cvdc2.py	/^            writer=writer,$/;"	variable	line:519
log	/home/neale/ctf_RL/run_cvdc2.py	/^            log=log,$/;"	variable	line:520
step	/home/neale/ctf_RL/run_cvdc2.py	/^            step=global_episodes,$/;"	variable	line:521
log_image	/home/neale/ctf_RL/run_cvdc2.py	/^            log_image=log_image,$/;"	variable	line:522
etime_train	/home/neale/ctf_RL/run_cvdc2.py	/^        etime_train = time.time()$/;"	variable	line:524
dec_batch	/home/neale/ctf_RL/run_cvdc2.py	/^        dec_batch = []$/;"	variable	line:525
log_tc_on	/home/neale/ctf_RL/run_cvdc2.py	/^        log_tc_on = interval_flag(global_episodes, save_image_frequency, 'tc_log')$/;"	variable	line:530
batch	/home/neale/ctf_RL/run_cvdc2.py	/^        batch = []$/;"	variable	line:532
log_on	/home/neale/ctf_RL/run_cvdc2.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:543
tag	/home/neale/ctf_RL/run_cvdc2.py	/^            tag = "baseline_training\/"$/;"	variable	line:546
step	/home/neale/ctf_RL/run_cvdc2.py	/^                step=global_episodes,$/;"	variable	line:561
save_on	/home/neale/ctf_RL/run_cvdc2.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:565
fig	/home/neale/ctf_RL/run_cvdc2.py	/^        fig = plt.figure(figsize=(8, 9))$/;"	variable	line:571
widths	/home/neale/ctf_RL/run_cvdc2.py	/^        widths = [1.2, 0.5, 1.5, 1.5]$/;"	variable	line:572
heights	/home/neale/ctf_RL/run_cvdc2.py	/^        heights = [2, 2, 4, 4, 4]$/;"	variable	line:573
gs	/home/neale/ctf_RL/run_cvdc2.py	/^        gs = fig.add_gridspec($/;"	variable	line:574
ax_env	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_env = fig.add_subplot(gs[:2, :2])$/;"	variable	line:577
ax_value	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_value = fig.add_subplot(gs[:2, 2:])$/;"	variable	line:581
ax_agent1	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_agent1 = fig.add_subplot(gs[2, 0])$/;"	variable	line:584
ax_agent2	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_agent2 = fig.add_subplot(gs[3, 0])$/;"	variable	line:588
ax_agent3	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_agent3 = fig.add_subplot(gs[4, 0])$/;"	variable	line:592
ax_reward3	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_reward3 = fig.add_subplot(gs[4, 1:])$/;"	variable	line:596
ax_reward2	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_reward2 = fig.add_subplot(gs[3, 1:], sharex=ax_reward3)$/;"	variable	line:598
ax_reward1	/home/neale/ctf_RL/run_cvdc2.py	/^        ax_reward1 = fig.add_subplot(gs[2, 1:], sharex=ax_reward3)$/;"	variable	line:601
env_image	/home/neale/ctf_RL/run_cvdc2.py	/^        env_image = ax_env.imshow(np.ones((map_size, map_size, 3)), vmin=0, vmax=1)$/;"	variable	line:605
agent_obs1	/home/neale/ctf_RL/run_cvdc2.py	/^        agent_obs1 = ax_agent1.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:606
agent_obs2	/home/neale/ctf_RL/run_cvdc2.py	/^        agent_obs2 = ax_agent2.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:607
agent_obs3	/home/neale/ctf_RL/run_cvdc2.py	/^        agent_obs3 = ax_agent3.imshow(np.ones((59, 59, 3)), vmin=0, vmax=1)$/;"	variable	line:608
animate	/home/neale/ctf_RL/run_cvdc2.py	/^        def animate(i, info, critic, env_idx):$/;"	function	line:618
_states	/home/neale/ctf_RL/run_cvdc2.py	/^        _states = None$/;"	variable	line:669
_agent1_r	/home/neale/ctf_RL/run_cvdc2.py	/^        _agent1_r = None$/;"	variable	line:670
_agent2_r	/home/neale/ctf_RL/run_cvdc2.py	/^        _agent2_r = None$/;"	variable	line:671
_agent3_r	/home/neale/ctf_RL/run_cvdc2.py	/^        _agent3_r = None$/;"	variable	line:672
_agent1_o	/home/neale/ctf_RL/run_cvdc2.py	/^        _agent1_o = None$/;"	variable	line:673
_agent2_o	/home/neale/ctf_RL/run_cvdc2.py	/^        _agent2_o = None$/;"	variable	line:674
_agent3_o	/home/neale/ctf_RL/run_cvdc2.py	/^        _agent3_o = None$/;"	variable	line:675
anim	/home/neale/ctf_RL/run_cvdc2.py	/^        anim = None$/;"	variable	line:676
device_t	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^device_t = sys.argv[2]$/;"	variable	line:46
N_ATT	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^N_ATT = int(sys.argv[3])$/;"	variable	line:47
N_SCT	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^N_SCT = int(sys.argv[4])$/;"	variable	line:48
N_DEF	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^N_DEF = int(sys.argv[5])$/;"	variable	line:49
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^PROGBAR = False$/;"	variable	line:51
LOGDEVICE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^LOGDEVICE = False$/;"	variable	line:52
RBETA	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^RBETA = 0.8$/;"	variable	line:53
num_mode	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^num_mode = 3$/;"	variable	line:55
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^TRAIN_NAME = sys.argv[1]$/;"	variable	line:58
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:59
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:60
SAVE_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:61
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:62
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:63
NENV	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^NENV = 8 # multiprocessing.cpu_count() $/;"	variable	line:65
MODEL_LOAD_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^MODEL_LOAD_PATH = '.\/model\/fix_baseline_80\/' # initialize values$/;"	variable	line:67
ENV_SETTING_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^ENV_SETTING_PATH = 'setting_full.ini'$/;"	variable	line:68
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^config = configparser.ConfigParser()$/;"	variable	line:76
total_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:80
max_ep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:81
gamma	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:82
lambd	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:83
ppo_e	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:84
critic_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:85
entropy_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:86
lr_a	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:87
lr_c	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:88
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:91
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:92
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')*4$/;"	variable	line:93
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:94
action_space	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:97
vision_range	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:98
keep_frame	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:99
map_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:100
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^minibatch_size = 256$/;"	variable	line:103
epoch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^epoch = 2$/;"	variable	line:104
minbatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^minbatch_size = 2000$/;"	variable	line:105
nchannel	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^nchannel = 7 * keep_frame$/;"	variable	line:109
input_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:110
log_episodic_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^log_episodic_reward = MA(moving_average_step)$/;"	variable	line:113
log_length	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^log_length = MA(moving_average_step)$/;"	variable	line:114
log_winrate	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^log_winrate = MA(moving_average_step)$/;"	variable	line:115
map_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:118
smoothstep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^def smoothstep(x, lowx=0.0, highx=1.0, lowy=0, highy=1):$/;"	function	line:120
use_this_map	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^def use_this_map(x, max_episode, max_prob):$/;"	function	line:129
heur_policy_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^heur_policy_list = [policy.Patrol, policy.Roomba, policy.Defense, policy.Random, policy.AStar]$/;"	variable	line:137
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^heur_weight = [1,1,1,1,1]$/;"	variable	line:138
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^heur_weight = np.array(heur_weight) \/ sum(heur_weight)$/;"	variable	line:139
use_this_policy	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^def use_this_policy():$/;"	function	line:140
make_env	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^def make_env(map_size):$/;"	function	line:144
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:147
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^envs = SubprocVecEnv(envs, keep_frame)$/;"	variable	line:148
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:149
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:150
gpu_options	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^gpu_options = tf.GPUOptions(allow_growth=True)$/;"	variable	line:153
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOGDEVICE, allow_soft_placement=True)$/;"	variable	line:154
sess	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^sess = tf.Session(config=config)$/;"	variable	line:156
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^global_episodes = 0$/;"	variable	line:158
global_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:159
global_step_next	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:160
network	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    network = Network(in_size=input_size, action_size=action_space, sess=sess, num_mode=num_mode, scope='main')$/;"	variable	line:162
saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^saver = tf.train.Saver(max_to_keep=3, var_list=network.get_vars+[global_step])$/;"	variable	line:163
pretrained_vars	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^pretrained_vars = []$/;"	variable	line:166
pretrained_vars_name	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^pretrained_vars_name = []$/;"	variable	line:167
restoring_saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^restoring_saver = tf.train.Saver(max_to_keep=3, var_list=pretrained_vars)$/;"	variable	line:174
writer	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:176
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^global_episodes = sess.run(global_step)$/;"	variable	line:178
train	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^def train(trajs, bootstrap=0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None, mode=None):$/;"	function	line:180
reward_shape	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^def reward_shape(prev_red_alive, red_alive, done):$/;"	function	line:212
get_action	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^def get_action(states):$/;"	function	line:241
batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^batch_att = []$/;"	variable	line:256
batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^batch_sct = []$/;"	variable	line:257
batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^batch_def = []$/;"	variable	line:258
num_batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^num_batch_att = 0$/;"	variable	line:259
num_batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^num_batch_sct = 0$/;"	variable	line:260
num_batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^num_batch_def = 0$/;"	variable	line:261
progbar	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:263
log_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:265
log_image_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:266
save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:267
play_save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    play_save_on = interval_flag(global_episodes, 50000, 'replay_save')$/;"	variable	line:268
episode_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:271
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    prev_rew = np.zeros(NENV)$/;"	variable	line:272
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:273
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    was_alive_red = [True for agent in envs.get_team_red().flat]$/;"	variable	line:274
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:275
trajs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    trajs = [Trajectory(depth=5) for _ in range(num_blue*NENV)]$/;"	variable	line:277
s1	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    s1 = envs.reset($/;"	variable	line:280
config_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^            config_path=ENV_SETTING_PATH,$/;"	variable	line:281
custom_board	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^            custom_board=use_this_map(global_episodes, max_at, max_epsilon),$/;"	variable	line:282
policy_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^            policy_red=use_this_policy()$/;"	variable	line:283
stime	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    stime = time.time()$/;"	variable	line:288
s0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        s0 = s1$/;"	variable	line:290
logits	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        logits = logits1$/;"	variable	line:292
is_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:295
is_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        is_alive_red = [agent.isAlive for agent in envs.get_team_red().flat]$/;"	variable	line:296
env_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        env_reward = (raw_reward-prev_rew-0.01)\/100.0$/;"	variable	line:297
reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        reward = reward_shape(was_alive_red, is_alive_red, done)$/;"	variable	line:303
env_idx	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:313
reward_function	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^                reward_function = (RBETA) * reward[idx] + (1-RBETA) * env_reward[env_idx]$/;"	variable	line:315
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        prev_rew = raw_reward$/;"	variable	line:318
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        was_alive = is_alive$/;"	variable	line:319
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        was_alive_red = is_alive_red$/;"	variable	line:320
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        was_done = done$/;"	variable	line:321
j	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        j = 0$/;"	variable	line:332
stime	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        stime = time.time()$/;"	variable	line:347
batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        batch_att = []$/;"	variable	line:349
num_batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        num_batch_att = 0$/;"	variable	line:350
stime	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        stime = time.time()$/;"	variable	line:352
batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        batch_sct = []$/;"	variable	line:354
num_batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        num_batch_sct = 0$/;"	variable	line:355
stime	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        stime = time.time()$/;"	variable	line:357
batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        batch_def = []$/;"	variable	line:359
num_batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        num_batch_def = 0$/;"	variable	line:360
steps	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^    steps = []$/;"	variable	line:362
step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        step = sess.run(global_step)$/;"	variable	line:370
tag	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix.py	/^        tag = 'fix_baseline\/'$/;"	variable	line:371
device_ground	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^device_ground = '\/gpu:0'$/;"	variable	line:37
device_air	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^device_air = '\/gpu:0'$/;"	variable	line:38
PROGBAR	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^PROGBAR = True$/;"	variable	line:40
LOG_DEVICE	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^LOG_DEVICE = False$/;"	variable	line:41
OVERRIDE	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^OVERRIDE = False$/;"	variable	line:42
TRAIN_NAME	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^TRAIN_NAME = 'UAV_TRAIN_SF_4'$/;"	variable	line:45
LOG_PATH	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:46
MODEL_PATH	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:47
SAVE_PATH	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:48
MAP_PATH	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:49
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:50
NENV	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^NENV = multiprocessing.cpu_count() \/\/ 2$/;"	variable	line:52
env_setting_path	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^env_setting_path = 'uav_settings.ini'$/;"	variable	line:55
config_path	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^config_path = 'config.ini'$/;"	variable	line:63
config	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^config = configparser.ConfigParser()$/;"	variable	line:64
total_episodes	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^total_episodes = 1000000#config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:68
max_ep	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:69
gamma	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:70
lambd	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:71
ppo_e	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:72
critic_beta	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:73
entropy_beta	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:74
lr_a	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:75
lr_c	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:76
save_network_frequency	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:79
save_stat_frequency	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:80
save_image_frequency	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ') \/\/ 2$/;"	variable	line:81
moving_average_step	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:82
action_space	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:85
vision_range	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:86
keep_frame	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:87
map_size	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:88
minibatch_size	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^minibatch_size = 256$/;"	variable	line:91
epoch	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^epoch = 2$/;"	variable	line:92
minimum_batch_size	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^minimum_batch_size = 4096$/;"	variable	line:93
nchannel	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^nchannel = 6 * keep_frame$/;"	variable	line:98
input_size	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:99
log_episodic_reward	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:102
log_length	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:103
log_winrate	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:104
log_redwinrate	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:105
log_looptime	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:106
log_explore	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^log_explore = MovingAverage(moving_average_step)$/;"	variable	line:107
log_traintime	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:108
map_list	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:111
use_fair_map	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^def use_fair_map():$/;"	function	line:112
make_env	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^def make_env(map_size):$/;"	function	line:116
envs_list	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^envs_list = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:122
envs	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^envs = SubprocVecEnv(envs_list, keep_frame)$/;"	variable	line:123
num_blue	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:124
num_red	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:125
gpu_options	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)$/;"	variable	line:128
config	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOG_DEVICE, allow_soft_placement=True)$/;"	variable	line:129
progbar	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:132
sess	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^sess = tf.Session(config=config)$/;"	variable	line:134
global_step	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:136
global_step_next	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:137
network	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    network = Network(input_shape=input_size, action_size=action_space, scope='ground', sess=sess)$/;"	variable	line:139
network_air	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    network_air = Network(input_shape=input_size, action_size=action_space, scope='uav', sess=sess)$/;"	variable	line:141
global_episodes	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^global_episodes = 0$/;"	variable	line:144
saver	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^saver = tf.train.Saver(max_to_keep=3, keep_checkpoint_every_n_hours=4)$/;"	variable	line:145
global_episodes	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:150
writer	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:152
train	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^def train(nn, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:157
get_action	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^def get_action(states):$/;"	function	line:191
num_batch	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^num_batch = 0$/;"	variable	line:227
log_on	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:229
log_image_on	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:230
save_on	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:231
play_save_on	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    play_save_on = interval_flag(global_episodes, 5000, 'replay_save')$/;"	variable	line:232
episode_rew	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:235
was_alive	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    was_alive = [True for agent in range(NENV*(num_blue*num_red))]$/;"	variable	line:236
was_done	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:237
is_air	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    is_air = np.array([agent.is_air for agent in envs.get_team_blue().flat]).reshape([NENV, num_blue])$/;"	variable	line:238
is_air_red	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    is_air_red = np.array([agent.is_air for agent in envs.get_team_red().flat]).reshape([NENV, num_red])$/;"	variable	line:239
is_air	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    is_air = np.concatenate([is_air, is_air_red], axis=1).reshape([-1])$/;"	variable	line:240
trajs	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    trajs = [Trajectory(depth=5) for _ in range((num_blue+num_red)*NENV)] # Trajectory per agent$/;"	variable	line:242
s1	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    s1 = envs.reset(config_path=env_setting_path)$/;"	variable	line:245
stime_roll	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    stime_roll = time.time()$/;"	variable	line:249
s0	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        s0 = s1$/;"	variable	line:251
logits	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        logits = logits1$/;"	variable	line:253
reward_red	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        reward_red = np.array([i['red_reward'] for i in info])$/;"	variable	line:258
env_reward	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        env_reward = np.vstack((reward, reward_red)).T.reshape([-1])$/;"	variable	line:259
is_alive	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        is_alive = np.array([agent.isAlive for agent in envs.get_team_blue().flat]).reshape([NENV, num_blue])$/;"	variable	line:261
is_alive_red	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        is_alive_red = np.array([agent.isAlive for agent in envs.get_team_red().flat]).reshape([NENV, num_red])$/;"	variable	line:262
is_alive	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        is_alive = np.concatenate([is_alive, is_alive_red], axis=1).reshape([-1])$/;"	variable	line:263
was_done	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        was_done = np.array(was_done, dtype=bool)$/;"	variable	line:276
pre_air_reward	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        pre_air_reward = (np.isin(s1[::6,:,:,-12],-1).sum(axis=2).sum(axis=1) * (-0.1) + np.isin(s1[::6,:,:,-10], [1, -1]).astype(int).sum(axis=2).sum(axis=1)) * np.repeat(np.array(~was_done,dtype=int), 2)$/;"	variable	line:277
air_reward	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        air_reward = (np.isin(s1[::6,:,:,-6],-1).sum(axis=2).sum(axis=1) * (-0.1) + np.isin(s1[::6,:,:,-4], [1, -1]).astype(int).sum(axis=2).sum(axis=1)) * np.repeat(np.array(~was_done,dtype=int), 2)$/;"	variable	line:278
env_idx	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^            env_idx = idx \/\/ (num_blue+num_red)$/;"	variable	line:281
env_team_idx	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^            env_team_idx = idx \/\/ 6$/;"	variable	line:282
agent_reward	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^                    agent_reward = air_reward[env_team_idx] - pre_air_reward[env_team_idx]$/;"	variable	line:286
agent_reward	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^                    agent_reward = env_reward[env_team_idx]$/;"	variable	line:288
was_alive	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        was_alive = is_alive$/;"	variable	line:291
was_done	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        was_done = done$/;"	variable	line:292
explore_factor	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^            explore_factor = np.mean(air_reward)$/;"	variable	line:295
etime_roll	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    etime_roll = time.time()$/;"	variable	line:297
stime_train	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        stime_train = time.time()$/;"	variable	line:307
etime_train	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        etime_train = time.time()$/;"	variable	line:310
num_batch	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        num_batch = 0$/;"	variable	line:312
steps	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^    steps = []$/;"	variable	line:315
tag	/home/neale/ctf_RL/archive_code/uav_trainer.py	/^        tag = 'uav_training\/'$/;"	variable	line:332
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^PROGBAR = True$/;"	variable	line:36
LOG_DEVICE	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^LOG_DEVICE = False$/;"	variable	line:37
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^TRAIN_NAME = 'ppo_baseline'$/;"	variable	line:40
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:41
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:42
SAVE_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:43
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:44
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^GPU_CAPACITY = 0.90$/;"	variable	line:45
NENV	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^NENV = multiprocessing.cpu_count()  $/;"	variable	line:47
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^env_setting_path = 'setting_full_selfplay.ini'$/;"	variable	line:50
config	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^config = configparser.ConfigParser()$/;"	variable	line:53
total_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^total_episodes = 300000#config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:57
max_ep	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:58
gamma	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:59
lambd	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:60
ppo_e	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:61
critic_beta	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:62
entropy_beta	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:63
lr_a	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:64
lr_c	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:65
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:68
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:69
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:70
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:71
action_space	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:74
vision_range	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:75
keep_frame	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:76
map_size	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:77
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^minibatch_size = 256$/;"	variable	line:80
epoch	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^epoch = 2$/;"	variable	line:81
minimum_batch_size	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^minimum_batch_size = 3000$/;"	variable	line:82
nchannel	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^nchannel = 7 * keep_frame$/;"	variable	line:86
input_size	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:87
selfplay_reload	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^selfplay_reload = 20000$/;"	variable	line:88
log_episodic_reward	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:91
log_length	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:92
log_winrate	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:93
log_redwinrate	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:94
log_looptime	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:95
log_traintime	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:96
map_list	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:99
max_epsilon	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^max_epsilon = 0.70;$/;"	variable	line:100
use_this_map	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^def use_this_map():$/;"	function	line:101
heur_policy_list	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^heur_policy_list = [policy.Patrol, policy.Roomba, policy.Defense, policy.Random, policy.AStar]$/;"	variable	line:108
heur_weight	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^heur_weight = [1,1,1,1,1]$/;"	variable	line:109
heur_weight	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^heur_weight = np.array(heur_weight) \/ sum(heur_weight)$/;"	variable	line:110
use_this_policy	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^def use_this_policy():$/;"	function	line:111
make_env	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^def make_env(map_size):$/;"	function	line:115
envs	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:121
envs	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^envs = SubprocVecEnv(envs, keep_frame)$/;"	variable	line:122
num_blue	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:123
num_red	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:124
gpu_options	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)$/;"	variable	line:127
config	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOG_DEVICE, allow_soft_placement=True)$/;"	variable	line:128
progbar	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:131
sess	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^sess = tf.Session(config=config)$/;"	variable	line:133
global_step	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:135
global_step_next	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:136
network	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    network = Network(input_shape=input_size, action_size=action_space, scope='main', sess=sess)$/;"	variable	line:138
global_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^global_episodes = 0$/;"	variable	line:141
saver	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^saver = tf.train.Saver(max_to_keep=3, var_list=network.get_vars+[global_step])$/;"	variable	line:142
global_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^global_episodes = sess.run(global_step)$/;"	variable	line:144
writer	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:146
forward_network	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^forward_network = TrainedNetwork($/;"	variable	line:150
model_name	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        model_name=TRAIN_NAME,$/;"	variable	line:151
input_tensor	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        input_tensor='main\/state:0',$/;"	variable	line:152
output_tensor	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        output_tensor='main\/PPO\/activation\/Softmax:0',$/;"	variable	line:153
import_scope	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        import_scope='forward',$/;"	variable	line:154
device	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        device='\/device:GPU:0'$/;"	variable	line:155
prob2act	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^def prob2act(prob):$/;"	function	line:158
train	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^def train(trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:163
get_action	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^def get_action(states):$/;"	function	line:195
batch	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^batch = []$/;"	variable	line:218
num_batch	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^num_batch = 0$/;"	variable	line:219
log_on	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:221
log_image_on	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:222
save_on	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:223
reload_on	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    reload_on = interval_flag(global_episodes,selfplay_reload, 'reload')$/;"	variable	line:224
play_save_on	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    play_save_on = interval_flag(global_episodes, 50000, 'replay_save')$/;"	variable	line:225
episode_rew	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:228
prev_rew	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    prev_rew = np.zeros(NENV)$/;"	variable	line:229
was_alive	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:230
was_done	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:231
trajs	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    trajs = [Trajectory(depth=5) for _ in range(num_blue*NENV)]$/;"	variable	line:233
s1	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    s1 = envs.reset($/;"	variable	line:236
config_path	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^            config_path=env_setting_path,$/;"	variable	line:237
custom_board	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^            custom_board=use_this_map(),$/;"	variable	line:238
policy_red	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^            policy_red=use_this_policy()$/;"	variable	line:239
stime_roll	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    stime_roll = time.time()$/;"	variable	line:244
s0	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        s0 = s1$/;"	variable	line:246
logits	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        logits = logits1$/;"	variable	line:248
is_alive	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:251
reward	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        reward = (raw_reward - prev_rew - 0.01)\/100.0$/;"	variable	line:252
env_idx	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:264
prev_rew	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        prev_rew = raw_reward$/;"	variable	line:268
was_alive	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        was_alive = is_alive$/;"	variable	line:269
was_done	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        was_done = done$/;"	variable	line:270
etime_roll	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    etime_roll = time.time()$/;"	variable	line:274
stime_train	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        stime_train = time.time()$/;"	variable	line:279
etime_train	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        etime_train = time.time()$/;"	variable	line:281
batch	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        batch = []$/;"	variable	line:282
num_batch	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        num_batch = 0$/;"	variable	line:283
steps	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^    steps = []$/;"	variable	line:286
tag	/home/neale/ctf_RL/archive_code/ppo_trainer_selfplay.py	/^        tag = 'baseline_training\/'$/;"	variable	line:302
target_setting_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^target_setting_path = sys.argv[1]$/;"	variable	line:36
LOGDEVICE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^LOGDEVICE = False$/;"	variable	line:38
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^PROGBAR = False$/;"	variable	line:39
TRAIN_SUBP	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^TRAIN_SUBP = True$/;"	variable	line:40
CONTINUE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^CONTINUE = False$/;"	variable	line:41
num_mode	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^num_mode = 3$/;"	variable	line:43
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^TRAIN_NAME = sys.argv[2]$/;"	variable	line:46
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:47
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:48
SAVE_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:49
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:50
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^GPU_CAPACITY = 0.90$/;"	variable	line:51
NENV	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^NENV = multiprocessing.cpu_count()  $/;"	variable	line:52
MODEL_LOAD_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^MODEL_LOAD_PATH = '.\/model\/confid_baseline_namsong'$/;"	variable	line:54
SWITCH_EP	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^SWITCH_EP = 10000$/;"	variable	line:55
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^env_setting_path = 'setting_full.ini'$/;"	variable	line:56
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^config = configparser.ConfigParser()$/;"	variable	line:64
total_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:68
max_ep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:69
gamma	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:70
lambd	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:71
ppo_e	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:72
critic_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:73
lr_a	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:74
lr_c	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:75
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:78
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:79
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:80
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:81
action_space	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:84
vision_range	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:85
keep_frame	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:86
map_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:87
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^minibatch_size = 128$/;"	variable	line:90
epoch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^epoch = 2$/;"	variable	line:91
batch_memory_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^batch_memory_size = 4000$/;"	variable	line:92
nchannel	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^nchannel = 7 * keep_frame$/;"	variable	line:96
input_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:97
log_episodic_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:100
log_length	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:101
log_winrate	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:102
map_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:105
smoothstep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^def smoothstep(x, lowx=0.0, highx=1.0, lowy=0, highy=1):$/;"	function	line:107
use_this_map	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^def use_this_map(x, max_episode, max_prob):$/;"	function	line:116
heur_policy_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^heur_policy_list = [policy.Patrol, policy.Roomba, policy.Defense, policy.Random, policy.AStar]$/;"	variable	line:124
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^heur_weight = [1,1,1,1,1]$/;"	variable	line:125
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^heur_weight = np.array(heur_weight) \/ sum(heur_weight)$/;"	variable	line:126
use_this_policy	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^def use_this_policy():$/;"	function	line:127
make_env	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^def make_env(map_size):$/;"	function	line:131
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:134
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^envs = SubprocVecEnv(envs, keep_frame)$/;"	variable	line:135
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:136
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:137
gpu_options	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^gpu_options = tf.GPUOptions(allow_growth=True)$/;"	variable	line:140
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOGDEVICE)$/;"	variable	line:141
sess	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^sess = tf.Session(config=config)$/;"	variable	line:143
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^global_episodes = 0$/;"	variable	line:145
global_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:146
global_step_next	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:147
network	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^network = Network(in_size=input_size, action_size=action_space, sess=sess, num_mode=num_mode, scope='main')$/;"	variable	line:148
meta_network	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^meta_network = MetaNetwork(input_shape=input_size, action_size=num_mode, sess=sess, scope='meta')$/;"	variable	line:149
saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^saver = tf.train.Saver(max_to_keep=3, var_list=network.get_vars+meta_network.get_vars+[global_step])$/;"	variable	line:151
writer	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:155
meta_train	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^def meta_train(trajs, bootstrap=0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:158
reward_shape	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^def reward_shape(prev_red_alive, red_alive, done):$/;"	function	line:242
get_action	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^def get_action(states, initial=False):$/;"	function	line:270
batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^batch = []$/;"	variable	line:281
num_batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^num_batch = 0$/;"	variable	line:282
progbar	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:284
log_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:286
log_image_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:287
save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:288
reload_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    reload_on = False # interval_flag(global_episodes,selfplay_reload, 'reload')$/;"	variable	line:289
play_save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    play_save_on = interval_flag(global_episodes, 50000, 'replay_save')$/;"	variable	line:290
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        env_setting_path = target_setting_path$/;"	variable	line:294
s1	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    s1 = envs.reset($/;"	variable	line:295
config_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^            config_path=env_setting_path,$/;"	variable	line:296
custom_board	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^            custom_board=use_this_map(global_episodes, max_at, max_epsilon),$/;"	variable	line:297
policy_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^            policy_red=use_this_policy()$/;"	variable	line:298
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:300
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    num_red = len(envs.get_team_red()[0])$/;"	variable	line:301
episode_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:304
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    prev_rew = np.zeros(NENV)$/;"	variable	line:305
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:306
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    was_alive_red = [True for agent in envs.get_team_red().flat]$/;"	variable	line:307
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:308
trajs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    trajs = [Trajectory(depth=11) for _ in range(num_blue*NENV)]$/;"	variable	line:310
cumul_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    cumul_reward = np.zeros(NENV)$/;"	variable	line:315
s0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        s0 = s1$/;"	variable	line:317
logits0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        logits0 = logits1$/;"	variable	line:319
sub_logits0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        sub_logits0 = sub_logits1$/;"	variable	line:321
is_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:324
is_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        is_alive_red = [agent.isAlive for agent in envs.get_team_red().flat]$/;"	variable	line:325
env_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        env_reward = (raw_reward - prev_rew - 0.01)\/100$/;"	variable	line:326
task_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        task_reward = reward_shape(was_alive_red, is_alive_red, done)$/;"	variable	line:333
env_idx	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:343
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        prev_rew = raw_reward$/;"	variable	line:359
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        was_alive = is_alive$/;"	variable	line:360
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        was_alive_red = is_alive_red$/;"	variable	line:361
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        was_done = done$/;"	variable	line:362
batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        batch = []$/;"	variable	line:377
num_batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        num_batch = 0$/;"	variable	line:378
steps	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^    steps = []$/;"	variable	line:380
tag	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid_target.py	/^        tag = 'kerasTest\/'$/;"	variable	line:388
physical_devices	/home/neale/ctf_RL/archive_code/run_sfk.py	/^physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:16
PROGBAR	/home/neale/ctf_RL/archive_code/run_sfk.py	/^PROGBAR = True$/;"	variable	line:39
LOG_DEVICE	/home/neale/ctf_RL/archive_code/run_sfk.py	/^LOG_DEVICE = False$/;"	variable	line:40
OVERRIDE	/home/neale/ctf_RL/archive_code/run_sfk.py	/^OVERRIDE = False$/;"	variable	line:41
TRAIN_NAME	/home/neale/ctf_RL/archive_code/run_sfk.py	/^TRAIN_NAME = 'DIST_SFK_03'$/;"	variable	line:44
TRAIN_TAG	/home/neale/ctf_RL/archive_code/run_sfk.py	/^TRAIN_TAG = 'Central value decentralized control, '+TRAIN_NAME$/;"	variable	line:45
LOG_PATH	/home/neale/ctf_RL/archive_code/run_sfk.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:46
MODEL_PATH	/home/neale/ctf_RL/archive_code/run_sfk.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:47
MAP_PATH	/home/neale/ctf_RL/archive_code/run_sfk.py	/^MAP_PATH = '.\/fair_3g_40'$/;"	variable	line:48
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/run_sfk.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:49
NENV	/home/neale/ctf_RL/archive_code/run_sfk.py	/^NENV = multiprocessing.cpu_count()$/;"	variable	line:51
env_setting_path	/home/neale/ctf_RL/archive_code/run_sfk.py	/^env_setting_path = 'env_setting_3v3_3g_partial.ini'$/;"	variable	line:54
config_path	/home/neale/ctf_RL/archive_code/run_sfk.py	/^config_path = 'config.ini'$/;"	variable	line:61
config	/home/neale/ctf_RL/archive_code/run_sfk.py	/^config = configparser.ConfigParser()$/;"	variable	line:62
total_episodes	/home/neale/ctf_RL/archive_code/run_sfk.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:66
max_ep	/home/neale/ctf_RL/archive_code/run_sfk.py	/^max_ep         = 200#config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:67
gamma	/home/neale/ctf_RL/archive_code/run_sfk.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:68
lambd	/home/neale/ctf_RL/archive_code/run_sfk.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:69
ppo_e	/home/neale/ctf_RL/archive_code/run_sfk.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:70
critic_beta	/home/neale/ctf_RL/archive_code/run_sfk.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:71
entropy_beta	/home/neale/ctf_RL/archive_code/run_sfk.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:72
lr_a	/home/neale/ctf_RL/archive_code/run_sfk.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:73
lr_c	/home/neale/ctf_RL/archive_code/run_sfk.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:74
save_network_frequency	/home/neale/ctf_RL/archive_code/run_sfk.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:77
save_stat_frequency	/home/neale/ctf_RL/archive_code/run_sfk.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:78
save_image_frequency	/home/neale/ctf_RL/archive_code/run_sfk.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:79
moving_average_step	/home/neale/ctf_RL/archive_code/run_sfk.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:80
action_space	/home/neale/ctf_RL/archive_code/run_sfk.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:83
vision_range	/home/neale/ctf_RL/archive_code/run_sfk.py	/^vision_range = 39#config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:84
keep_frame	/home/neale/ctf_RL/archive_code/run_sfk.py	/^keep_frame   = 1#config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:85
map_size	/home/neale/ctf_RL/archive_code/run_sfk.py	/^map_size     = 40#config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:86
minibatch_size	/home/neale/ctf_RL/archive_code/run_sfk.py	/^minibatch_size = 256$/;"	variable	line:89
epoch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^epoch = 2$/;"	variable	line:90
minimum_batch_size	/home/neale/ctf_RL/archive_code/run_sfk.py	/^minimum_batch_size = 1024 * 4$/;"	variable	line:91
nchannel	/home/neale/ctf_RL/archive_code/run_sfk.py	/^nchannel = 6 * keep_frame$/;"	variable	line:96
input_size	/home/neale/ctf_RL/archive_code/run_sfk.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:97
cent_input_size	/home/neale/ctf_RL/archive_code/run_sfk.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:98
log_episodic_reward	/home/neale/ctf_RL/archive_code/run_sfk.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:101
log_winrate	/home/neale/ctf_RL/archive_code/run_sfk.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:102
log_redwinrate	/home/neale/ctf_RL/archive_code/run_sfk.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:103
log_looptime	/home/neale/ctf_RL/archive_code/run_sfk.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:104
log_traintime	/home/neale/ctf_RL/archive_code/run_sfk.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:105
map_list	/home/neale/ctf_RL/archive_code/run_sfk.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH) if path[:5]=='board']$/;"	variable	line:108
make_env	/home/neale/ctf_RL/archive_code/run_sfk.py	/^def make_env(map_size):$/;"	function	line:109
envs	/home/neale/ctf_RL/archive_code/run_sfk.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:112
envs	/home/neale/ctf_RL/archive_code/run_sfk.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:113
num_blue	/home/neale/ctf_RL/archive_code/run_sfk.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:114
num_red	/home/neale/ctf_RL/archive_code/run_sfk.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:115
num_agent	/home/neale/ctf_RL/archive_code/run_sfk.py	/^num_agent = num_blue#+num_red$/;"	variable	line:116
progbar	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name=TRAIN_TAG)$/;"	variable	line:119
atoms	/home/neale/ctf_RL/archive_code/run_sfk.py	/^atoms = 8$/;"	variable	line:121
network	/home/neale/ctf_RL/archive_code/run_sfk.py	/^network = Network($/;"	variable	line:122
central_obs_shape	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        central_obs_shape=cent_input_size,$/;"	variable	line:123
decentral_obs_shape	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        decentral_obs_shape=input_size,$/;"	variable	line:124
action_size	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        action_size=action_space, $/;"	variable	line:125
atoms	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        atoms=atoms,$/;"	variable	line:126
save_path	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        save_path=MODEL_PATH)$/;"	variable	line:127
global_episodes	/home/neale/ctf_RL/archive_code/run_sfk.py	/^global_episodes = network.initiate()$/;"	variable	line:130
writer	/home/neale/ctf_RL/archive_code/run_sfk.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:132
train_central	/home/neale/ctf_RL/archive_code/run_sfk.py	/^def train_central(network, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, step=None):$/;"	function	line:137
train_decentral	/home/neale/ctf_RL/archive_code/run_sfk.py	/^def train_decentral(network, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, step=None):$/;"	function	line:195
train_reward_prediction	/home/neale/ctf_RL/archive_code/run_sfk.py	/^def train_reward_prediction(network, traj, epoch, batch_size, writer=None, log=False, step=None):$/;"	function	line:247
train_ma_value	/home/neale/ctf_RL/archive_code/run_sfk.py	/^def train_ma_value(network, trajs, bootstrap=0.0, writer=None, log=False, step=None):$/;"	function	line:267
get_action	/home/neale/ctf_RL/archive_code/run_sfk.py	/^def get_action(log_logits):$/;"	function	line:299
reward_training_buffer	/home/neale/ctf_RL/archive_code/run_sfk.py	/^reward_training_buffer = Trajectory(depth=4) # Centralized$/;"	variable	line:304
batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^batch = []$/;"	variable	line:305
dec_batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^dec_batch = []$/;"	variable	line:306
ma_batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^ma_batch = []$/;"	variable	line:307
num_batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^num_batch = 0$/;"	variable	line:308
episode_rew	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:312
is_alive	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:313
is_done	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    is_done = [False for env in range(NENV*num_agent)]$/;"	variable	line:314
trajs	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    trajs = [Trajectory(depth=10) for _ in range(NENV*num_agent)]$/;"	variable	line:316
ma_trajs	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    ma_trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:317
cent_trajs	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    cent_trajs = [Trajectory(depth=11) for _ in range(NENV)]$/;"	variable	line:318
bmean1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    bmean1 = np.zeros([NENV, atoms], dtype=np.float32)$/;"	variable	line:319
blogvar1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    blogvar1 = np.zeros([NENV, atoms], dtype=np.float32)$/;"	variable	line:320
s1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    s1 = envs.reset($/;"	variable	line:323
map_pool	/home/neale/ctf_RL/archive_code/run_sfk.py	/^            map_pool=map_list,$/;"	variable	line:324
config_path	/home/neale/ctf_RL/archive_code/run_sfk.py	/^            config_path=env_setting_path,$/;"	variable	line:325
policy_red	/home/neale/ctf_RL/archive_code/run_sfk.py	/^            policy_red=policy.Roomba,$/;"	variable	line:326
mode	/home/neale/ctf_RL/archive_code/run_sfk.py	/^            mode='continue')$/;"	variable	line:328
s1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:329
cent_s1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    cent_s1 = envs.get_obs_blue().astype(np.float32) # Centralized$/;"	variable	line:330
belief	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    belief = env_feature['latent'].numpy()$/;"	variable	line:333
belief	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    belief = np.repeat(belief, num_agent, axis=0)$/;"	variable	line:334
stime_roll	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    stime_roll = time.time()$/;"	variable	line:339
s0	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        s0 = s1$/;"	variable	line:341
a0	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        a0 = a1$/;"	variable	line:342
v0	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        v0 = critic['critic'].numpy()[:,0]$/;"	variable	line:343
psi0	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        psi0 = critic['psi'].numpy()$/;"	variable	line:344
log_logits0	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        log_logits0 = actor['log_softmax'].numpy()$/;"	variable	line:345
cent_s0	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        cent_s0 = cent_s1$/;"	variable	line:346
cent_v0	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        cent_v0 = env_critic['critic'].numpy()[:,0]$/;"	variable	line:348
cent_psi	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        cent_psi = env_critic['psi'].numpy()$/;"	variable	line:349
cent_pmean	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        cent_pmean = env_pred_feature['pred_z_mean'].numpy()$/;"	variable	line:350
cent_plogvar	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        cent_plogvar = env_pred_feature['pred_z_log_var'].numpy()$/;"	variable	line:351
was_alive	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        was_alive = is_alive$/;"	variable	line:352
was_done	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        was_done = is_done$/;"	variable	line:353
is_alive	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:357
s1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        s1 = s1.astype(np.float32) # Decentralize observation$/;"	variable	line:358
cent_s1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        cent_s1 = envs.get_obs_blue().astype(np.float32) # Centralized$/;"	variable	line:359
cent_phi1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        cent_phi1 = env_critic['phi'].numpy()$/;"	variable	line:364
bmean1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        bmean1 = env_feature['z_mean'].numpy() # stop gradient$/;"	variable	line:365
blogvar1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        blogvar1 = env_feature['z_log_var'].numpy()$/;"	variable	line:366
belief	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        belief = env_feature['latent'].numpy()$/;"	variable	line:367
belief	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        belief = np.repeat(belief, num_agent, axis=0)$/;"	variable	line:368
phi1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        phi1 = critic['phi'].numpy()$/;"	variable	line:372
reward_pred1	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        reward_pred1 = critic['reward_predict'].numpy()[:,0]$/;"	variable	line:373
env_idx	/home/neale/ctf_RL/archive_code/run_sfk.py	/^            env_idx = idx \/\/ num_agent$/;"	variable	line:377
etime_roll	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    etime_roll = time.time()$/;"	variable	line:422
stime_train	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        stime_train = time.time()$/;"	variable	line:437
log_image_on	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:438
etime_train	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        etime_train = time.time()$/;"	variable	line:442
batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        batch = []$/;"	variable	line:443
dec_batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        dec_batch = []$/;"	variable	line:444
ma_batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        ma_batch = []$/;"	variable	line:445
num_batch	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        num_batch = 0$/;"	variable	line:446
log_rt_on	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        log_rt_on = interval_flag(global_episodes, save_image_frequency, 'rt_log')$/;"	variable	line:450
temp_buffer	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        temp_buffer = Trajectory(depth=4)$/;"	variable	line:453
r	/home/neale/ctf_RL/archive_code/run_sfk.py	/^            r = [col[i] for col in reward_training_buffer.buffer]$/;"	variable	line:455
reward_training_buffer	/home/neale/ctf_RL/archive_code/run_sfk.py	/^        reward_training_buffer = temp_buffer$/;"	variable	line:459
log_on	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:472
tag	/home/neale/ctf_RL/archive_code/run_sfk.py	/^            tag = 'baseline_training\/'$/;"	variable	line:475
save_on	/home/neale/ctf_RL/archive_code/run_sfk.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:483
target_setting_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^target_setting_path = sys.argv[6]$/;"	variable	line:45
device_t	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^device_t = sys.argv[2]$/;"	variable	line:46
N_ATT	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^N_ATT = int(sys.argv[3])$/;"	variable	line:48
N_SCT	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^N_SCT = int(sys.argv[4])$/;"	variable	line:49
N_DEF	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^N_DEF = int(sys.argv[5])$/;"	variable	line:50
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^PROGBAR = False$/;"	variable	line:52
LOGDEVICE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^LOGDEVICE = False$/;"	variable	line:53
CONTINUE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^CONTINUE = True$/;"	variable	line:54
RBETA	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^RBETA = 0.5$/;"	variable	line:55
num_mode	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^num_mode = 3$/;"	variable	line:57
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^TRAIN_NAME = sys.argv[1]$/;"	variable	line:60
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:61
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:62
SAVE_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:63
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:64
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:65
NENV	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^NENV = 8 # multiprocessing.cpu_count() $/;"	variable	line:67
SWITCH_EP	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^SWITCH_EP = 0$/;"	variable	line:68
MODEL_LOAD_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^MODEL_LOAD_PATH = '.\/model\/fix_baseline_team{}{}{}_r1\/'.format(N_ATT, N_SCT, N_DEF) # initialize values$/;"	variable	line:71
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^env_setting_path = 'setting_full.ini'$/;"	variable	line:74
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^config = configparser.ConfigParser()$/;"	variable	line:82
total_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:86
max_ep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:87
gamma	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:88
lambd	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:89
ppo_e	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:90
critic_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:91
entropy_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:92
lr_a	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:93
lr_c	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:94
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:97
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:98
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:99
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:100
action_space	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:103
vision_range	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:104
keep_frame	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:105
map_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:106
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^minibatch_size = 256$/;"	variable	line:109
epoch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^epoch = 3$/;"	variable	line:110
minbatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^minbatch_size = 2048$/;"	variable	line:111
nchannel	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^nchannel = 7 * keep_frame$/;"	variable	line:115
input_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:116
log_episodic_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^log_episodic_reward = MA(moving_average_step)$/;"	variable	line:119
log_length	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^log_length = MA(moving_average_step)$/;"	variable	line:120
log_winrate	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^log_winrate = MA(moving_average_step)$/;"	variable	line:121
log_attack_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^log_attack_reward = MA(moving_average_step)$/;"	variable	line:123
log_scout_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^log_scout_reward = MA(moving_average_step)$/;"	variable	line:124
log_defense_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^log_defense_reward = MA(moving_average_step)$/;"	variable	line:125
map_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:128
smoothstep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def smoothstep(x, lowx=0.0, highx=1.0, lowy=0, highy=1):$/;"	function	line:130
use_this_map	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def use_this_map(x, max_episode, max_prob):$/;"	function	line:139
heur_policy_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^heur_policy_list = [policy.Patrol, policy.Roomba, policy.Defense, policy.Random, policy.AStar]$/;"	variable	line:147
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^heur_weight = [1,1,1,1,1]$/;"	variable	line:148
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^heur_weight = np.array(heur_weight) \/ sum(heur_weight)$/;"	variable	line:149
use_this_policy	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def use_this_policy():$/;"	function	line:150
make_env	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def make_env(map_size):$/;"	function	line:154
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:157
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^envs = SubprocVecEnv(envs, keep_frame)$/;"	variable	line:158
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:159
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:160
gpu_options	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^gpu_options = tf.GPUOptions(allow_growth=True)$/;"	variable	line:163
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOGDEVICE, allow_soft_placement=True)$/;"	variable	line:164
sess	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^sess = tf.Session(config=config)$/;"	variable	line:166
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^global_episodes = 0$/;"	variable	line:168
global_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:169
global_step_next	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:170
network	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    network = Network(in_size=input_size, action_size=action_space, sess=sess, num_mode=num_mode, scope='main')$/;"	variable	line:172
saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^saver = tf.train.Saver(max_to_keep=3, var_list=network.get_vars+[global_step])$/;"	variable	line:173
pretrained_vars	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^pretrained_vars = []$/;"	variable	line:176
pretrained_vars_name	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^pretrained_vars_name = []$/;"	variable	line:177
restoring_saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^restoring_saver = tf.train.Saver(max_to_keep=3, var_list=pretrained_vars)$/;"	variable	line:184
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:187
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:190
writer	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:191
train	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def train(trajs, bootstrap=0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None, mode=None):$/;"	function	line:195
reward_shape	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def reward_shape(prev_red_alive, red_alive, done):$/;"	function	line:227
ghost_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def ghost_reward(prev_red_alive, red_alive, done):$/;"	function	line:255
get_action	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^def get_action(states):$/;"	function	line:286
batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^batch_att = []$/;"	variable	line:301
batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^batch_sct = []$/;"	variable	line:302
batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^batch_def = []$/;"	variable	line:303
num_batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^num_batch_att = 0$/;"	variable	line:304
num_batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^num_batch_sct = 0$/;"	variable	line:305
num_batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^num_batch_def = 0$/;"	variable	line:306
progbar	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:308
log_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:310
log_image_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:311
save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:312
play_save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    play_save_on = interval_flag(global_episodes, 50000, 'replay_save')$/;"	variable	line:313
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        env_setting_path = target_setting_path$/;"	variable	line:317
s1	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    s1 = envs.reset($/;"	variable	line:318
config_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^            config_path=env_setting_path,$/;"	variable	line:319
custom_board	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^            custom_board=use_this_map(global_episodes, max_at, max_epsilon),$/;"	variable	line:320
policy_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^            policy_red=use_this_policy()$/;"	variable	line:321
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:323
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    num_red = len(envs.get_team_red()[0])$/;"	variable	line:324
episode_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:326
case_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    case_rew = [np.zeros(NENV) for _ in range(3)]$/;"	variable	line:327
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    prev_rew = np.zeros(NENV)$/;"	variable	line:328
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:329
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    was_alive_red = [True for agent in envs.get_team_red().flat]$/;"	variable	line:330
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:331
trajs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    trajs = [Trajectory(depth=5) for _ in range(num_blue*NENV)]$/;"	variable	line:333
stime	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    stime = time.time()$/;"	variable	line:339
s0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        s0 = s1$/;"	variable	line:341
logits	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        logits = logits1$/;"	variable	line:343
is_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:346
is_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        is_alive_red = [agent.isAlive for agent in envs.get_team_red().flat]$/;"	variable	line:347
env_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        env_reward = (raw_reward-prev_rew-0.01)\/100.0$/;"	variable	line:348
reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        reward = reward_shape(was_alive_red, is_alive_red, done)$/;"	variable	line:354
shaped_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        shaped_reward = ghost_reward(was_alive_red, is_alive_red, done)$/;"	variable	line:357
env_idx	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:370
reward_function	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^                reward_function = (RBETA) * reward[idx] + (1-RBETA) * env_reward[env_idx]$/;"	variable	line:372
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        prev_rew = raw_reward$/;"	variable	line:375
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        was_alive = is_alive$/;"	variable	line:376
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        was_alive_red = is_alive_red$/;"	variable	line:377
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        was_done = done$/;"	variable	line:378
j	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        j = 0$/;"	variable	line:389
stime	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        stime = time.time()$/;"	variable	line:404
batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        batch_att = []$/;"	variable	line:406
num_batch_att	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        num_batch_att = 0$/;"	variable	line:407
batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        batch_sct = []$/;"	variable	line:410
num_batch_sct	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        num_batch_sct = 0$/;"	variable	line:411
batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        batch_def = []$/;"	variable	line:414
num_batch_def	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        num_batch_def = 0$/;"	variable	line:415
steps	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^    steps = []$/;"	variable	line:417
step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        step = sess.run(global_step)$/;"	variable	line:429
tag	/home/neale/ctf_RL/archive_code/ppo_subpolicy_fix_target.py	/^        tag = 'kerasTest\/'$/;"	variable	line:430
device_ground	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^device_ground = '\/gpu:0'$/;"	variable	line:37
device_air	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^device_air = '\/gpu:1'$/;"	variable	line:38
PROGBAR	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^PROGBAR = True$/;"	variable	line:40
LOG_DEVICE	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^LOG_DEVICE = False$/;"	variable	line:41
OVERRIDE	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^OVERRIDE = False$/;"	variable	line:42
TRAIN_NAME	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^TRAIN_NAME = 'SF_TRAIN_softmax_01'$/;"	variable	line:45
LOG_PATH	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:46
MODEL_PATH	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:47
SAVE_PATH	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:48
MAP_PATH	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:49
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:50
NENV	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^NENV = multiprocessing.cpu_count() \/\/ 2$/;"	variable	line:52
env_setting_path	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^env_setting_path = 'uav_settings.ini'$/;"	variable	line:55
config_path	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^config_path = 'config.ini'$/;"	variable	line:63
config	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^config = configparser.ConfigParser()$/;"	variable	line:64
total_episodes	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^total_episodes = 1000000#config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:68
max_ep	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:69
gamma	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:70
lambd	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:71
ppo_e	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:72
critic_beta	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:73
entropy_beta	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:74
lr_a	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:75
lr_c	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:76
save_network_frequency	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:79
save_stat_frequency	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:80
save_image_frequency	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ') \/\/ 2$/;"	variable	line:81
moving_average_step	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:82
action_space	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:85
vision_range	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:86
keep_frame	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^keep_frame   = 2#config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:87
map_size	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:88
minibatch_size	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^minibatch_size = 256$/;"	variable	line:91
epoch	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^epoch = 2$/;"	variable	line:92
minimum_batch_size	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^minimum_batch_size = 4096$/;"	variable	line:93
nchannel	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^nchannel = 6 * keep_frame$/;"	variable	line:98
input_size	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:99
log_episodic_reward	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:102
log_length	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:103
log_winrate	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:104
log_redwinrate	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:105
log_looptime	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:106
log_explore	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^log_explore = MovingAverage(moving_average_step)$/;"	variable	line:107
log_traintime	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:108
map_list	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:111
use_fair_map	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^def use_fair_map():$/;"	function	line:112
make_env	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^def make_env(map_size):$/;"	function	line:116
envs_list	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^envs_list = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:122
envs	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^envs = SubprocVecEnv(envs_list, keep_frame)$/;"	variable	line:123
num_blue	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:124
num_red	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:125
gpu_options	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)$/;"	variable	line:128
config	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOG_DEVICE, allow_soft_placement=True)$/;"	variable	line:129
progbar	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name='SF Training')$/;"	variable	line:132
sess	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^sess = tf.Session(config=config)$/;"	variable	line:134
global_step	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:136
global_step_next	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:137
network	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    network = Network(input_shape=input_size, action_size=action_space, scope='ground', sess=sess)$/;"	variable	line:139
network_air	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    network_air = Network(input_shape=input_size, action_size=action_space, scope='uav', sess=sess)$/;"	variable	line:141
global_episodes	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^global_episodes = 0$/;"	variable	line:144
saver	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^saver = tf.train.Saver(max_to_keep=3, keep_checkpoint_every_n_hours=4)$/;"	variable	line:145
global_episodes	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:150
writer	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:152
train	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^def train(nn, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:157
get_action	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^def get_action(states, N=16):$/;"	function	line:199
num_batch	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^num_batch = 0$/;"	variable	line:241
log_on	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:243
log_image_on	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:244
save_on	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:245
play_save_on	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    play_save_on = interval_flag(global_episodes, 5000, 'replay_save')$/;"	variable	line:246
episode_rew	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:249
was_alive	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    was_alive = [True for agent in range(NENV*(num_blue*num_red))]$/;"	variable	line:250
was_done	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:251
is_air	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    is_air = np.array([agent.is_air for agent in envs.get_team_blue().flat]).reshape([NENV, num_blue])$/;"	variable	line:252
is_air_red	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    is_air_red = np.array([agent.is_air for agent in envs.get_team_red().flat]).reshape([NENV, num_red])$/;"	variable	line:253
is_air	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    is_air = np.concatenate([is_air, is_air_red], axis=1).reshape([-1])$/;"	variable	line:254
trajs	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    trajs = [Trajectory(depth=9) for _ in range((num_blue+num_red)*NENV)] # Trajectory per agent$/;"	variable	line:256
s1	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    s1 = envs.reset(config_path=env_setting_path) #, custom_board='fair_uav\/board_0001.txt')$/;"	variable	line:259
stime_roll	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    stime_roll = time.time()$/;"	variable	line:263
s0	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        s0 = s1$/;"	variable	line:265
psi0	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        psi0 = psi$/;"	variable	line:267
logits	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        logits = logits1$/;"	variable	line:268
reward_red	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        reward_red = np.array([i['red_reward'] for i in info])$/;"	variable	line:272
env_reward	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        env_reward = np.vstack((reward, reward_red)).T.reshape([-1])$/;"	variable	line:273
is_alive	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        is_alive = np.array([agent.isAlive for agent in envs.get_team_blue().flat]).reshape([NENV, num_blue])$/;"	variable	line:275
is_alive_red	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        is_alive_red = np.array([agent.isAlive for agent in envs.get_team_red().flat]).reshape([NENV, num_red])$/;"	variable	line:276
is_alive	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        is_alive = np.concatenate([is_alive, is_alive_red], axis=1).reshape([-1])$/;"	variable	line:277
was_done	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        was_done = np.array(was_done, dtype=bool)$/;"	variable	line:284
pre_air_reward	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        pre_air_reward = (np.isin(s1[::6,:,:,-12],-1).sum(axis=2).sum(axis=1) * (-0.1) + np.isin(s1[::6,:,:,-10], [1, -1]).astype(int).sum(axis=2).sum(axis=1)*10.0) * np.repeat(np.array(~was_done,dtype=int), 2)$/;"	variable	line:285
air_reward	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        air_reward = (np.isin(s1[::6,:,:,-6],-1).sum(axis=2).sum(axis=1) * (-0.1) + np.isin(s1[::6,:,:,-4], [1, -1]).astype(int).sum(axis=2).sum(axis=1)*10.0) * np.repeat(np.array(~was_done,dtype=int), 2)$/;"	variable	line:286
env_idx	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^            env_idx = idx \/\/ (num_blue+num_red)$/;"	variable	line:289
env_team_idx	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^            env_team_idx = idx \/\/ 6$/;"	variable	line:290
agent_reward	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^                    agent_reward = air_reward[env_team_idx] - pre_air_reward[env_team_idx]$/;"	variable	line:293
agent_reward	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^                    agent_reward = env_reward[env_team_idx]$/;"	variable	line:295
was_alive	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        was_alive = is_alive$/;"	variable	line:307
was_done	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        was_done = done$/;"	variable	line:308
explore_factor	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^            explore_factor = np.mean(air_reward)$/;"	variable	line:311
etime_roll	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    etime_roll = time.time()$/;"	variable	line:313
stime_train	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        stime_train = time.time()$/;"	variable	line:323
etime_train	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        etime_train = time.time()$/;"	variable	line:326
num_batch	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        num_batch = 0$/;"	variable	line:328
steps	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^    steps = []$/;"	variable	line:331
tag	/home/neale/ctf_RL/archive_code/sf_trainer.py	/^        tag = 'uav_training\/'$/;"	variable	line:348
LOGDEVICE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^LOGDEVICE = False$/;"	variable	line:37
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^PROGBAR = True$/;"	variable	line:38
TRAIN_SUBP	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^TRAIN_SUBP = True$/;"	variable	line:39
CONTINUE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^CONTINUE = False$/;"	variable	line:40
num_mode	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^num_mode = 3$/;"	variable	line:42
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^TRAIN_NAME = sys.argv[1]$/;"	variable	line:45
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:46
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:47
SAVE_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:48
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:49
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^GPU_CAPACITY = 0.90$/;"	variable	line:50
NENV	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^NENV = multiprocessing.cpu_count()\/\/2$/;"	variable	line:51
MODEL_LOAD_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^MODEL_LOAD_PATH = '.\/model\/fix_baseline2'$/;"	variable	line:53
ENV_SETTING_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^ENV_SETTING_PATH = 'setting_full.ini'$/;"	variable	line:54
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^config = configparser.ConfigParser()$/;"	variable	line:62
total_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:66
max_ep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:67
gamma	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:68
lambd	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:69
ppo_e	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:70
critic_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:71
entropy_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:72
lr_a	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:73
lr_c	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:74
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:77
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:78
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')*4$/;"	variable	line:79
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:80
action_space	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:83
vision_range	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:84
keep_frame	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:85
map_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:86
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^minibatch_size = 256$/;"	variable	line:89
epoch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^epoch = 2$/;"	variable	line:90
batch_memory_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^batch_memory_size = 4000$/;"	variable	line:91
nchannel	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^nchannel = 7 * keep_frame$/;"	variable	line:95
input_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:96
log_episodic_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:99
log_length	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:100
log_winrate	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:101
map_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:104
smoothstep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^def smoothstep(x, lowx=0.0, highx=1.0, lowy=0, highy=1):$/;"	function	line:106
use_this_map	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^def use_this_map(x, max_episode, max_prob):$/;"	function	line:115
heur_policy_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^heur_policy_list = [policy.Patrol, policy.Roomba, policy.Defense, policy.Random, policy.AStar]$/;"	variable	line:123
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^heur_weight = [1,1,1,1,1]$/;"	variable	line:124
heur_weight	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^heur_weight = np.array(heur_weight) \/ sum(heur_weight)$/;"	variable	line:125
use_this_policy	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^def use_this_policy():$/;"	function	line:126
make_env	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^def make_env(map_size):$/;"	function	line:130
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:133
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^envs = SubprocVecEnv(envs, keep_frame)$/;"	variable	line:134
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:135
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:136
gpu_options	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^gpu_options = tf.GPUOptions(allow_growth=True)$/;"	variable	line:139
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOGDEVICE)$/;"	variable	line:140
sess	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^sess = tf.Session(config=config)$/;"	variable	line:142
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^global_episodes = 0$/;"	variable	line:144
global_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:145
global_step_next	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:146
network	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^network = Network(in_size=input_size, action_size=action_space, sess=sess, num_mode=num_mode, scope='main')$/;"	variable	line:147
meta_network	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^meta_network = MetaNetwork(input_shape=input_size, action_size=num_mode, sess=sess, scope='meta')$/;"	variable	line:148
saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^saver = tf.train.Saver(max_to_keep=3, var_list=network.get_vars+meta_network.get_vars+[global_step])$/;"	variable	line:150
pretrained_vars	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    pretrained_vars = []$/;"	variable	line:154
pretrained_vars_name	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    pretrained_vars_name = []$/;"	variable	line:155
restoring_saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    restoring_saver = tf.train.Saver(max_to_keep=3, var_list=pretrained_vars)$/;"	variable	line:162
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:166
writer	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:167
meta_train	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^def meta_train(trajs, bootstrap=0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:171
reward_shape	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^def reward_shape(prev_red_alive, red_alive, done):$/;"	function	line:256
get_action	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^def get_action(states, initial=False):$/;"	function	line:284
batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^batch = []$/;"	variable	line:295
num_batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^num_batch = 0$/;"	variable	line:296
progbar	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:298
log_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:300
log_image_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:301
save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:302
reload_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    reload_on = False # interval_flag(global_episodes,selfplay_reload, 'reload')$/;"	variable	line:303
play_save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    play_save_on = False # interval_flag(global_episodes, 50000, 'replay_save')$/;"	variable	line:304
episode_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:307
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    prev_rew = np.zeros(NENV)$/;"	variable	line:308
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:309
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    was_alive_red = [True for agent in envs.get_team_red().flat]$/;"	variable	line:310
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:311
trajs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    trajs = [Trajectory(depth=11) for _ in range(num_blue*NENV)]$/;"	variable	line:313
s1	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    s1 = envs.reset($/;"	variable	line:316
custom_board	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^            custom_board=use_this_map(global_episodes, max_at, max_epsilon),$/;"	variable	line:317
policy_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^            policy_red=use_this_policy()$/;"	variable	line:318
cumul_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    cumul_reward = np.zeros(NENV)$/;"	variable	line:323
s0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        s0 = s1$/;"	variable	line:325
logits0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        logits0 = logits1$/;"	variable	line:327
sub_logits0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        sub_logits0 = sub_logits1$/;"	variable	line:329
is_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:332
is_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        is_alive_red = [agent.isAlive for agent in envs.get_team_red().flat]$/;"	variable	line:333
env_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        env_reward = (raw_reward - prev_rew - 0.01)\/100$/;"	variable	line:334
task_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        task_reward = reward_shape(was_alive_red, is_alive_red, done)$/;"	variable	line:341
env_idx	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:351
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        prev_rew = raw_reward$/;"	variable	line:367
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        was_alive = is_alive$/;"	variable	line:368
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        was_alive_red = is_alive_red$/;"	variable	line:369
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        was_done = done$/;"	variable	line:370
batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        batch = []$/;"	variable	line:385
num_batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        num_batch = 0$/;"	variable	line:386
steps	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^    steps = []$/;"	variable	line:388
tag	/home/neale/ctf_RL/archive_code/ppo_subpolicy_confid.py	/^        tag = 'adapt_train_log\/'$/;"	variable	line:396
target_setting_path	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^target_setting_path = sys.argv[1]$/;"	variable	line:38
device_t	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^device_t = sys.argv[3]$/;"	variable	line:39
OVERRIDE	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^OVERRIDE = bool(sys.argv[4])$/;"	variable	line:41
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^PROGBAR = False$/;"	variable	line:42
LOG_DEVICE	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^LOG_DEVICE = False$/;"	variable	line:43
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^TRAIN_NAME = sys.argv[2]$/;"	variable	line:46
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:47
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:48
SAVE_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:49
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:50
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:51
MODEL_LOAD_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^MODEL_LOAD_PATH = '.\/model\/ppo_baseline' # initialize values$/;"	variable	line:53
SWITCH_EP	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^SWITCH_EP = 0$/;"	variable	line:54
NENV	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^NENV = 8#multiprocessing.cpu_count() \/\/ 2$/;"	variable	line:56
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^env_setting_path = 'setting_full.ini'$/;"	variable	line:59
config	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^config = configparser.ConfigParser()$/;"	variable	line:67
total_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:71
max_ep	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:72
gamma	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:73
lambd	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:74
ppo_e	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:75
critic_beta	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:76
entropy_beta	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:77
lr_a	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:78
lr_c	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:79
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:82
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:83
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ') * 4$/;"	variable	line:84
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:85
action_space	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:88
vision_range	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:89
keep_frame	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:90
map_size	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^map_size     = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:91
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^minibatch_size = 256$/;"	variable	line:94
epoch	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^epoch = 2$/;"	variable	line:95
minimum_batch_size	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^minimum_batch_size = 5000$/;"	variable	line:96
nchannel	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^nchannel = 7 * keep_frame$/;"	variable	line:100
input_size	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:101
log_episodic_reward	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:104
log_length	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:105
log_winrate	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:106
log_redwinrate	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:107
log_looptime	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:108
log_traintime	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:109
log_attack_reward	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_attack_reward = MovingAverage(moving_average_step)$/;"	variable	line:111
log_scout_reward	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_scout_reward = MovingAverage(moving_average_step)$/;"	variable	line:112
log_defense_reward	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^log_defense_reward = MovingAverage(moving_average_step)$/;"	variable	line:113
map_list	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:116
smoothstep	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^def smoothstep(x, lowx=0.0, highx=1.0, lowy=0, highy=1):$/;"	function	line:118
use_this_map	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^def use_this_map(x, max_episode, max_prob):$/;"	function	line:127
heur_policy_list	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^heur_policy_list = [policy.Patrol, policy.Roomba, policy.Defense, policy.Random, policy.AStar]$/;"	variable	line:135
heur_weight	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^heur_weight = [1,1,1,1,1]$/;"	variable	line:136
heur_weight	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^heur_weight = np.array(heur_weight) \/ sum(heur_weight)$/;"	variable	line:137
use_this_policy	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^def use_this_policy():$/;"	function	line:138
make_env	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^def make_env(map_size):$/;"	function	line:142
envs	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:148
envs	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^envs = SubprocVecEnv(envs, keep_frame)$/;"	variable	line:149
num_blue	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:150
num_red	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:151
gpu_options	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)$/;"	variable	line:154
config	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOG_DEVICE, allow_soft_placement=True)$/;"	variable	line:155
progbar	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:158
sess	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^sess = tf.Session(config=config)$/;"	variable	line:160
global_step	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:163
global_step_next	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:164
network	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    network = Network(input_shape=input_size, action_size=action_space, scope='main', sess=sess)$/;"	variable	line:165
global_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^global_episodes = 0$/;"	variable	line:168
saver	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^saver = tf.train.Saver(max_to_keep=3)$/;"	variable	line:169
global_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:172
writer	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:177
train	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^def train(trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:182
get_action	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^def get_action(states):$/;"	function	line:214
reward_shape	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^def reward_shape(prev_red_alive, red_alive, done):$/;"	function	line:219
batch	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^batch = []$/;"	variable	line:248
num_batch	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^num_batch = 0$/;"	variable	line:249
log_on	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:251
log_image_on	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:252
save_on	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:253
play_save_on	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    play_save_on = interval_flag(global_episodes, 50000, 'replay_save')$/;"	variable	line:254
episode_rew	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:257
case_rew	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    case_rew = [np.zeros(NENV) for _ in range(3)]$/;"	variable	line:258
prev_rew	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    prev_rew = np.zeros(NENV)$/;"	variable	line:259
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        env_setting_path = target_setting_path$/;"	variable	line:263
s1	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    s1 = envs.reset($/;"	variable	line:264
config_path	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^            config_path=env_setting_path,$/;"	variable	line:265
custom_board	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^            custom_board=use_this_map(global_episodes, max_at, max_epsilon),$/;"	variable	line:266
policy_red	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^            policy_red=use_this_policy()$/;"	variable	line:267
num_blue	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:270
num_red	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    num_red = len(envs.get_team_red()[0])$/;"	variable	line:271
was_alive	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:272
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    was_alive_red = [True for agent in envs.get_team_red().flat]$/;"	variable	line:273
was_done	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:274
trajs	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    trajs = [Trajectory(depth=5) for _ in range(num_blue*NENV)]$/;"	variable	line:276
stime_roll	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    stime_roll = time.time()$/;"	variable	line:281
s0	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        s0 = s1$/;"	variable	line:283
logits	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        logits = logits1$/;"	variable	line:285
is_alive	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:288
is_alive_red	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        is_alive_red = [agent.isAlive for agent in envs.get_team_red().flat]$/;"	variable	line:289
reward	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        reward = (raw_reward - prev_rew - 0.01)\/100.0$/;"	variable	line:290
shaped_reward	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        shaped_reward = reward_shape(was_alive_red, is_alive_red, done)$/;"	variable	line:296
env_idx	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:306
prev_rew	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        prev_rew = raw_reward$/;"	variable	line:310
was_alive	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        was_alive = is_alive$/;"	variable	line:311
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        was_alive_red = is_alive_red$/;"	variable	line:312
was_done	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        was_done = done$/;"	variable	line:313
etime_roll	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    etime_roll = time.time()$/;"	variable	line:317
stime_train	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        stime_train = time.time()$/;"	variable	line:322
etime_train	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        etime_train = time.time()$/;"	variable	line:324
batch	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        batch = []$/;"	variable	line:325
num_batch	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        num_batch = 0$/;"	variable	line:326
steps	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^    steps = []$/;"	variable	line:329
tag	/home/neale/ctf_RL/archive_code/ppo_trainer_target.py	/^        tag = 'kerasTest\/'$/;"	variable	line:349
physical_devices	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:16
PROGBAR	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^PROGBAR = True$/;"	variable	line:39
LOG_DEVICE	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^LOG_DEVICE = False$/;"	variable	line:40
OVERRIDE	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^OVERRIDE = False$/;"	variable	line:41
TRAIN_NAME	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^TRAIN_NAME = 'DIST_KALMAN_PASV_00'$/;"	variable	line:44
TRAIN_TAG	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^TRAIN_TAG = 'Dist model w Kalman: '+TRAIN_NAME$/;"	variable	line:45
LOG_PATH	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:46
MODEL_PATH	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:47
MAP_PATH	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^MAP_PATH = '.\/fair_3g_40'$/;"	variable	line:48
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:49
NENV	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^NENV = multiprocessing.cpu_count()$/;"	variable	line:51
env_setting_path	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^env_setting_path = 'env_setting_3v3_3g_partial.ini'$/;"	variable	line:54
config_path	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^config_path = 'config.ini'$/;"	variable	line:61
config	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^config = configparser.ConfigParser()$/;"	variable	line:62
total_episodes	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:66
max_ep	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^max_ep         = 200#config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:67
gamma	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:68
lambd	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:69
ppo_e	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:70
critic_beta	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:71
entropy_beta	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:72
lr_a	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:73
lr_c	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:74
save_network_frequency	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:77
save_stat_frequency	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:78
save_image_frequency	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^save_image_frequency   = 4#config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:79
moving_average_step	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:80
action_space	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:83
vision_range	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^vision_range = 39#config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:84
keep_frame	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^keep_frame   = 1#config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:85
map_size	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^map_size     = 40#config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:86
minibatch_size	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^minibatch_size = 256$/;"	variable	line:89
epoch	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^epoch = 2$/;"	variable	line:90
minimum_batch_size	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^minimum_batch_size = 4096$/;"	variable	line:91
nchannel	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^nchannel = 6 * keep_frame$/;"	variable	line:96
input_size	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:97
cent_input_size	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:98
log_episodic_reward	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:101
log_winrate	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:102
log_redwinrate	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:103
log_looptime	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:104
log_traintime	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:105
map_list	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH) if path[:5]=='board']$/;"	variable	line:108
make_env	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^def make_env(map_size):$/;"	function	line:109
envs	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:112
envs	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:113
num_blue	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:114
num_red	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:115
num_agent	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^num_agent = num_blue#+num_red$/;"	variable	line:116
progbar	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name=TRAIN_TAG)$/;"	variable	line:119
atoms	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^atoms = 8$/;"	variable	line:121
cent_network	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^cent_network = Network(input_shape=cent_input_size, action_size=action_space, atoms=atoms, scope='main', save_path=MODEL_PATH)$/;"	variable	line:122
global_episodes	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^global_episodes = 0$/;"	variable	line:125
writer	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:128
train	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^def train(network, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, step=None):$/;"	function	line:133
train_reward_prediction	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^def train_reward_prediction(network, traj, epoch, batch_size, writer=None, log=False, step=None):$/;"	function	line:197
reward_training_buffer	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^reward_training_buffer = Trajectory(depth=4) # Centralized$/;"	variable	line:218
batch	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^batch = []$/;"	variable	line:219
num_batch	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^num_batch = 0$/;"	variable	line:220
episode_rew	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:224
was_done	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    was_done = [False for env in range(NENV*num_agent)]$/;"	variable	line:225
cent_trajs	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    cent_trajs = [Trajectory(depth=6) for _ in range(NENV)]$/;"	variable	line:228
bmean1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    bmean1 = np.zeros([NENV*num_agent, atoms], dtype=np.float32)$/;"	variable	line:229
blogvar1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    blogvar1 = np.zeros([NENV*num_agent, atoms], dtype=np.float32)$/;"	variable	line:230
cent_bmean1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    cent_bmean1 = np.zeros([NENV, atoms], dtype=np.float32)$/;"	variable	line:231
cent_blogvar1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    cent_blogvar1 = np.zeros([NENV, atoms], dtype=np.float32)$/;"	variable	line:232
s1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    s1 = envs.reset($/;"	variable	line:235
map_pool	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^            map_pool=map_list,$/;"	variable	line:236
config_path	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^            config_path=env_setting_path,$/;"	variable	line:237
policy_red	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^            policy_red=policy.Roomba,$/;"	variable	line:238
policy_blue	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^            policy_blue=policy.Roomba,$/;"	variable	line:239
mode	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^            mode='continue')$/;"	variable	line:240
is_air	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    is_air = np.array([agent.is_air for agent in envs.get_team_blue().flat])#.reshape([NENV, num_blue])$/;"	variable	line:242
cent_s1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    cent_s1 = envs.get_obs_blue() # Centralized$/;"	variable	line:243
stime_roll	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    stime_roll = time.time()$/;"	variable	line:246
s0	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        s0 = s1$/;"	variable	line:248
cent_s0	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        cent_s0 = cent_s1$/;"	variable	line:250
cent_s1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        cent_s1 = envs.get_obs_blue() # Centralized$/;"	variable	line:255
cent_bmean1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        cent_bmean1 = cent_bmean1[:].numpy()$/;"	variable	line:261
cent_blogvar1	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        cent_blogvar1 = cent_blogvar1[:].numpy()$/;"	variable	line:262
was_done	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        was_done = done$/;"	variable	line:292
etime_roll	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    etime_roll = time.time()$/;"	variable	line:296
stime_train	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        stime_train = time.time()$/;"	variable	line:309
log_image_on	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:310
etime_train	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        etime_train = time.time()$/;"	variable	line:312
batch	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        batch = []$/;"	variable	line:313
num_batch	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        num_batch = 0$/;"	variable	line:314
log_rt_on	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        log_rt_on = interval_flag(global_episodes, save_image_frequency, 'rt_log')$/;"	variable	line:318
temp_buffer	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        temp_buffer = Trajectory(depth=4)$/;"	variable	line:321
r	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^            r = [col[i] for col in reward_training_buffer.buffer]$/;"	variable	line:323
reward_training_buffer	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^        reward_training_buffer = temp_buffer$/;"	variable	line:327
log_on	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:340
tag	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^            tag = 'baseline_training\/'$/;"	variable	line:343
save_on	/home/neale/ctf_RL/archive_code/run_dist_kalman.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:351
device_ground	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^device_ground = '\/gpu:1'$/;"	variable	line:37
PROGBAR	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^PROGBAR = True$/;"	variable	line:39
LOG_DEVICE	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^LOG_DEVICE = False$/;"	variable	line:40
OVERRIDE	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^OVERRIDE = False$/;"	variable	line:41
TRAIN_NAME	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^TRAIN_NAME = 'LSTM_TEST_03'$/;"	variable	line:44
LOG_PATH	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:45
MODEL_PATH	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:46
SAVE_PATH	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:47
MAP_PATH	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:48
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:49
NENV	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^NENV = multiprocessing.cpu_count() $/;"	variable	line:51
env_setting_path	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^env_setting_path = 'setting_full.ini'$/;"	variable	line:55
config_path	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^config_path = 'config.ini'$/;"	variable	line:63
config	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^config = configparser.ConfigParser()$/;"	variable	line:64
total_episodes	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^total_episodes = 1000000#config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:68
max_ep	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:69
gamma	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:70
lambd	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:71
ppo_e	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:72
critic_beta	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:73
entropy_beta	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:74
lr_a	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:75
lr_c	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:76
save_network_frequency	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:79
save_stat_frequency	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:80
save_image_frequency	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ') \/\/ 2$/;"	variable	line:81
moving_average_step	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:82
action_space	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:85
map_size	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^map_size     = 40 # config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:86
vision_range	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^vision_range = 39# config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:87
keep_frame	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^keep_frame   = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:88
minibatch_size	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^minibatch_size = 256$/;"	variable	line:91
epoch	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^epoch = 2$/;"	variable	line:92
minimum_batch_size	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^minimum_batch_size = 2048$/;"	variable	line:93
nchannel	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^nchannel = 6$/;"	variable	line:98
input_size	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^input_size = [None, keep_frame, vision_dx, vision_dy, nchannel]$/;"	variable	line:99
log_episodic_reward	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:102
log_length	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:103
log_winrate	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:104
log_redwinrate	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:105
log_looptime	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:106
log_traintime	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:107
map_list	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:110
use_fair_map	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^def use_fair_map():$/;"	function	line:111
make_env	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^def make_env(map_size):$/;"	function	line:115
envs_list	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^envs_list = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:121
envs	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^envs = SubprocVecEnv(envs_list, keep_frame, map_size*2-1)$/;"	variable	line:122
num_blue	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:123
num_red	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:124
gpu_options	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)$/;"	variable	line:127
config	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOG_DEVICE, allow_soft_placement=True)$/;"	variable	line:128
progbar	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name='episode : '+TRAIN_NAME)$/;"	variable	line:131
sess	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^sess = tf.Session(config=config)$/;"	variable	line:133
global_step	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:135
global_step_next	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:136
network	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    network = Network(input_shape=input_size, action_size=action_space, scope='ground', sess=sess)$/;"	variable	line:138
global_episodes	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^global_episodes = 0$/;"	variable	line:141
saver	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^saver = tf.train.Saver(max_to_keep=3, keep_checkpoint_every_n_hours=4)$/;"	variable	line:142
global_episodes	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:147
writer	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:149
train	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^def train(nn, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:154
get_action	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^def get_action(states, prev_action, prev_reward, prev_hidden):$/;"	function	line:201
batch_ground	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^batch_ground = []$/;"	variable	line:208
num_batch	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^num_batch = 0$/;"	variable	line:209
log_on	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:211
log_image_on	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:212
save_on	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:213
play_save_on	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    play_save_on = interval_flag(global_episodes, 5000, 'replay_save')$/;"	variable	line:214
episode_rew	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:217
was_alive	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    was_alive = [True for _ in range(NENV*num_blue)]$/;"	variable	line:218
was_done	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:219
prev_reward	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    prev_reward = np.zeros((NENV*num_blue, keep_frame, 1))$/;"	variable	line:220
prev_action	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    prev_action = np.zeros((NENV*num_blue, keep_frame, 1))$/;"	variable	line:221
prev_hidden	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    prev_hidden = network.hidden_init(NENV*num_blue)$/;"	variable	line:222
trajs	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    trajs = [Trajectory(depth=7) for _ in range(num_blue*NENV)] # Trajectory per agent$/;"	variable	line:224
s1	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    s1 = envs.reset(config_path=env_setting_path, policy_red=policy.Roomba, custom_board='fair_map_40\/board_0001.txt')$/;"	variable	line:227
stime_roll	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    stime_roll = time.time()$/;"	variable	line:231
s0	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        s0 = s1$/;"	variable	line:233
logits	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        logits = logits1$/;"	variable	line:235
prev_hidden	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        prev_hidden = [hh0, hc0]$/;"	variable	line:237
r0	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        r0 = np.repeat(reward, num_blue)$/;"	variable	line:241
is_alive	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        is_alive = np.array([agent.isAlive for agent in envs.get_team_blue().flat])$/;"	variable	line:244
env_idx	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^            env_idx = idx \/\/ (num_blue)$/;"	variable	line:252
was_alive	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        was_alive = is_alive$/;"	variable	line:256
was_done	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        was_done = done$/;"	variable	line:257
etime_roll	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    etime_roll = time.time()$/;"	variable	line:261
stime_train	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        stime_train = time.time()$/;"	variable	line:268
etime_train	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        etime_train = time.time()$/;"	variable	line:270
batch_ground	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        batch_ground = []$/;"	variable	line:271
num_batch	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        num_batch = 0$/;"	variable	line:272
steps	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^    steps = []$/;"	variable	line:275
tag	/home/neale/ctf_RL/archive_code/partial_trainer.py	/^        tag = 'lstm_training\/'$/;"	variable	line:291
device_t	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^device_t = sys.argv[3]$/;"	variable	line:42
num_mode	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^num_mode = 3$/;"	variable	line:44
MODE_NAME	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^MODE_NAME = lambda mode: ['_attack', '_scout', '_defense', ''][mode]$/;"	variable	line:45
setting_paths	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^setting_paths = ['setting_ppo_attacker.ini', 'setting_ppo_scout.ini', 'setting_ppo_defense.ini']$/;"	variable	line:47
red_policies	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^red_policies = [policy.Roomba, policy.Roomba, policy.AStar]$/;"	variable	line:48
OVERRIDE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^OVERRIDE = False$/;"	variable	line:50
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^PROGBAR = False$/;"	variable	line:51
LOG_DEVICE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^LOG_DEVICE = False$/;"	variable	line:52
RBETA	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^RBETA = 0.8$/;"	variable	line:53
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^TRAIN_NAME = sys.argv[2]+str(time.time()) #'fix_baseline'$/;"	variable	line:56
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:57
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:58
SAVE_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:59
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:60
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:61
NENV	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^NENV = 8 #multiprocessing.cpu_count() \/\/ 2$/;"	variable	line:62
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^config = configparser.ConfigParser()$/;"	variable	line:70
total_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:74
max_ep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^max_ep = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:75
gamma	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^gamma = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:77
lambd	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^lambd = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:78
ppo_e	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^ppo_e = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:79
critic_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^critic_beta = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:80
entropy_beta	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^entropy_beta = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:81
lr_a	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^lr_a = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:83
lr_c	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^lr_c = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:84
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:87
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^save_stat_frequency = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:88
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^save_image_frequency = config.getint('LOG', 'SAVE_STATISTICS_FREQ')*4$/;"	variable	line:89
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^moving_average_step = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:90
action_space	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:93
vision_range	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^vision_range = config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:94
keep_frame	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^keep_frame = config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:95
map_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^map_size = config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:96
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^minibatch_size = 128$/;"	variable	line:99
epoch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^epoch = 2$/;"	variable	line:100
minbatch_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^minbatch_size = 2000$/;"	variable	line:101
nchannel	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^nchannel = 7 * keep_frame$/;"	variable	line:105
input_size	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:106
global_episode_rewards	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^global_episode_rewards = [MA(moving_average_step) for _ in range(num_mode)]$/;"	variable	line:109
global_environment_rewards	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^global_environment_rewards = [MA(moving_average_step) for _ in range(num_mode)]$/;"	variable	line:110
global_length	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^global_length = [MA(moving_average_step) for _ in range(num_mode)]$/;"	variable	line:111
global_succeed	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^global_succeed = [MA(moving_average_step) for _ in range(num_mode)]$/;"	variable	line:112
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^global_episodes = 0$/;"	variable	line:113
map_list	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH)]$/;"	variable	line:116
smoothstep	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^def smoothstep(x, lowx=0.0, highx=1.0, lowy=0, highy=1):$/;"	function	line:118
use_this_map	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^def use_this_map(x, max_episode, max_prob):$/;"	function	line:127
make_env	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^def make_env(map_size):$/;"	function	line:135
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:138
envs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^envs = SubprocVecEnv(envs, keep_frame)$/;"	variable	line:139
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:140
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:141
gpu_options	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)$/;"	variable	line:144
config	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^config = tf.ConfigProto(gpu_options=gpu_options, allow_soft_placement=True)$/;"	variable	line:145
sess	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^sess = tf.Session(config=config)$/;"	variable	line:147
train	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^def train(trajs, updater, bootstrap=0, epoch=epoch, batch_size=minibatch_size, **kwargv):$/;"	function	line:156
saver	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^saver = tf.train.Saver(max_to_keep=3, keep_checkpoint_every_n_hours=3)$/;"	variable	line:189
writer	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:192
global_episodes	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^global_episodes = sess.run(global_step) # Reset the counter$/;"	variable	line:193
reward_shape	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^def reward_shape(prev_red_alive, red_alive, done, idx=None, additional_reward=None):$/;"	function	line:196
get_action	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^def get_action(states):$/;"	function	line:226
batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^batch = []$/;"	variable	line:231
num_batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^num_batch = 0$/;"	variable	line:232
mode_changed	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^mode_changed = False$/;"	variable	line:233
MODE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^MODE = 0$/;"	variable	line:234
progbar	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    progbar = tf.keras.utils.Progbar(None)$/;"	variable	line:237
mode_changed	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        mode_changed = False$/;"	variable	line:240
MODE	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        MODE = (MODE + 1) % num_mode$/;"	variable	line:241
mode_episode	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    mode_episode = sess.run(subtrain_step[MODE])$/;"	variable	line:243
log_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    log_on = interval_flag(mode_episode, save_stat_frequency, 'log{}'.format(MODE))$/;"	variable	line:245
log_image_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:246
save_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:247
reload_on	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    reload_on = False # interval_flag(global_episodes,selfplay_reload, 'reload')$/;"	variable	line:248
s1	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    s1 = envs.reset($/;"	variable	line:251
config_path	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^            config_path=setting_paths[MODE],$/;"	variable	line:252
custom_board	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^            custom_board=use_this_map(global_episodes, max_at, max_epsilon),$/;"	variable	line:253
policy_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^            policy_red=red_policies[MODE]$/;"	variable	line:254
num_blue	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:256
num_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    num_red = len(envs.get_team_red()[0])$/;"	variable	line:257
episode_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:260
episode_env_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    episode_env_rew = np.zeros(NENV)$/;"	variable	line:261
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    prev_rew = np.zeros(NENV)$/;"	variable	line:262
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:263
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    was_alive_red = [True for agent in envs.get_team_red().flat]$/;"	variable	line:264
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:265
trajs	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    trajs = [Trajectory(depth=5) for _ in range(num_blue*NENV)]$/;"	variable	line:267
stime	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    stime = time.time()$/;"	variable	line:272
s0	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        s0 = s1$/;"	variable	line:274
logits	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        logits = logits1$/;"	variable	line:276
is_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:279
is_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        is_alive_red = [agent.isAlive for agent in envs.get_team_red().flat]$/;"	variable	line:280
reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        reward = reward_shape(was_alive_red, is_alive_red, done, MODE)$/;"	variable	line:284
env_reward	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^            env_reward = (raw_reward - prev_rew - 0.01)\/100$/;"	variable	line:289
env_idx	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:304
reward_function	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^                reward_function = (RBETA) * reward[env_idx] + (1-RBETA) * env_reward[env_idx] $/;"	variable	line:306
prev_rew	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        prev_rew = raw_reward$/;"	variable	line:309
was_alive	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        was_alive = is_alive$/;"	variable	line:310
was_alive_red	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        was_alive_red = is_alive_red$/;"	variable	line:311
was_done	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        was_done = done$/;"	variable	line:312
batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        batch = []$/;"	variable	line:328
num_batch	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        num_batch = 0$/;"	variable	line:329
mode_changed	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        mode_changed = True$/;"	variable	line:330
steps	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^    steps = []$/;"	variable	line:332
tag	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        tag = 'baseline_training\/'$/;"	variable	line:342
step	/home/neale/ctf_RL/archive_code/ppo_subpolicy_trainer.py	/^        step = sess.run(subtrain_step[MODE])$/;"	variable	line:343
physical_devices	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:16
PROGBAR	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^PROGBAR = True$/;"	variable	line:43
LOG_DEVICE	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^LOG_DEVICE = False$/;"	variable	line:44
OVERRIDE	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^OVERRIDE = False$/;"	variable	line:45
TRAIN_NAME	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^TRAIN_NAME = 'DIST_SF_CVDC_Test'$/;"	variable	line:49
TRAIN_TAG	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^TRAIN_TAG = 'Central value decentralized control, '+TRAIN_NAME$/;"	variable	line:50
LOG_PATH	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:51
MODEL_PATH	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:52
MAP_PATH	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^MAP_PATH = '.\/fair_3g_40'$/;"	variable	line:53
PRESAVED_PATH	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^PRESAVED_PATH = '.\/prerun_saves\/'$/;"	variable	line:54
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:55
slack_assist	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^slack_assist = SlackAssist(training_name=TRAIN_NAME, channel_name="#nodes")$/;"	variable	line:57
NENV	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^NENV = multiprocessing.cpu_count()$/;"	variable	line:59
env_setting_path	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^env_setting_path = 'env_setting_3v3_3g_full.ini'$/;"	variable	line:62
config_path	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^config_path = 'config.ini'$/;"	variable	line:69
config	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^config = configparser.ConfigParser()$/;"	variable	line:70
total_episodes	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^total_episodes = 30000# config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:74
max_ep	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^max_ep         = 200#config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:75
gamma	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:76
lambd	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:77
ppo_e	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:78
critic_beta	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:79
entropy_beta	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:80
lr_a	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:81
lr_c	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:82
save_network_frequency	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:85
save_stat_frequency	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^save_stat_frequency    = 128#config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:86
save_image_frequency	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^save_image_frequency   = 128#config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:87
moving_average_step	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:88
action_space	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:91
vision_range	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^vision_range = 39#config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:92
keep_frame	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^keep_frame   = 1#config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:93
map_size	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^map_size     = 40#config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:94
minibatch_size	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^minibatch_size = 512$/;"	variable	line:97
epoch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^epoch = 4$/;"	variable	line:98
minimum_batch_size	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^minimum_batch_size = 1024 * 4$/;"	variable	line:99
nchannel	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^nchannel = 6 * keep_frame$/;"	variable	line:104
input_size	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:105
cent_input_size	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:106
atoms	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^atoms = 64$/;"	variable	line:109
network	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^network = Network($/;"	variable	line:110
central_obs_shape	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        central_obs_shape=cent_input_size,$/;"	variable	line:111
decentral_obs_shape	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        decentral_obs_shape=input_size,$/;"	variable	line:112
action_size	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        action_size=action_space, $/;"	variable	line:113
atoms	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        atoms=atoms,$/;"	variable	line:114
save_path	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        save_path=MODEL_PATH)$/;"	variable	line:115
global_episodes	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^global_episodes = network.initiate()$/;"	variable	line:118
writer	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:119
progbar	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name=TRAIN_TAG)$/;"	variable	line:121
make_ds	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^def make_ds(features, labels, buffer_size=64):$/;"	function	line:123
save_list	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^save_list = [os.path.join(PRESAVED_PATH, path) for path in os.listdir(PRESAVED_PATH) if path[:5]=='batch']*10$/;"	variable	line:129
train_number	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^train_number = 0$/;"	variable	line:130
log_interval	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^log_interval = 100$/;"	variable	line:131
rmse_loss	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^rmse_loss = []$/;"	variable	line:132
rp_loss	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^rp_loss = []$/;"	variable	line:133
states	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            states = data['env_states']$/;"	variable	line:137
rewards	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            rewards = data['env_rewards']$/;"	variable	line:138
bool_pos_reward	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        bool_pos_reward = rewards > 0$/;"	variable	line:141
bool_neg_reward	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        bool_neg_reward = rewards < 0$/;"	variable	line:142
pos_ds	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        pos_ds = make_ds(states[bool_pos_reward], rewards[bool_pos_reward])$/;"	variable	line:146
neg_ds	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        neg_ds = make_ds(states[bool_neg_reward], rewards[bool_neg_reward])$/;"	variable	line:147
zero_ds	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        zero_ds = make_ds(states[bool_zero_reward], rewards[bool_zero_reward])$/;"	variable	line:148
states	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        states = []$/;"	variable	line:149
rewards	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        rewards = []$/;"	variable	line:150
train_dataset	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        train_dataset = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds, zero_ds], seed=0, weights=[0.1, 0.1, 0.8]).batch(128)$/;"	variable	line:154
_rmse_loss	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        _rmse_loss = []$/;"	variable	line:155
_rp_loss	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        _rp_loss = []$/;"	variable	line:156
count	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        count = 0$/;"	variable	line:157
info	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            info = network.update_reward_prediction(state, reward)$/;"	variable	line:159
_rmse_loss	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^                _rmse_loss = []$/;"	variable	line:165
_rp_loss	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^                _rp_loss = []$/;"	variable	line:167
t	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^                t = np.arange(len(rmse_loss))$/;"	variable	line:171
ax2	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^                ax2 = ax1.twinx()$/;"	variable	line:173
train_dataset	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        train_dataset = None$/;"	variable	line:182
log_episodic_reward	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:198
log_winrate	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:199
log_redwinrate	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:200
log_looptime	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:201
log_traintime	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:202
map_list	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH) if path[:5]=='board']$/;"	variable	line:205
make_env	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^def make_env(map_size):$/;"	function	line:206
envs	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:209
envs	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:210
num_blue	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:211
num_red	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:212
num_agent	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^num_agent = num_blue#+num_red$/;"	variable	line:213
train_central	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^def train_central(network, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, step=None):$/;"	function	line:217
train_decentral	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^def train_decentral(network, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, step=None):$/;"	function	line:266
train_reward_prediction	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^def train_reward_prediction(network, traj, epoch, batch_size, writer=None, log=False, step=None):$/;"	function	line:321
train_ma_value	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^def train_ma_value(network, trajs, bootstrap=0.0, writer=None, log=False, step=None):$/;"	function	line:340
get_action	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^def get_action(log_logits):$/;"	function	line:372
reward_training_buffer	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^reward_training_buffer = Trajectory(depth=4) # Centralized$/;"	variable	line:379
batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^batch = []$/;"	variable	line:380
dec_batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^dec_batch = []$/;"	variable	line:381
ma_batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^ma_batch = []$/;"	variable	line:382
num_batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^num_batch = 0$/;"	variable	line:383
episode_rew	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:387
is_alive	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:388
is_done	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    is_done = [False for env in range(NENV*num_agent)]$/;"	variable	line:389
trajs	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    trajs = [Trajectory(depth=10) for _ in range(NENV*num_agent)]$/;"	variable	line:391
ma_trajs	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    ma_trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:392
cent_trajs	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    cent_trajs = [Trajectory(depth=9) for _ in range(NENV)]$/;"	variable	line:393
s1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    s1 = envs.reset($/;"	variable	line:396
map_pool	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            map_pool=map_list,$/;"	variable	line:397
config_path	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            config_path=env_setting_path,$/;"	variable	line:398
policy_red	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            policy_red=policy.Roomba,$/;"	variable	line:399
policy_blue	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            policy_blue=policy.Roomba,$/;"	variable	line:400
mode	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            mode='continue')$/;"	variable	line:401
s1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:402
cent_s1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    cent_s1 = envs.get_obs_blue().astype(np.float32) # Centralized$/;"	variable	line:403
belief	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    belief = env_feature['latent'].numpy()$/;"	variable	line:406
belief	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    belief = np.repeat(belief, num_agent, axis=0)$/;"	variable	line:407
stime_roll	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    stime_roll = time.time()$/;"	variable	line:412
s0	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        s0 = s1$/;"	variable	line:414
a0	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        a0 = a1$/;"	variable	line:415
v0	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        v0 = critic['critic'].numpy()[:,0]$/;"	variable	line:416
psi0	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        psi0 = critic['psi'].numpy()$/;"	variable	line:417
log_logits0	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        log_logits0 = actor['log_softmax'].numpy()$/;"	variable	line:418
cent_s0	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        cent_s0 = cent_s1$/;"	variable	line:419
cent_v0	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        cent_v0 = env_critic['critic'].numpy()[:,0]$/;"	variable	line:420
cent_psi	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        cent_psi = env_critic['psi'].numpy()$/;"	variable	line:421
was_alive	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        was_alive = is_alive$/;"	variable	line:422
was_done	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        was_done = is_done$/;"	variable	line:423
is_alive	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:427
s1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        s1 = s1.astype(np.float32) # Decentralize observation$/;"	variable	line:428
cent_s1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        cent_s1 = envs.get_obs_blue().astype(np.float32) # Centralized$/;"	variable	line:429
cent_phi1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        cent_phi1 = env_critic['phi'].numpy()$/;"	variable	line:434
belief	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        belief = env_feature['latent'].numpy()$/;"	variable	line:435
belief	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        belief = np.repeat(belief, num_agent, axis=0)$/;"	variable	line:436
phi1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        phi1 = critic['phi'].numpy()$/;"	variable	line:440
reward_pred1	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        reward_pred1 = critic['reward_predict'].numpy()[:,0]$/;"	variable	line:441
env_idx	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            env_idx = idx \/\/ num_agent$/;"	variable	line:445
etime_roll	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    etime_roll = time.time()$/;"	variable	line:488
stime_train	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        stime_train = time.time()$/;"	variable	line:503
log_image_on	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:504
etime_train	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        etime_train = time.time()$/;"	variable	line:508
batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        batch = []$/;"	variable	line:509
dec_batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        dec_batch = []$/;"	variable	line:510
ma_batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        ma_batch = []$/;"	variable	line:511
num_batch	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        num_batch = 0$/;"	variable	line:512
log_rt_on	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^        log_rt_on = interval_flag(global_episodes, save_image_frequency, 'rt_log')$/;"	variable	line:516
log_on	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:540
tag	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^            tag = 'baseline_training\/'$/;"	variable	line:543
save_on	/home/neale/ctf_RL/archive_code/run_sfk_full_pretrain.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:551
physical_devices	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:15
PROGBAR	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^PROGBAR = True$/;"	variable	line:38
LOG_DEVICE	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^LOG_DEVICE = False$/;"	variable	line:39
OVERRIDE	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^OVERRIDE = False$/;"	variable	line:40
TRAIN_NAME	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^TRAIN_NAME = 'PPO_STACK_00' $/;"	variable	line:43
TRAIN_TAG	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^TRAIN_TAG = 'PPO model w Stacked Frames: '+TRAIN_NAME$/;"	variable	line:44
LOG_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:45
MODEL_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:46
MAP_PATH	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^MAP_PATH = '.\/fair_3g_40'$/;"	variable	line:47
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:48
NENV	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^NENV = 8 # multiprocessing.cpu_count() \/\/ 2$/;"	variable	line:50
env_setting_path	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^env_setting_path = 'env_setting_3v3_3g_partial.ini'$/;"	variable	line:53
config_path	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^config_path = 'config.ini'$/;"	variable	line:60
config	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^config = configparser.ConfigParser()$/;"	variable	line:61
total_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:65
max_ep	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^max_ep         = 200#config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:66
gamma	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:67
lambd	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:68
ppo_e	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:69
critic_beta	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:70
entropy_beta	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:71
lr_a	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:72
lr_c	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:73
save_network_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:76
save_stat_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:77
save_image_frequency	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:78
moving_average_step	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:79
action_space	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:82
vision_range	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^vision_range = 39#config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:83
keep_frame	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^keep_frame   = 4#config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:84
map_size	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^map_size     = 40#config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:85
minibatch_size	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^minibatch_size = 256$/;"	variable	line:88
epoch	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^epoch = 2$/;"	variable	line:89
minimum_batch_size	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^minimum_batch_size = 4096$/;"	variable	line:90
nchannel	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^nchannel = 6 * keep_frame$/;"	variable	line:95
input_size	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:96
log_episodic_reward	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:99
log_winrate	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:100
log_redwinrate	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:101
log_looptime	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:102
log_traintime	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:103
map_list	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH) if path[:5]=='board']$/;"	variable	line:106
make_env	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^def make_env(map_size):$/;"	function	line:107
envs	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:110
envs	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:111
num_blue	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:112
num_red	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:113
num_agent	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^num_agent = num_blue# + num_red$/;"	variable	line:114
progbar	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name=TRAIN_TAG)$/;"	variable	line:117
network	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^network = Network(input_shape=input_size, action_size=action_space, scope='main', save_path=MODEL_PATH)$/;"	variable	line:119
global_episodes	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^global_episodes = 0$/;"	variable	line:122
writer	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:125
train	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^def train(network, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, step=None):$/;"	function	line:130
get_action	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^def get_action(states):$/;"	function	line:175
batch	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^batch = []$/;"	variable	line:180
num_batch	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^num_batch = 0$/;"	variable	line:181
episode_rew	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:186
was_alive	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    was_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:187
was_done	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:188
trajs	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    trajs = [Trajectory(depth=5) for _ in range(num_blue*NENV)]$/;"	variable	line:190
s1	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    s1 = envs.reset($/;"	variable	line:193
map_pool	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^            map_pool=map_list,$/;"	variable	line:194
config_path	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^            config_path=env_setting_path,$/;"	variable	line:195
policy_red	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^            policy_red=policy.Roomba)$/;"	variable	line:196
s1	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:197
stime_roll	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    stime_roll = time.time()$/;"	variable	line:201
s0	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        s0 = s1$/;"	variable	line:203
p0	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        p0 = p1 $/;"	variable	line:205
s1	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        s1 = s1.astype(np.float32)$/;"	variable	line:208
is_alive	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:209
env_idx	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^            env_idx = idx \/\/ num_blue$/;"	variable	line:216
was_alive	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        was_alive = is_alive$/;"	variable	line:220
was_done	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        was_done = done$/;"	variable	line:221
etime_roll	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    etime_roll = time.time()$/;"	variable	line:225
stime_train	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        stime_train = time.time()$/;"	variable	line:230
log_image_on	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:231
etime_train	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        etime_train = time.time()$/;"	variable	line:233
batch	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        batch = []$/;"	variable	line:234
num_batch	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^        num_batch = 0$/;"	variable	line:235
log_on	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:247
tag	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^            tag = 'baseline_training\/'$/;"	variable	line:250
save_on	/home/neale/ctf_RL/archive_code/ppo_trainer.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:258
physical_devices	/home/neale/ctf_RL/archive_code/run_td_central.py	/^physical_devices = tf.config.experimental.list_physical_devices('GPU')$/;"	variable	line:16
PROGBAR	/home/neale/ctf_RL/archive_code/run_td_central.py	/^PROGBAR = True$/;"	variable	line:40
LOG_DEVICE	/home/neale/ctf_RL/archive_code/run_td_central.py	/^LOG_DEVICE = False$/;"	variable	line:41
OVERRIDE	/home/neale/ctf_RL/archive_code/run_td_central.py	/^OVERRIDE = False$/;"	variable	line:42
TRAIN_NAME	/home/neale/ctf_RL/archive_code/run_td_central.py	/^TRAIN_NAME = 'TD_CENTRAL_PASV_01'$/;"	variable	line:45
TRAIN_TAG	/home/neale/ctf_RL/archive_code/run_td_central.py	/^TRAIN_TAG = 'TD model (Central) w\/out Kalman: '+TRAIN_NAME$/;"	variable	line:46
LOG_PATH	/home/neale/ctf_RL/archive_code/run_td_central.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:47
MODEL_PATH	/home/neale/ctf_RL/archive_code/run_td_central.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:48
MAP_PATH	/home/neale/ctf_RL/archive_code/run_td_central.py	/^MAP_PATH = '.\/fair_3g_40'$/;"	variable	line:49
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/run_td_central.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:50
NENV	/home/neale/ctf_RL/archive_code/run_td_central.py	/^NENV = multiprocessing.cpu_count()$/;"	variable	line:52
env_setting_path	/home/neale/ctf_RL/archive_code/run_td_central.py	/^env_setting_path = 'env_setting_3v3_3g_partial.ini'$/;"	variable	line:55
config_path	/home/neale/ctf_RL/archive_code/run_td_central.py	/^config_path = 'config.ini'$/;"	variable	line:62
config	/home/neale/ctf_RL/archive_code/run_td_central.py	/^config = configparser.ConfigParser()$/;"	variable	line:63
total_episodes	/home/neale/ctf_RL/archive_code/run_td_central.py	/^total_episodes = config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:67
max_ep	/home/neale/ctf_RL/archive_code/run_td_central.py	/^max_ep         = 200#config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:68
gamma	/home/neale/ctf_RL/archive_code/run_td_central.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:69
lambd	/home/neale/ctf_RL/archive_code/run_td_central.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:70
ppo_e	/home/neale/ctf_RL/archive_code/run_td_central.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:71
critic_beta	/home/neale/ctf_RL/archive_code/run_td_central.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:72
entropy_beta	/home/neale/ctf_RL/archive_code/run_td_central.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:73
lr_a	/home/neale/ctf_RL/archive_code/run_td_central.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:74
lr_c	/home/neale/ctf_RL/archive_code/run_td_central.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:75
save_network_frequency	/home/neale/ctf_RL/archive_code/run_td_central.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:78
save_stat_frequency	/home/neale/ctf_RL/archive_code/run_td_central.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:79
save_image_frequency	/home/neale/ctf_RL/archive_code/run_td_central.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:80
moving_average_step	/home/neale/ctf_RL/archive_code/run_td_central.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:81
action_space	/home/neale/ctf_RL/archive_code/run_td_central.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:84
vision_range	/home/neale/ctf_RL/archive_code/run_td_central.py	/^vision_range = 39#config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:85
keep_frame	/home/neale/ctf_RL/archive_code/run_td_central.py	/^keep_frame   = 1#config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:86
map_size	/home/neale/ctf_RL/archive_code/run_td_central.py	/^map_size     = 40#config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:87
minibatch_size	/home/neale/ctf_RL/archive_code/run_td_central.py	/^minibatch_size = 256$/;"	variable	line:90
epoch	/home/neale/ctf_RL/archive_code/run_td_central.py	/^epoch = 1$/;"	variable	line:91
minimum_batch_size	/home/neale/ctf_RL/archive_code/run_td_central.py	/^minimum_batch_size = 1024$/;"	variable	line:92
nchannel	/home/neale/ctf_RL/archive_code/run_td_central.py	/^nchannel = 6 * keep_frame$/;"	variable	line:97
input_size	/home/neale/ctf_RL/archive_code/run_td_central.py	/^input_size = [None,40,40,6]$/;"	variable	line:98
log_episodic_reward	/home/neale/ctf_RL/archive_code/run_td_central.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:101
log_winrate	/home/neale/ctf_RL/archive_code/run_td_central.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:102
log_redwinrate	/home/neale/ctf_RL/archive_code/run_td_central.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:103
log_looptime	/home/neale/ctf_RL/archive_code/run_td_central.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:104
log_traintime	/home/neale/ctf_RL/archive_code/run_td_central.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:105
map_list	/home/neale/ctf_RL/archive_code/run_td_central.py	/^map_list = [os.path.join(MAP_PATH, path) for path in os.listdir(MAP_PATH) if path[:5]=='board']$/;"	variable	line:108
make_env	/home/neale/ctf_RL/archive_code/run_td_central.py	/^def make_env(map_size):$/;"	function	line:109
envs	/home/neale/ctf_RL/archive_code/run_td_central.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:112
envs	/home/neale/ctf_RL/archive_code/run_td_central.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:113
num_blue	/home/neale/ctf_RL/archive_code/run_td_central.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:114
num_red	/home/neale/ctf_RL/archive_code/run_td_central.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:115
num_agent	/home/neale/ctf_RL/archive_code/run_td_central.py	/^num_agent = num_blue#+num_red$/;"	variable	line:116
progbar	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name=TRAIN_TAG)$/;"	variable	line:119
atoms	/home/neale/ctf_RL/archive_code/run_td_central.py	/^atoms=8$/;"	variable	line:121
network	/home/neale/ctf_RL/archive_code/run_td_central.py	/^network = Network(input_shape=input_size, action_size=action_space, scope='main', save_path=MODEL_PATH, atoms=atoms)$/;"	variable	line:122
global_episodes	/home/neale/ctf_RL/archive_code/run_td_central.py	/^global_episodes = 0$/;"	variable	line:125
writer	/home/neale/ctf_RL/archive_code/run_td_central.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:128
train	/home/neale/ctf_RL/archive_code/run_td_central.py	/^def train(trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, step=None):$/;"	function	line:133
batch	/home/neale/ctf_RL/archive_code/run_td_central.py	/^batch = []$/;"	variable	line:179
num_batch	/home/neale/ctf_RL/archive_code/run_td_central.py	/^num_batch = 0$/;"	variable	line:180
log_on	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:183
save_on	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:184
episode_rew	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:187
was_done	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:188
trajs	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:190
s1	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    s1 = envs.reset($/;"	variable	line:193
map_pool	/home/neale/ctf_RL/archive_code/run_td_central.py	/^            map_pool=map_list,$/;"	variable	line:194
config_path	/home/neale/ctf_RL/archive_code/run_td_central.py	/^            config_path=env_setting_path,$/;"	variable	line:195
policy_red	/home/neale/ctf_RL/archive_code/run_td_central.py	/^            policy_red=policy.Roomba,$/;"	variable	line:196
policy_blue	/home/neale/ctf_RL/archive_code/run_td_central.py	/^            policy_blue=policy.Roomba)$/;"	variable	line:197
s1	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    s1 = envs.get_obs_blue()$/;"	variable	line:198
stime_roll	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    stime_roll = time.time()$/;"	variable	line:201
s0	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        s0 = s1$/;"	variable	line:203
s1	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        s1 = envs.get_obs_blue()$/;"	variable	line:206
was_done	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        was_done = done$/;"	variable	line:218
etime_roll	/home/neale/ctf_RL/archive_code/run_td_central.py	/^    etime_roll = time.time()$/;"	variable	line:222
log_image_on	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:227
stime_train	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        stime_train = time.time()$/;"	variable	line:228
etime_train	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        etime_train = time.time()$/;"	variable	line:230
batch	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        batch = []$/;"	variable	line:231
num_batch	/home/neale/ctf_RL/archive_code/run_td_central.py	/^        num_batch = 0$/;"	variable	line:232
tag	/home/neale/ctf_RL/archive_code/run_td_central.py	/^            tag = 'baseline_training\/'$/;"	variable	line:246
device_ground	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^device_ground = '\/gpu:0'$/;"	variable	line:37
device_air	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^device_air = '\/gpu:1'$/;"	variable	line:38
PROGBAR	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^PROGBAR = True$/;"	variable	line:40
LOG_DEVICE	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^LOG_DEVICE = False$/;"	variable	line:41
OVERRIDE	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^OVERRIDE = False$/;"	variable	line:42
TRAIN_NAME	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^TRAIN_NAME = sys.argv[1]$/;"	variable	line:45
LOG_PATH	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^LOG_PATH = '.\/logs\/'+TRAIN_NAME$/;"	variable	line:46
MODEL_PATH	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^MODEL_PATH = '.\/model\/' + TRAIN_NAME$/;"	variable	line:47
SAVE_PATH	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^SAVE_PATH = '.\/save\/' + TRAIN_NAME$/;"	variable	line:48
MAP_PATH	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^MAP_PATH = '.\/fair_map'$/;"	variable	line:49
GPU_CAPACITY	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:50
NENV	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^NENV = 8# multiprocessing.cpu_count() \/\/ 2$/;"	variable	line:52
env_setting_path	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^env_setting_path = sys.argv[2]$/;"	variable	line:57
config_path	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^config_path = 'config.ini'$/;"	variable	line:65
config	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^config = configparser.ConfigParser()$/;"	variable	line:66
total_episodes	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^total_episodes = 1000000#config.getint('TRAINING', 'TOTAL_EPISODES')$/;"	variable	line:70
max_ep	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^max_ep         = config.getint('TRAINING', 'MAX_STEP')$/;"	variable	line:71
gamma	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^gamma          = config.getfloat('TRAINING', 'DISCOUNT_RATE')$/;"	variable	line:72
lambd	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^lambd          = config.getfloat('TRAINING', 'GAE_LAMBDA')$/;"	variable	line:73
ppo_e	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^ppo_e          = config.getfloat('TRAINING', 'PPO_EPSILON')$/;"	variable	line:74
critic_beta	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^critic_beta    = config.getfloat('TRAINING', 'CRITIC_BETA')$/;"	variable	line:75
entropy_beta	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^entropy_beta   = config.getfloat('TRAINING', 'ENTROPY_BETA')$/;"	variable	line:76
lr_a	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^lr_a           = config.getfloat('TRAINING', 'LR_ACTOR')$/;"	variable	line:77
lr_c	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^lr_c           = config.getfloat('TRAINING', 'LR_CRITIC')$/;"	variable	line:78
save_network_frequency	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^save_network_frequency = config.getint('LOG', 'SAVE_NETWORK_FREQ')$/;"	variable	line:81
save_stat_frequency	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^save_stat_frequency    = config.getint('LOG', 'SAVE_STATISTICS_FREQ')$/;"	variable	line:82
save_image_frequency	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^save_image_frequency   = config.getint('LOG', 'SAVE_STATISTICS_FREQ') \/\/ 2$/;"	variable	line:83
moving_average_step	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^moving_average_step    = config.getint('LOG', 'MOVING_AVERAGE_SIZE')$/;"	variable	line:84
action_space	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^action_space = config.getint('DEFAULT', 'ACTION_SPACE')$/;"	variable	line:87
vision_range	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^vision_range = 29 #config.getint('DEFAULT', 'VISION_RANGE')$/;"	variable	line:88
keep_frame	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^keep_frame   = 1 #config.getint('DEFAULT', 'KEEP_FRAME')$/;"	variable	line:89
map_size	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^map_size     = 30 #config.getint('DEFAULT', 'MAP_SIZE')$/;"	variable	line:90
minibatch_size	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^minibatch_size = 256$/;"	variable	line:93
epoch	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^epoch = 1$/;"	variable	line:94
minimum_batch_size	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^minimum_batch_size = 4096$/;"	variable	line:95
nchannel	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^nchannel = 6 * keep_frame$/;"	variable	line:99
input_size	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:100
log_episodic_reward	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:103
log_length	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^log_length = MovingAverage(moving_average_step)$/;"	variable	line:104
log_winrate	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:105
log_redwinrate	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:106
log_looptime	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:107
log_traintime	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:108
make_env	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^def make_env(map_size):$/;"	function	line:111
envs_list	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^envs_list = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:117
envs	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^envs = SubprocVecEnv(envs_list, keep_frame, size=vision_dx)$/;"	variable	line:118
num_blue	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:119
num_red	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:120
gpu_options	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=GPU_CAPACITY, allow_growth=True)$/;"	variable	line:123
config	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^config = tf.ConfigProto(gpu_options=gpu_options, log_device_placement=LOG_DEVICE, allow_soft_placement=True)$/;"	variable	line:124
progbar	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    progbar = tf.keras.utils.Progbar(None, unit_name=TRAIN_NAME)$/;"	variable	line:127
sess	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^sess = tf.Session(config=config)$/;"	variable	line:129
global_step	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^global_step = tf.Variable(0, trainable=False, name='global_step')$/;"	variable	line:131
global_step_next	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^global_step_next = tf.assign_add(global_step, NENV)$/;"	variable	line:132
network	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    network = Network(input_shape=input_size, action_size=action_space, scope='ground', sess=sess)$/;"	variable	line:134
network_air	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    network_air = Network(input_shape=input_size, action_size=action_space, scope='uav', sess=sess)$/;"	variable	line:136
global_episodes	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^global_episodes = 0$/;"	variable	line:139
saver	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^saver = tf.train.Saver(max_to_keep=3, keep_checkpoint_every_n_hours=4)$/;"	variable	line:140
global_episodes	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    global_episodes = sess.run(global_step)$/;"	variable	line:145
writer	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^writer = tf.summary.FileWriter(LOG_PATH, sess.graph)$/;"	variable	line:147
train	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^def train(nn, trajs, bootstrap=0.0, epoch=epoch, batch_size=minibatch_size, writer=None, log=False, global_episodes=None):$/;"	function	line:151
get_action	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^def get_action(states, N=16):$/;"	function	line:191
num_batch	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^num_batch = 0$/;"	variable	line:240
log_on	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, 'log')$/;"	variable	line:242
log_image_on	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    log_image_on = interval_flag(global_episodes, save_image_frequency, 'im_log')$/;"	variable	line:243
save_on	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, 'save')$/;"	variable	line:244
episode_rew	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:247
was_alive	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    was_alive = [True for agent in range(NENV*(num_blue*num_red))]$/;"	variable	line:248
was_done	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    was_done = [False for env in range(NENV)]$/;"	variable	line:249
is_air	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    is_air = np.array([agent.is_air for agent in envs.get_team_blue().flat]).reshape([NENV, num_blue])$/;"	variable	line:250
is_air_red	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    is_air_red = np.array([agent.is_air for agent in envs.get_team_red().flat]).reshape([NENV, num_red])$/;"	variable	line:251
is_air	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    is_air = np.concatenate([is_air, is_air_red], axis=1).reshape([-1])$/;"	variable	line:252
trajs	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    trajs = [Trajectory(depth=9) for _ in range((num_blue+num_red)*NENV)] # Trajectory per agent$/;"	variable	line:254
s1	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    s1 = envs.reset(config_path=env_setting_path) #, custom_board='fair_uav\/board_0001.txt')$/;"	variable	line:257
stime_roll	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    stime_roll = time.time()$/;"	variable	line:261
s0	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        s0 = s1$/;"	variable	line:263
psi0	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        psi0 = psi$/;"	variable	line:265
logits	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        logits = logits1$/;"	variable	line:266
reward_red	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        reward_red = np.array([i['red_reward'] for i in info])$/;"	variable	line:270
env_reward	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        env_reward = np.vstack((reward, reward_red)).T.reshape([-1])$/;"	variable	line:271
is_alive	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        is_alive = np.array([agent.isAlive for agent in envs.get_team_blue().flat]).reshape([NENV, num_blue])$/;"	variable	line:273
is_alive_red	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        is_alive_red = np.array([agent.isAlive for agent in envs.get_team_red().flat]).reshape([NENV, num_red])$/;"	variable	line:274
is_alive	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        is_alive = np.concatenate([is_alive, is_alive_red], axis=1).reshape([-1])$/;"	variable	line:275
was_done	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        was_done = np.array(was_done, dtype=bool)$/;"	variable	line:281
env_idx	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^            env_idx = idx \/\/ (num_blue+num_red)$/;"	variable	line:284
env_team_idx	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^            env_team_idx = idx \/\/ num_blue$/;"	variable	line:285
agent_reward	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^                agent_reward = env_reward[env_team_idx]$/;"	variable	line:287
was_alive	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        was_alive = is_alive$/;"	variable	line:290
was_done	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        was_done = done$/;"	variable	line:291
etime_roll	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    etime_roll = time.time()$/;"	variable	line:295
stime_train	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        stime_train = time.time()$/;"	variable	line:305
etime_train	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        etime_train = time.time()$/;"	variable	line:308
num_batch	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        num_batch = 0$/;"	variable	line:310
steps	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^    steps = []$/;"	variable	line:313
tag	/home/neale/ctf_RL/archive_code/psi_trainer.py	/^        tag = 'psi_training\/'$/;"	variable	line:329
physical_devices	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:21
parser	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^parser = argparse.ArgumentParser(description="CVDC(learnability) trainer for convoy")$/;"	variable	line:51
args	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^args = parser.parse_args()$/;"	variable	line:62
PROGBAR	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^PROGBAR = args.silence$/;"	variable	line:64
TRAIN_NAME	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^TRAIN_NAME = "CVDC_decOnly_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:67
TRAIN_TAG	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^TRAIN_TAG = "decentralized control(learnability), " + TRAIN_NAME$/;"	variable	line:76
LOG_PATH	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:77
MODEL_PATH	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:78
SAVE_PATH	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^SAVE_PATH = ".\/save\/" + TRAIN_NAME$/;"	variable	line:79
MAP_PATH	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:80
GPU_CAPACITY	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:81
NENV	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:85
env_setting_path	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:88
game_config	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:89
config_path	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^config_path = "config.ini"$/;"	variable	line:102
config	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^config = configparser.ConfigParser()$/;"	variable	line:103
total_episodes	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^total_episodes = 100000$/;"	variable	line:107
max_ep	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^max_ep = 200$/;"	variable	line:108
gamma	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:109
lambd	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:110
save_network_frequency	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^save_network_frequency = 1024$/;"	variable	line:112
save_stat_frequency	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^save_stat_frequency = 128$/;"	variable	line:113
save_image_frequency	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^save_image_frequency = 128$/;"	variable	line:114
moving_average_step	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:115
action_space	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^action_space = 5$/;"	variable	line:117
keep_frame	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^keep_frame = 1$/;"	variable	line:118
map_size	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^map_size = args.map_size$/;"	variable	line:119
vision_range	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^vision_range = map_size - 1$/;"	variable	line:120
nchannel	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^nchannel = 6 * keep_frame$/;"	variable	line:122
input_size	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:123
cent_input_size	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:124
minibatch_size	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^minibatch_size = 128$/;"	variable	line:126
epoch	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^epoch = 1$/;"	variable	line:127
minimum_batch_size	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^minimum_batch_size = 1024 * 4$/;"	variable	line:128
log_episodic_reward	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:131
log_winrate	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:132
log_redwinrate	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:133
log_looptime	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:134
log_traintime	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:135
_qenv	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^_qenv = gym.make("cap-v0", map_size=map_size, config_path=game_config)$/;"	variable	line:139
make_env	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^def make_env(map_size):$/;"	function	line:140
envs	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:144
envs	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:145
num_blue	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:146
num_red	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:147
num_agent	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^num_agent = num_blue  # +num_red$/;"	variable	line:148
agent_type	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^agent_type = []$/;"	variable	line:154
num_type	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^num_type = len(agent_type)$/;"	variable	line:159
agent_type_masking	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:160
agent_type_index	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:161
prev_i	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^prev_i = 0$/;"	variable	line:162
prev_i	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    prev_i = i$/;"	variable	line:166
agent_type_masking	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:167
atoms	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^atoms = 256$/;"	variable	line:170
network	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^network = Network($/;"	variable	line:171
central_obs_shape	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    central_obs_shape=cent_input_size,$/;"	variable	line:172
decentral_obs_shape	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    decentral_obs_shape=input_size,$/;"	variable	line:173
action_size	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    action_size=action_space,$/;"	variable	line:174
agent_type	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    agent_type=agent_type,$/;"	variable	line:175
atoms	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    atoms=atoms,$/;"	variable	line:176
save_path	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    save_path=MODEL_PATH,$/;"	variable	line:177
global_episodes	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^global_episodes = network.initiate()$/;"	variable	line:181
writer	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:185
train_decentral	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^def train_decentral($/;"	function	line:188
train_datasets	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    train_datasets = []$/;"	variable	line:197
traj_buffer_list	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:200
advantage_lists	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    advantage_lists = [[] for _ in range(num_type)]$/;"	variable	line:201
atype	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            atype = agent_type_index[idx]$/;"	variable	line:204
reward	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            reward = traj[2]$/;"	variable	line:206
mask	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            mask = traj[3]$/;"	variable	line:207
critic	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            critic = traj[5]$/;"	variable	line:208
phi	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            phi = np.array(traj[7]).tolist()$/;"	variable	line:209
psi	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            psi = np.array(traj[8]).tolist()$/;"	variable	line:210
_critic	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            _critic = traj[9][-1]$/;"	variable	line:211
_psi	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            _psi = np.array(traj[10][-1])$/;"	variable	line:212
normalize	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^                normalize=False$/;"	variable	line:218
normalize	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^                normalize=False,$/;"	variable	line:227
discount_adv	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^                discount_adv=False,$/;"	variable	line:246
normalize	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^                normalize=False,$/;"	variable	line:247
beta	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            beta = max(min((-0.9\/30000)*step + 1, 1.0),0.1)$/;"	variable	line:249
traj_buffer	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:251
traj_buffer	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:262
train_dataset	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        train_dataset = ($/;"	variable	line:263
tag	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            tag = "advantages\/"$/;"	variable	line:288
run_network	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^def run_network(states):$/;"	function	line:297
dec_batch	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^dec_batch = []$/;"	variable	line:336
log_save_analysis	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    log_save_analysis = False  #interval_flag(global_episodes, 1024 * 4, "save_log")$/;"	variable	line:340
episode_rew	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:343
is_alive	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:344
is_done	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    is_done = [False for env in range(NENV * num_agent)]$/;"	variable	line:345
trajs	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    trajs = [[Trajectory(depth=14) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:347
s1	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    s1 = envs.reset(config_path=game_config, policy_red=policy.Roomba,)$/;"	variable	line:354
s1	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:355
reward_pred_list	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    reward_pred_list = []$/;"	variable	line:359
stime_roll	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    stime_roll = time.time()$/;"	variable	line:362
_states	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    _states = [[] for _ in range(NENV)]$/;"	variable	line:363
_agent1_r	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    _agent1_r = [[] for _ in range(NENV)]$/;"	variable	line:364
_agent2_r	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    _agent2_r = [[] for _ in range(NENV)]$/;"	variable	line:365
_agent3_r	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    _agent3_r = [[] for _ in range(NENV)]$/;"	variable	line:366
_agent1_o	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    _agent1_o = [[] for _ in range(NENV)]$/;"	variable	line:367
_agent2_o	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    _agent2_o = [[] for _ in range(NENV)]$/;"	variable	line:368
_agent3_o	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    _agent3_o = [[] for _ in range(NENV)]$/;"	variable	line:369
s0	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        s0 = s1$/;"	variable	line:372
a0	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        a0 = a1$/;"	variable	line:373
vg0	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        vg0 = vg1$/;"	variable	line:374
vc0	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        vc0 = vc1$/;"	variable	line:375
psi0	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        psi0 = psi1$/;"	variable	line:376
phi0	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        phi0 = phi1$/;"	variable	line:377
log_logits0	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        log_logits0 = log_logits1$/;"	variable	line:378
was_alive	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        was_alive = is_alive$/;"	variable	line:379
was_done	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        was_done = is_done$/;"	variable	line:380
is_alive	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:384
s1	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        s1 = s1.astype(np.float32)  # Decentralize observation$/;"	variable	line:385
idx	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:396
etime_roll	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    etime_roll = time.time()$/;"	variable	line:420
stime_train	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        stime_train = time.time()$/;"	variable	line:425
log	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        log = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:426
log_image	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        log_image = interval_flag(global_episodes, 1024, "ima_log")$/;"	variable	line:427
epoch	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            epoch=epoch,$/;"	variable	line:430
batch_size	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            batch_size=minibatch_size,$/;"	variable	line:431
writer	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            writer=writer,$/;"	variable	line:432
log	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            log=log,$/;"	variable	line:433
step	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            step=global_episodes,$/;"	variable	line:434
log_image	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            log_image=log_image,$/;"	variable	line:435
etime_train	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        etime_train = time.time()$/;"	variable	line:437
dec_batch	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^        dec_batch = []$/;"	variable	line:438
log_on	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:450
tag	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^            tag = "baseline_training\/"$/;"	variable	line:453
step	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^                step=global_episodes,$/;"	variable	line:468
save_on	/home/neale/ctf_RL/run_cvdc_decOnly.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:472
physical_devices	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^physical_devices = tf.config.experimental.list_physical_devices("GPU")$/;"	variable	line:18
parser	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^parser = argparse.ArgumentParser(description="CVDC(learnability) trainer for convoy")$/;"	variable	line:48
args	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^args = parser.parse_args()$/;"	variable	line:59
PROGBAR	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^PROGBAR = args.silence$/;"	variable	line:61
TRAIN_NAME	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^TRAIN_NAME = "PPO_SF_{}_{:02d}_convoy_{}g{}a_{}g{}a_m{}".format($/;"	variable	line:64
TRAIN_TAG	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^TRAIN_TAG = "Central value decentralized control(PPO+SF), " + TRAIN_NAME$/;"	variable	line:73
LOG_PATH	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^LOG_PATH = ".\/logs\/" + TRAIN_NAME$/;"	variable	line:74
MODEL_PATH	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^MODEL_PATH = ".\/model\/" + TRAIN_NAME$/;"	variable	line:75
SAVE_PATH	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^SAVE_PATH = ".\/save\/" + TRAIN_NAME$/;"	variable	line:76
MAP_PATH	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^MAP_PATH = ".\/fair_3g_20"$/;"	variable	line:77
GPU_CAPACITY	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^GPU_CAPACITY = 0.95$/;"	variable	line:78
NENV	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^NENV = multiprocessing.cpu_count() \/\/ 4$/;"	variable	line:82
env_setting_path	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^env_setting_path = "env_setting_convoy.ini"$/;"	variable	line:85
game_config	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^game_config = configparser.ConfigParser()$/;"	variable	line:86
config_path	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^config_path = "config.ini"$/;"	variable	line:99
config	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^config = configparser.ConfigParser()$/;"	variable	line:100
total_episodes	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^total_episodes = 100000$/;"	variable	line:104
max_ep	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^max_ep = 200$/;"	variable	line:105
gamma	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^gamma = 0.98  # GAE - discount$/;"	variable	line:106
lambd	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^lambd = 0.98  # GAE - lambda$/;"	variable	line:107
save_network_frequency	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^save_network_frequency = 1024$/;"	variable	line:109
save_stat_frequency	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^save_stat_frequency = 128$/;"	variable	line:110
save_image_frequency	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^save_image_frequency = 128$/;"	variable	line:111
moving_average_step	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^moving_average_step = 256  # MA for recording episode statistics$/;"	variable	line:112
action_space	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^action_space = 5$/;"	variable	line:114
keep_frame	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^keep_frame = 1$/;"	variable	line:115
map_size	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^map_size = args.map_size$/;"	variable	line:116
vision_range	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^vision_range = map_size - 1$/;"	variable	line:117
nchannel	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^nchannel = 6 * keep_frame$/;"	variable	line:119
input_size	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^input_size = [None, vision_dx, vision_dy, nchannel]$/;"	variable	line:120
cent_input_size	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^cent_input_size = [None, map_size, map_size, nchannel]$/;"	variable	line:121
minibatch_size	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^minibatch_size = 128$/;"	variable	line:123
epoch	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^epoch = 1$/;"	variable	line:124
minimum_batch_size	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^minimum_batch_size = 1024 * 4$/;"	variable	line:125
log_episodic_reward	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^log_episodic_reward = MovingAverage(moving_average_step)$/;"	variable	line:128
log_winrate	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^log_winrate = MovingAverage(moving_average_step)$/;"	variable	line:129
log_redwinrate	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^log_redwinrate = MovingAverage(moving_average_step)$/;"	variable	line:130
log_looptime	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^log_looptime = MovingAverage(moving_average_step)$/;"	variable	line:131
log_traintime	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^log_traintime = MovingAverage(moving_average_step)$/;"	variable	line:132
_qenv	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^_qenv = gym.make("cap-v0", map_size=map_size, config_path=game_config)$/;"	variable	line:136
make_env	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^def make_env(map_size):$/;"	function	line:137
envs	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^envs = [make_env(map_size) for i in range(NENV)]$/;"	variable	line:141
envs	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^envs = SubprocVecEnv(envs, keep_frame=keep_frame, size=vision_dx)$/;"	variable	line:142
num_blue	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^num_blue = len(envs.get_team_blue()[0])$/;"	variable	line:143
num_red	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^num_red = len(envs.get_team_red()[0])$/;"	variable	line:144
num_agent	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^num_agent = num_blue  # +num_red$/;"	variable	line:145
agent_type	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^agent_type = []$/;"	variable	line:151
num_type	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^num_type = len(agent_type)$/;"	variable	line:156
agent_type_masking	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^agent_type_masking = np.zeros([num_type, num_blue], dtype=bool)$/;"	variable	line:157
agent_type_index	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^agent_type_index = np.zeros([num_blue], dtype=int)$/;"	variable	line:158
prev_i	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^prev_i = 0$/;"	variable	line:159
prev_i	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    prev_i = i$/;"	variable	line:163
agent_type_masking	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^agent_type_masking = np.tile(agent_type_masking, NENV)$/;"	variable	line:164
atoms	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^atoms = 256$/;"	variable	line:167
network	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^network = Network($/;"	variable	line:168
central_obs_shape	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    central_obs_shape=cent_input_size,$/;"	variable	line:169
decentral_obs_shape	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    decentral_obs_shape=input_size,$/;"	variable	line:170
action_size	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    action_size=action_space,$/;"	variable	line:171
agent_type	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    agent_type=agent_type,$/;"	variable	line:172
atoms	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    atoms=atoms,$/;"	variable	line:173
save_path	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    save_path=MODEL_PATH,$/;"	variable	line:174
global_episodes	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^global_episodes = network.initiate()$/;"	variable	line:178
writer	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^writer = tf.summary.create_file_writer(LOG_PATH)$/;"	variable	line:182
train_central	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^def train_central($/;"	function	line:185
traj_buffer	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    traj_buffer = defaultdict(list)$/;"	variable	line:195
states	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        states = np.array(traj[0])$/;"	variable	line:198
last_state	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        last_state = np.array(traj[3])[-1:, :, :, :]$/;"	variable	line:199
reward	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        reward = traj[2]$/;"	variable	line:200
critic	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        critic = env_critic["critic"].numpy()[:, 0].tolist()$/;"	variable	line:204
_critic	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        _critic = _env_critic["critic"].numpy()[0, 0]$/;"	variable	line:205
train_dataset	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    train_dataset = ($/;"	variable	line:213
train_decentral	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^def train_decentral($/;"	function	line:230
train_datasets	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    train_datasets = []$/;"	variable	line:239
traj_buffer_list	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    traj_buffer_list = [defaultdict(list) for _ in range(num_type)]$/;"	variable	line:242
advantage_lists	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    advantage_lists = [[] for _ in range(num_type)]$/;"	variable	line:243
atype	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            atype = agent_type_index[idx]$/;"	variable	line:246
reward	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            reward = traj[2]$/;"	variable	line:248
mask	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            mask = traj[3]$/;"	variable	line:249
critic	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            critic = traj[5]$/;"	variable	line:250
phi	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            phi = np.array(traj[7]).tolist()$/;"	variable	line:251
psi	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            psi = np.array(traj[8]).tolist()$/;"	variable	line:252
_critic	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            _critic = traj[9][-1]$/;"	variable	line:253
_psi	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            _psi = np.array(traj[10][-1])$/;"	variable	line:254
normalize	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^                normalize=False$/;"	variable	line:267
discount_adv	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^                discount_adv=False,$/;"	variable	line:276
normalize	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^                normalize=False,$/;"	variable	line:277
traj_buffer	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            traj_buffer = traj_buffer_list[atype]$/;"	variable	line:281
traj_buffer	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        traj_buffer = traj_buffer_list[atype]$/;"	variable	line:292
train_dataset	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        train_dataset = ($/;"	variable	line:293
tag	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            tag = "advantages\/"$/;"	variable	line:318
run_network	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^def run_network(states):$/;"	function	line:326
batch	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^batch = []$/;"	variable	line:365
dec_batch	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^dec_batch = []$/;"	variable	line:366
log_save_analysis	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    log_save_analysis = False  #interval_flag(global_episodes, 1024 * 4, "save_log")$/;"	variable	line:370
episode_rew	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    episode_rew = np.zeros(NENV)$/;"	variable	line:373
is_alive	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    is_alive = [True for agent in envs.get_team_blue().flat]$/;"	variable	line:374
is_done	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    is_done = [False for env in range(NENV * num_agent)]$/;"	variable	line:375
trajs	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    trajs = [[Trajectory(depth=12) for _ in range(num_agent)] for _ in range(NENV)]$/;"	variable	line:377
cent_trajs	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    cent_trajs = [Trajectory(depth=4) for _ in range(NENV)]$/;"	variable	line:378
s1	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    s1 = envs.reset(config_path=game_config, policy_red=policy.Roomba,)$/;"	variable	line:385
s1	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    s1 = s1.astype(np.float32)$/;"	variable	line:386
cent_s1	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:387
reward_pred_list	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    reward_pred_list = []$/;"	variable	line:391
stime_roll	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    stime_roll = time.time()$/;"	variable	line:394
_states	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    _states = [[] for _ in range(NENV)]$/;"	variable	line:395
_agent1_r	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    _agent1_r = [[] for _ in range(NENV)]$/;"	variable	line:396
_agent2_r	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    _agent2_r = [[] for _ in range(NENV)]$/;"	variable	line:397
_agent3_r	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    _agent3_r = [[] for _ in range(NENV)]$/;"	variable	line:398
_agent1_o	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    _agent1_o = [[] for _ in range(NENV)]$/;"	variable	line:399
_agent2_o	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    _agent2_o = [[] for _ in range(NENV)]$/;"	variable	line:400
_agent3_o	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    _agent3_o = [[] for _ in range(NENV)]$/;"	variable	line:401
s0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        s0 = s1$/;"	variable	line:404
a0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        a0 = a1$/;"	variable	line:405
vg0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        vg0 = vg1$/;"	variable	line:406
vc0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        vc0 = vc1$/;"	variable	line:407
psi0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        psi0 = psi1$/;"	variable	line:408
phi0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        phi0 = phi1$/;"	variable	line:409
log_logits0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        log_logits0 = log_logits1$/;"	variable	line:410
was_alive	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        was_alive = is_alive$/;"	variable	line:411
was_done	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        was_done = is_done$/;"	variable	line:412
cent_s0	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        cent_s0 = cent_s1$/;"	variable	line:413
is_alive	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        is_alive = [agent.isAlive for agent in envs.get_team_blue().flat]$/;"	variable	line:417
s1	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        s1 = s1.astype(np.float32)  # Decentralize observation$/;"	variable	line:418
cent_s1	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        cent_s1 = envs.get_obs_blue().astype(np.float32)  # Centralized$/;"	variable	line:419
idx	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^                idx = env_idx * num_agent + agent_id$/;"	variable	line:430
etime_roll	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    etime_roll = time.time()$/;"	variable	line:468
stime_train	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        stime_train = time.time()$/;"	variable	line:473
log	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        log = interval_flag(global_episodes, save_image_frequency, "im_log")$/;"	variable	line:474
log_image	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        log_image = interval_flag(global_episodes, 1024, "ima_log")$/;"	variable	line:475
epoch	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            epoch=epoch,$/;"	variable	line:478
batch_size	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            batch_size=minibatch_size,$/;"	variable	line:479
writer	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            writer=writer,$/;"	variable	line:480
log	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            log=log,$/;"	variable	line:481
step	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            step=global_episodes,$/;"	variable	line:482
log_image	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            log_image=log_image,$/;"	variable	line:483
etime_train	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        etime_train = time.time()$/;"	variable	line:485
dec_batch	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        dec_batch = []$/;"	variable	line:486
log_tc_on	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        log_tc_on = interval_flag(global_episodes, save_image_frequency, 'tc_log')$/;"	variable	line:491
batch	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^        batch = []$/;"	variable	line:493
log_on	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    log_on = interval_flag(global_episodes, save_stat_frequency, "log")$/;"	variable	line:504
tag	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^            tag = "baseline_training\/"$/;"	variable	line:507
step	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^                step=global_episodes,$/;"	variable	line:522
save_on	/home/neale/ctf_RL/run_multiagent_ppo_SF.py	/^    save_on = interval_flag(global_episodes, save_network_frequency, "save")$/;"	variable	line:526
discount	/home/neale/ctf_RL/utility/gae.py	/^def discount(x, gamma):$/;"	function	line:6
gae	/home/neale/ctf_RL/utility/gae.py	/^def gae($/;"	function	line:10
reward_np	/home/neale/ctf_RL/utility/gae.py	/^    reward_np = np.array(reward_list)$/;"	variable	line:47
value_ext	/home/neale/ctf_RL/utility/gae.py	/^    value_ext = np.array(value_list + [bootstrap])$/;"	variable	line:48
td_target	/home/neale/ctf_RL/utility/gae.py	/^        td_target = reward_np + gamma * value_ext[1:]$/;"	variable	line:51
td_target	/home/neale/ctf_RL/utility/gae.py	/^        td_target = reward_np + gamma * value_ext[1:] * (1 - np.array(mask))$/;"	variable	line:53
advantages	/home/neale/ctf_RL/utility/gae.py	/^    advantages = td_target - value_ext[:-1]$/;"	variable	line:54
advantages	/home/neale/ctf_RL/utility/gae.py	/^        advantages = discount(advantages, gamma * lambd)$/;"	variable	line:58
td_target	/home/neale/ctf_RL/utility/gae.py	/^        td_target = advantages + value_ext[:-1]$/;"	variable	line:61
advantages	/home/neale/ctf_RL/utility/gae.py	/^        advantages = (advantages - np.mean(advantages)) \/ (np.std(advantages) + 1e-6)$/;"	variable	line:65
centering	/home/neale/ctf_RL/utility/dataModule.py	/^def centering(obs, agents, H, W, padder=[1,0,0,0,0,0,0]):$/;"	function	line:5
Stacked_state	/home/neale/ctf_RL/utility/dataModule.py	/^class Stacked_state:$/;"	class	line:21
__init__	/home/neale/ctf_RL/utility/dataModule.py	/^    def __init__(self, keep_frame, axis):$/;"	member	line:22	class:Stacked_state
initiate	/home/neale/ctf_RL/utility/dataModule.py	/^    def initiate(self, obj):$/;"	member	line:27	class:Stacked_state
__call__	/home/neale/ctf_RL/utility/dataModule.py	/^    def __call__(self, obj=None):$/;"	member	line:30	class:Stacked_state	file:
VStacked_state	/home/neale/ctf_RL/utility/dataModule.py	/^class VStacked_state(Stacked_state):$/;"	class	line:38
__call__	/home/neale/ctf_RL/utility/dataModule.py	/^    def __call__(self, obj=None):$/;"	member	line:39	class:VStacked_state	file:
oh_to_rgb	/home/neale/ctf_RL/utility/dataModule.py	/^def oh_to_rgb(state):$/;"	function	line:47
record	/home/neale/ctf_RL/utility/logger.py	/^def record(item, writer, step):$/;"	function	line:10
tb_log_histogram	/home/neale/ctf_RL/utility/logger.py	/^def tb_log_histogram(data, tag, step, **kargs):$/;"	function	line:17
tb_log_ctf_frame	/home/neale/ctf_RL/utility/logger.py	/^def tb_log_ctf_frame(frame, tag, step):$/;"	function	line:20
TrainedNetwork	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^class TrainedNetwork:$/;"	class	line:8
__init__	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def __init__($/;"	member	line:17	class:TrainedNetwork
get_action	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def get_action(self, input_tensor):$/;"	member	line:44	class:TrainedNetwork
reset_network_weight	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def reset_network_weight(self, path=None, step=None):$/;"	member	line:53	class:TrainedNetwork
_initialize_network	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def _initialize_network(self, verbose=False):$/;"	member	line:76	class:TrainedNetwork
vprint	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^        def vprint(*args):$/;"	function	line:80	function:TrainedNetwork._initialize_network
_get_node	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def _get_node(self, name):$/;"	member	line:115	class:TrainedNetwork
TrainedNetworkV2	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^class TrainedNetworkV2:$/;"	class	line:122
__init__	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def __init__($/;"	member	line:131	class:TrainedNetworkV2
reset_network_weight	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def reset_network_weight(self, path=None, step=None):$/;"	member	line:151	class:TrainedNetworkV2
_initialize_network	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def _initialize_network(self, verbose=False):$/;"	member	line:174	class:TrainedNetworkV2
vprint	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^        def vprint(*args):$/;"	function	line:178	function:TrainedNetworkV2._initialize_network
_get_node	/home/neale/ctf_RL/utility/RL_Wrapper.py	/^    def _get_node(self, name):$/;"	member	line:201	class:TrainedNetworkV2
CloudpickleWrapper	/home/neale/ctf_RL/utility/multiprocessing.py	/^class CloudpickleWrapper(object):$/;"	class	line:10
__init__	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def __init__(self, x):$/;"	member	line:14	class:CloudpickleWrapper
__getstate__	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def __getstate__(self):$/;"	member	line:16	class:CloudpickleWrapper	file:
__setstate__	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def __setstate__(self, ob):$/;"	member	line:19	class:CloudpickleWrapper	file:
worker	/home/neale/ctf_RL/utility/multiprocessing.py	/^def worker(idx, remote, parent_remote, env_fn_wrapper, keep_frame=1, size=39):$/;"	function	line:23
SubprocVecEnv	/home/neale/ctf_RL/utility/multiprocessing.py	/^class SubprocVecEnv:$/;"	class	line:72
__init__	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def __init__(self, env_fns, keep_frame=1, size=39):$/;"	member	line:76	class:SubprocVecEnv
step	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def step(self, actions=None):$/;"	member	line:108	class:SubprocVecEnv
reset	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def reset(self, map_pool=None, **kwargs):$/;"	member	line:121	class:SubprocVecEnv
get_static_map	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def get_static_map(self):$/;"	member	line:132	class:SubprocVecEnv
get_obs_blue	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def get_obs_blue(self):$/;"	member	line:137	class:SubprocVecEnv
get_obs_red	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def get_obs_red(self):$/;"	member	line:142	class:SubprocVecEnv
get_full_state	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def get_full_state(self):$/;"	member	line:147	class:SubprocVecEnv
get_team_blue	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def get_team_blue(self):$/;"	member	line:152	class:SubprocVecEnv
get_team_red	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def get_team_red(self):$/;"	member	line:157	class:SubprocVecEnv
blue_win	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def blue_win(self):$/;"	member	line:162	class:SubprocVecEnv
red_win	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def red_win(self):$/;"	member	line:167	class:SubprocVecEnv
blue_flag_captured	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def blue_flag_captured(self):$/;"	member	line:172	class:SubprocVecEnv
red_flag_captured	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def red_flag_captured(self):$/;"	member	line:177	class:SubprocVecEnv
close	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def close(self):$/;"	member	line:182	class:SubprocVecEnv
__len__	/home/neale/ctf_RL/utility/multiprocessing.py	/^    def __len__(self):$/;"	member	line:197	class:SubprocVecEnv	file:
CloudpickleWrapper	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^class CloudpickleWrapper(object):$/;"	class	line:10
__init__	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def __init__(self, x):$/;"	member	line:14	class:CloudpickleWrapper
__getstate__	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def __getstate__(self):$/;"	member	line:16	class:CloudpickleWrapper	file:
__setstate__	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def __setstate__(self, ob):$/;"	member	line:19	class:CloudpickleWrapper	file:
worker	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^def worker(idx, remote, parent_remote, env_fn_wrapper, keep_frame=1, size=39):$/;"	function	line:23
SubprocVecEnv	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^class SubprocVecEnv:$/;"	class	line:72
__init__	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def __init__(self, env_fns, keep_frame=1, size=39):$/;"	member	line:76	class:SubprocVecEnv
step	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def step(self, actions=None):$/;"	member	line:108	class:SubprocVecEnv
reset	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def reset(self, **kwargs):$/;"	member	line:121	class:SubprocVecEnv
get_static_map	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def get_static_map(self):$/;"	member	line:126	class:SubprocVecEnv
get_full_state	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def get_full_state(self):$/;"	member	line:131	class:SubprocVecEnv
get_team_blue	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def get_team_blue(self):$/;"	member	line:136	class:SubprocVecEnv
get_team_red	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def get_team_red(self):$/;"	member	line:142	class:SubprocVecEnv
blue_win	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def blue_win(self):$/;"	member	line:147	class:SubprocVecEnv
red_win	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def red_win(self):$/;"	member	line:152	class:SubprocVecEnv
blue_flag_captured	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def blue_flag_captured(self):$/;"	member	line:157	class:SubprocVecEnv
red_flag_captured	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def red_flag_captured(self):$/;"	member	line:162	class:SubprocVecEnv
close	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def close(self):$/;"	member	line:167	class:SubprocVecEnv
__len__	/home/neale/ctf_RL/utility/multiprocessing_seq.py	/^    def __len__(self):$/;"	member	line:182	class:SubprocVecEnv	file:
Elo	/home/neale/ctf_RL/utility/elopy.py	/^class Elo:$/;"	class	line:4
__init__	/home/neale/ctf_RL/utility/elopy.py	/^    def __init__(self, base_rating=1500):$/;"	member	line:5	class:Elo
__str__	/home/neale/ctf_RL/utility/elopy.py	/^    def __str__(self):$/;"	member	line:9	class:Elo	file:
save	/home/neale/ctf_RL/utility/elopy.py	/^    def save(self, filename='elo.pkl'):$/;"	member	line:21	class:Elo
load	/home/neale/ctf_RL/utility/elopy.py	/^    def load(self, filename='elo.pkl'):$/;"	member	line:25	class:Elo
getPlayer	/home/neale/ctf_RL/utility/elopy.py	/^    def getPlayer(self, name):$/;"	member	line:32	class:Elo
contains	/home/neale/ctf_RL/utility/elopy.py	/^    def contains(self, name):$/;"	member	line:38	class:Elo
addPlayer	/home/neale/ctf_RL/utility/elopy.py	/^    def addPlayer(self, name, rating=None):$/;"	member	line:44	class:Elo
removePlayer	/home/neale/ctf_RL/utility/elopy.py	/^    def removePlayer(self, name):$/;"	member	line:50	class:Elo
recordMatch	/home/neale/ctf_RL/utility/elopy.py	/^    def recordMatch(self, name1, name2, winner=None, draw=False, verbose=False):$/;"	member	line:54	class:Elo
getPlayerRating	/home/neale/ctf_RL/utility/elopy.py	/^    def getPlayerRating(self, name):$/;"	member	line:95	class:Elo
getPlayerList	/home/neale/ctf_RL/utility/elopy.py	/^    def getPlayerList(self):$/;"	member	line:100	class:Elo
getRatingList	/home/neale/ctf_RL/utility/elopy.py	/^    def getRatingList(self):$/;"	member	line:104	class:Elo
_Player	/home/neale/ctf_RL/utility/elopy.py	/^class _Player:$/;"	class	line:110
__init__	/home/neale/ctf_RL/utility/elopy.py	/^    def __init__(self, name, rating):$/;"	member	line:111	class:_Player
compareRating	/home/neale/ctf_RL/utility/elopy.py	/^    def compareRating(self, opponent, b=10, alpha=400.0):$/;"	member	line:115	class:_Player
store_args	/home/neale/ctf_RL/utility/utils.py	/^def store_args(method):$/;"	function	line:25
wrapper	/home/neale/ctf_RL/utility/utils.py	/^    def wrapper(*positional_args, **keyword_args):$/;"	function	line:41	function:store_args
interval_flag	/home/neale/ctf_RL/utility/utils.py	/^def interval_flag(step, freq, name):$/;"	function	line:54
path_create	/home/neale/ctf_RL/utility/utils.py	/^def path_create(path, override=False):$/;"	function	line:84
q_retrace	/home/neale/ctf_RL/utility/utils.py	/^def q_retrace(reward, value_ext, gamma, is_weight):$/;"	function	line:101
normalize	/home/neale/ctf_RL/utility/utils.py	/^def normalize(r):$/;"	function	line:115
retrace	/home/neale/ctf_RL/utility/utils.py	/^def retrace(targets, behaviors, lambda_=0.2):$/;"	function	line:129
retrace_prod	/home/neale/ctf_RL/utility/utils.py	/^def retrace_prod(targets, behaviors, lambda_=0.2):$/;"	function	line:151
MovingAverage	/home/neale/ctf_RL/utility/utils.py	/^class MovingAverage:$/;"	class	line:168
__init__	/home/neale/ctf_RL/utility/utils.py	/^    def __init__(self, size):$/;"	member	line:174	class:MovingAverage
__call__	/home/neale/ctf_RL/utility/utils.py	/^    def __call__(self):$/;"	member	line:185	class:MovingAverage	file:
tolist	/home/neale/ctf_RL/utility/utils.py	/^    def tolist(self):$/;"	member	line:189	class:MovingAverage
extend	/home/neale/ctf_RL/utility/utils.py	/^    def extend(self, l: list):$/;"	member	line:195	class:MovingAverage
append	/home/neale/ctf_RL/utility/utils.py	/^    def append(self, n):$/;"	member	line:207	class:MovingAverage
clear	/home/neale/ctf_RL/utility/utils.py	/^    def clear(self):$/;"	member	line:221	class:MovingAverage
random_batch_sampling	/home/neale/ctf_RL/utility/buffer.py	/^def random_batch_sampling(batch_size, epoch, *argv):$/;"	function	line:32
expense_batch_sampling	/home/neale/ctf_RL/utility/buffer.py	/^def expense_batch_sampling(batch_size, epoch, *argv):$/;"	function	line:50
Trajectory	/home/neale/ctf_RL/utility/buffer.py	/^class Trajectory:$/;"	class	line:72
__init__	/home/neale/ctf_RL/utility/buffer.py	/^    def __init__(self, depth=4):$/;"	member	line:97	class:Trajectory
__repr__	/home/neale/ctf_RL/utility/buffer.py	/^    def __repr__(self):$/;"	member	line:104	class:Trajectory	file:
__len__	/home/neale/ctf_RL/utility/buffer.py	/^    def __len__(self):$/;"	member	line:107	class:Trajectory	file:
__getitem__	/home/neale/ctf_RL/utility/buffer.py	/^    def __getitem__(self, index):$/;"	member	line:110	class:Trajectory	file:
__setitem__	/home/neale/ctf_RL/utility/buffer.py	/^    def __setitem__(self, key, item):$/;"	member	line:113	class:Trajectory	file:
append	/home/neale/ctf_RL/utility/buffer.py	/^    def append(self, mdp_tup):$/;"	member	line:116	class:Trajectory
trim	/home/neale/ctf_RL/utility/buffer.py	/^    def trim(self, trim_length):$/;"	member	line:120	class:Trajectory
clear	/home/neale/ctf_RL/utility/buffer.py	/^    def clear(self):$/;"	member	line:132	class:Trajectory
Trajectory_buffer	/home/neale/ctf_RL/utility/buffer.py	/^class Trajectory_buffer:$/;"	class	line:135
__init__	/home/neale/ctf_RL/utility/buffer.py	/^    def __init__(self, depth=4, capacity=256):$/;"	member	line:167	class:Trajectory_buffer
__call__	/home/neale/ctf_RL/utility/buffer.py	/^    def __call__(self):$/;"	member	line:182	class:Trajectory_buffer	file:
__repr__	/home/neale/ctf_RL/utility/buffer.py	/^    def __repr__(self):$/;"	member	line:185	class:Trajectory_buffer	file:
__len__	/home/neale/ctf_RL/utility/buffer.py	/^    def __len__(self):$/;"	member	line:189	class:Trajectory_buffer	file:
__getitem__	/home/neale/ctf_RL/utility/buffer.py	/^    def __getitem__(self, index):$/;"	member	line:192	class:Trajectory_buffer	file:
__setitem__	/home/neale/ctf_RL/utility/buffer.py	/^    def __setitem__(self, index, item):$/;"	member	line:195	class:Trajectory_buffer	file:
is_empty	/home/neale/ctf_RL/utility/buffer.py	/^    def is_empty(self):$/;"	member	line:198	class:Trajectory_buffer
is_full	/home/neale/ctf_RL/utility/buffer.py	/^    def is_full(self):$/;"	member	line:201	class:Trajectory_buffer
append	/home/neale/ctf_RL/utility/buffer.py	/^    def append(self, traj):$/;"	member	line:204	class:Trajectory_buffer
extend	/home/neale/ctf_RL/utility/buffer.py	/^    def extend(self, trajs):$/;"	member	line:209	class:Trajectory_buffer
sample	/home/neale/ctf_RL/utility/buffer.py	/^    def sample(self, flush=True):$/;"	member	line:218	class:Trajectory_buffer
Replay_buffer	/home/neale/ctf_RL/utility/buffer.py	/^class Replay_buffer:$/;"	class	line:246
__init__	/home/neale/ctf_RL/utility/buffer.py	/^    def __init__(self, depth=4, buffer_size=5000):$/;"	member	line:264	class:Replay_buffer
__len__	/home/neale/ctf_RL/utility/buffer.py	/^    def __len__(self):$/;"	member	line:269	class:Replay_buffer	file:
__call__	/home/neale/ctf_RL/utility/buffer.py	/^    def __call__(self):$/;"	member	line:272	class:Replay_buffer	file:
__getitem__	/home/neale/ctf_RL/utility/buffer.py	/^    def __getitem__(self, index):$/;"	member	line:275	class:Replay_buffer	file:
append	/home/neale/ctf_RL/utility/buffer.py	/^    def append(self, sample):$/;"	member	line:278	class:Replay_buffer
extend	/home/neale/ctf_RL/utility/buffer.py	/^    def extend(self, samples):$/;"	member	line:283	class:Replay_buffer
flush	/home/neale/ctf_RL/utility/buffer.py	/^    def flush(self):$/;"	member	line:289	class:Replay_buffer
empty	/home/neale/ctf_RL/utility/buffer.py	/^    def empty(self):$/;"	member	line:295	class:Replay_buffer
full	/home/neale/ctf_RL/utility/buffer.py	/^    def full(self):$/;"	member	line:298	class:Replay_buffer
shuffle	/home/neale/ctf_RL/utility/buffer.py	/^    def shuffle(self):$/;"	member	line:301	class:Replay_buffer
pop	/home/neale/ctf_RL/utility/buffer.py	/^    def pop(self, size, shuffle=False):$/;"	member	line:304	class:Replay_buffer
a	/home/neale/ctf_RL/utility/buffer.py	/^    a = np.random.randint(5, size=10)$/;"	variable	line:315	class:Replay_buffer
b	/home/neale/ctf_RL/utility/buffer.py	/^    b = np.random.randint(5, size=10)$/;"	variable	line:316	class:Replay_buffer
tr	/home/neale/ctf_RL/utility/buffer.py	/^    tr = Trajectory(depth=2)$/;"	variable	line:318	class:Replay_buffer
tr_buf	/home/neale/ctf_RL/utility/buffer.py	/^    tr_buf = Trajectory_buffer(depth=2)$/;"	variable	line:325	class:Replay_buffer
tf_clipped_log	/home/neale/ctf_RL/utility/tf_utils.py	/^def tf_clipped_log(val, vmin=1e-10, vmax=10.0):$/;"	function	line:5
